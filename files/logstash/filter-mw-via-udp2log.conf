# vim:set sw=2 ts=2 sts=2 et
# Parse MediaWiki log output as sent by udp2log
filter {

  if [type] == "udp2log" {
    # Parse a udp2log relay packet
    # Capture sequence_id and channel from packet and trim from message body
    grok {
      match => [
        "message",
        "^%{NUMBER:sequence_id} %{NOTSPACE:channel} %{GREEDYDATA:message}$"
      ]
      overwrite => [ "message" ]
      named_captures_only => true
    }

    # Explode message body on newlines
    split {
      add_tag => [ "split" ]
    }

    # Change message type to be channel name and discard
    mutate {
      replace => [ "type", "%{channel}" ]
      add_tag => [ "udp2log" ]
      remove_field => [ "channel" ]
    }

    # Hang on to the UDP packet sender in case later rules figure out another
    # host to attribute the message to
    mutate {
      add_field => [ "udp_sender", "%{host}" ]
    }

    # NOTE: `add_tag => [ "es" ]` is not done here, so by default none of the
    # events created by this initial parse phase will be added to
    # Elasticsearch. Individual types should be tagged below. This is intended
    # to keep spammy events out of the serach index.
  } # end [type] == "udp2log"


  if "udp2log" in [tags] {
    # Common "<DATE> <HOST> <WIKI>: <MESSAGE>" message format
    if [type] in [
      "antispoof",
      #"api", API logging is more suited for kafka than logstash
      "badpass",
      "bug46577",
      "Bug54847",
      "captcha",
      "centralauth",
      "CirrusSearch",
      "dbperformance",
      "dnsblacklist",
      "exception",
      "exception-json",
      "exec",
      "external",
      "filebackend-ops",
      "generated-pp-node-count",
      "geodata",
      "gettingstarted",
      "MassMessage",
      "memcached-serious",
      "mobile",
      "mwsearch",
      "poolcounter",
      "privatewiki-slow-parse",
      "recursion-guard",
      "redis",
      "resourceloader",
      "runJobs",
      "slow-parse",
      "spam",
      "sql-bagostuff",
      "squid",
      "swift-backend",
      "texvc",
      "thumbnail",
      "torblock",
      "wap",
      "zero"
    ] {
      # Join sequential lines into a single event
      multiline {
        pattern => "^2\d{3}-\d\d-\d\d \d\d:\d\d:\d\d "
        negate => true
        what => "previous"
      }

      # Separate logdate, host and wikidb from message
      grok {
        match => [
          "message",
          "^(?m)(?<logdate>2\d{3}-\d\d-\d\d \d\d:\d\d:\d\d) %{NOTSPACE:host} %{NOTSPACE:wikidb}: %{GREEDYDATA:message}$"
        ]
        overwrite => [ "host", "message" ]
        named_captures_only => true
        add_tag => [ "es" ]
      }

      if !("_grokparsefailure" in [tags]) {
        # Use the parsed timestamp as canonical for the event
        date {
          match => [ "logdate", "YYYY-MM-dd HH:mm:ss" ]
          remove_field => [ "logdate" ]
          add_tag => [ "logdate" ]
        }
      }
    } # end "<DATE> <HOST> <WIKI>:" logs


    if [type] == "apache2" {
      grok {
        match => [
          "message",
          "^(?<logdate>%{MONTH} %{MONTHDAY} %{TIME}) %{NOTSPACE:host}: %{GREEDYDATA:message}$"
        ]
        overwrite => [ "host", "message" ]
        named_captures_only => true
        add_tag => [ "es" ]
      }

      if !("_grokparsefailure" in [tags]) {
        # Use the parsed timestamp as canonical for the event
        date {
          match => [ "logdate", "MMM dd HH:mm:ss" ]
          remove_field => [ "logdate" ]
          add_tag => [ "logdate" ]
        }
      }
    } # end [type] == "apache2"


    if [type] == "api" {
      # Remove `API` from message and capture HTTP verb, user name and
      # remote_ip
      grok {
        match => [
          "message",
          "^(?m)API (?<message>%{WORD:verb} %{NOTSPACE:user} %{NOTSPACE:remote_ip} %{GREEDYDATA})$"
        ]
        overwrite => [ "message" ]
        named_captures_only => true
      }
      # Copy key=value pairs out of the message into a collection named "args"
      kv {
        target => "args"
      }
    } # end [type] == "api"


    if [type] == "dberror" {
      # Join sequential lines into a single event
      multiline {
        pattern => "^\w{3} \w{3} \d"
        negate => true
        what => "previous"
      }
      # Separate logdate, host and wikidb from message
      grok {
        match => [
          "message",
          "^(?<logdate>%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR})\t%{NOTSPACE:host}\t%{NOTSPACE:wikidb}\t%{GREEDYDATA:message}$"
        ]
        overwrite => [ "host", "message" ]
        named_captures_only => true
        add_tag => [ "es" ]
      }
      if !("_grokparsefailure" in [tags]) {
        # Use the parsed timestamp as canonical for the event
        date {
          match => [ "logdate", "EEE MMM d H:mm:ss z YYYY" ]
          remove_field => [ "logdate" ]
          add_tag => [ "logdate" ]
        }
      }
    } # end [type] == "dberror"


    if [type] == "exception-json" {
      # Parse message as json and put elements in event
      json {
        source => "message"
        add_tag => [ "json" ]
      }
      # Rename the `id` field to `exeception_id`
      mutate {
        rename => [ "id", "exception_id" ]
      }
      # Add a checksum value based on message + file + line
      # This should help us write tools to count duplicates
      checksum {
        keys => [ "message", "file", "line" ]
      }
    } # end [type] == "exception-json"


    if [type] == "fatal" {
      # Parse wmerrors php extension output
      # Join sequential lines into a single event
      multiline {
        pattern => "^\[\d\d-\w"
        negate => true
        what => "previous"
      }
      # Separate logdate, host and other data from message
      grok {
        match => [
          "message",
          "^(?m)\[(?<logdate>\d\d-\w{3}-\d{4} \d\d:\d\d:\d\d)\] %{GREEDYDATA:message}\nServer: %{NOTSPACE:host}\n(Method: %{NOTSPACE:verb}\n)?URL: %{NOTSPACE:url}\n(Cookie: %{GREEDYDATA:cookie}\n)?(Backtrace:\n%{GREEDYDATA:backtrace})?"
        ]
        overwrite => [ "host", "message" ]
        named_captures_only => true
        add_tag => [ "es" ]
      }
      if !("_grokparsefailure" in [tags]) {
        # Use the parsed timestamp as canonical for the event
        date {
          match => [ "logdate", "dd-MMM-YYYY-MM HH:mm:ss" ]
          remove_field => [ "logdate" ]
          add_tag => [ "logdate" ]
        }
        # Add a checksum value based on message
        # This should help us write tools to count duplicates
        checksum {
          keys => [ "message" ]
        }
        # Split cookie line
        kv {
          source => "cookie"
          target => "cookies"
          remove_field => [ "cookie" ]
        }
      }
    } # end [type] == "fatal"


    if [type] == "scholarships" {
      # Parse message as json and put elements in event
      json {
        source => "message"
        add_tag => [ "json" ]
      }
    } # end [type] == "scholarships"


    if [type] == "wap" {
      # Copy the user-agent from the message
      grok {
        match => [
          "message",
          "^User-agent: '(?<ua_raw>.*'(?:, |$))"
        ]
        named_captures_only => true
      }
      if !("_grokparsefailure" in [tags]) {
        # Analize ua using BrowserScope rules
        useragent {
          source => "ua_raw"
          prefix => "ua_"
          remove_field => [ "ua_raw" ]
        }
      }
    } # end [type] == "wap"


    if [type] == "xff" {
      # Separate logdate from message
      grok {
        match => [
          "message",
          "^(?<logdate>%{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE})\t%{GREEDYDATA:message}$"
        ]
        overwrite => [ "message" ]
        named_captures_only => true
      }
      if !("_grokparsefailure" in [tags]) {
        # Use the parsed timestamp as canonical for the event
        date {
          match => [ "logdate", "EEE, dd MMM YYYY HH:mm:ss Z" ]
          remove_field => [ "logdate" ]
          add_tag => [ "logdate" ]
        }
        # Copy XFF addresses from message
        grok {
          match => [
            "message",
            "^%{URI:url}\t(?:, )?(?<xff>(?:%{IP}(?:, )?)+)\t"
          ]
          named_captures_only => true
        }
        # Turn comma separated list of XFF addresses into a real list
        mutate {
          split => [ "xff", ", " ]
        }
      }
    } # end [type] == "xff"


    if [type] in [ "web", "cli" ] {
      # wfDebug logs from beta
      multiline {
        pattern => "^\w+-[0-9a-f]{8}: "
        negate => true
        what => "previous"
      }
      grok {
        match => [
          "message",
          "^(?m)(?<wikidb>\w+)-(?<request_id>[0-9a-f]{8}):%{SPACE}%{GREEDYDATA:message}$"
        ]
        overwrite => [ "message" ]
        named_captures_only => true
      }
    } # end [type] in [ "web", "cli" ]


  } # end "udp2log" in [tags]

}

