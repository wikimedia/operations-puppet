# vim:set sw=2 ts=2 sts=2 et
# Process GELF log output from various sources
filter {

  if [type] == "gelf" {
    # Tag for storage in elasticsearch and that GELF was the input protocol.
    mutate {
      add_tag => [ "es", "gelf" ]
    }

    # Convert GELF's 'facility' to Logstash's 'type'.
    # Convert GELF's 'short_message' to Logstash's 'message'.
    mutate {
      replace => [ "type", "%{facility}" ]
      replace => [ "message", "%{short_message}" ]
      remove_field => [ "facility", "short_message" ]
    }

    if [type] == "Hadoop" {
      mutate {
        replace => [ "channel", "%{SourceSimpleClassName}" ]
      }

      # Overwrite the Thread field with a shorter more generic version, saving
      # details to separate fields
      grok {
        # Oniguruma syntax for ‘named capture’: (?<field_name>the pattern here)
        match => [ "Thread", "(?<Thread>IPC Server handler) %{NUMBER:IPC_Server_handler_id} on %{NUMBER:IPC_Server_handler_port}" ]
        match => [ "Thread", "(?<Thread>DeletionService) #%{NUMBER:DeletionService_id}" ]
        match => [ "Thread", "(?<Thread>LocalizerRunner) for %{NOTSPACE:ContainerId}" ]
        match => [ "Thread", "(?<Thread>ContainersLauncher) #%{NUMBER:ContainersLauncher_id}" ]
        match => [ "Thread", "(?<Thread>LogAggregationService) #%{NUMBER:LogAggregationService_id}" ]
        match => [ "Thread", "(?<Thread>CacheReplicationMonitor)\(%{NUMBER:CacheReplicationMonitor_id}\)" ]
        overwrite => [ "Thread" ]
      }

      if [message] =~ /attempt_/ {
        # Extract attempt ID to field: attempt_1409078537822_52431_m_000009_1, attempt_1409078537822_55176_r_000000_0
        grok {
          match => [ "message", "attempt_(?<attempt_id>[0-9]+_[0-9]+_[mr]_[0-9]+_[0-9]+)" ]
          tag_on_failure => ["_grokparsefailure_attempt"]
        }
      }
      if [message] =~ /task_/ {
        # Extract task ID to field:       task_1409078537822_52431_m_000044
        grok {
          match => [ "message", "task_(?<task_id>[0-9]+_[0-9]+_[mr]_[0-9]+)" ]
          tag_on_failure => ["_grokparsefailure_task"]
        }
      }
      if [message] =~ /job_/ {
        # Extract job ID to field:         job_1409078537822_52431
        grok {
          match => [ "message", "job_(?<job_id>[0-9]+_[0-9]+)" ]
          tag_on_failure => ["_grokparsefailure_job"]
        }
      }

      if [task_id] != "" {
        # Infer task ID from attempt ID
        grok {
          match => [ "attempt_id", "(?<task_id>[0-9]+_[0-9]+_[mr]_[0-9]+)_[0-9]+" ]
          tag_on_failure => ["_grokparsefailure_infer_task"]
        }
      }

      if [job_id] != "" {
        # Infer job ID from task ID
        grok {
          match => [ "task_id", "(?<job_id>[0-9]+_[0-9]+)_[mr]_[0-9]+" ]
          tag_on_failure => ["_grokparsefailure_infer_job"]
        }
      }

      dns {
        reverse => [ "host" ]
        action  => "replace"
      }

    } # end [type] == "Hadoop"

    # Drop duplicated json content added by nodejs gelf-stream library
    mutate {
      remove_field => [ "full_message" ]
    }

    # Get rid of a whole bunch of useless _* fields that are duplicated.
    # We must keep _id, _index, and _type
    prune {
      blacklist_names => [ "^_(?!id$|index$|type$).*" ]
    }

  } # end [type] == "gelf"

}
