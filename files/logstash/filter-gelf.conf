filter {
  if [type] == "gelf" {
    mutate {
      # tag gelf messages for storage in elasticsearch
      add_tag => [ "es", "gelf" ]
    }
    mutate {
      # move field names to what we're expecting
      replace => [ "type", "%{facility}" ]
      replace => [ "message", "%{short_message}" ]
    }
    mutate {
      # get rid of fields we moved
      remove_field => [ "facility", "short_message" ]
    }
    if [facility] == "Hadoop" {
      mutate {
        replace => [ "channel", "%{SourceSimpleClassName}" ]
      }
      grok {
        # Oniguruma syntax for ‘named capture’: (?<field_name>the pattern here)
        # this overwrites the Thread field with a shorter more generic version, saving details to separate fields
        match => [ "Thread", "(?<Thread>IPC Server handler) %{NUMBER:IPC_Server_handler_id} on %{NUMBER:IPC_Server_handler_port}" ]
        match => [ "Thread", "(?<Thread>DeletionService) #%{NUMBER:DeletionService_id}" ]
        match => [ "Thread", "(?<Thread>LocalizerRunner) for %{NOTSPACE:ContainerId}" ]
        match => [ "Thread", "(?<Thread>ContainersLauncher) #%{NUMBER:ContainersLauncher_id}" ]
        match => [ "Thread", "(?<Thread>LogAggregationService) #%{NUMBER:LogAggregationService_id}" ]
        match => [ "Thread", "(?<Thread>CacheReplicationMonitor)\(%{NUMBER:CacheReplicationMonitor_id}\)" ]
        overwrite => [ "Thread" ]
      }
      if [message] =~ /attempt_/ {
        grok {
          # so that we can search by job id and find tasks and attempts:
          # extract attempt ID to field: attempt_1409078537822_52431_m_000009_1, attempt_1409078537822_55176_r_000000_0
          match => [ "message",         "attempt_(?<attempt_id>[0-9]+_[0-9]+_[mr]_[0-9]+_[0-9]+)" ]
          tag_on_failure => ["_grokparsefailure_attempt"]
        }
      }
      if [message] =~ /task_/ {
        grok {
          # extract task ID to field:       task_1409078537822_52431_m_000044
          match => [ "message",            "task_(?<task_id>[0-9]+_[0-9]+_[mr]_[0-9]+)" ]
          tag_on_failure => ["_grokparsefailure_task"]
        }
      }
      if [message] =~ /job_/ {
        grok {
          # extract job ID to field:         job_1409078537822_52431
          match => [ "message",             "job_(?<job_id>[0-9]+_[0-9]+)" ]
          tag_on_failure => ["_grokparsefailure_job"]
        }
      }
      if [task_id] != "" {
        grok {
          # infer task ID from attempt ID
          match => [ "attempt_id", "(?<task_id>[0-9]+_[0-9]+_[mr]_[0-9]+)_[0-9]+" ]
          tag_on_failure => ["_grokparsefailure_infer_task"]
        }
      }
      if [job_id] != "" {
        grok {
          # infer job ID from task ID
          match => [ "task_id",    "(?<job_id>[0-9]+_[0-9]+)_[mr]_[0-9]+" ]
          tag_on_failure => ["_grokparsefailure_infer_job"]
        }
      }
      dns {
        reverse => [ "host" ]
        action  => "replace"
      }
    }

    # Drop duplicated json content added by nodejs gelf-stream library
    mutate {
      remove_field => [ "full_message" ]
    }

    prune {
      # get rid of a whole bunch of useless _ fields that are duplicated
      # We must keep _id, _index, and _type
      blacklist_names => [ "^_(?!id$|index$|type$).*" ]
    }
  }
}
