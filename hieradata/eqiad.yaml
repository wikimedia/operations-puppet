#
# mediawiki
#
mediawiki_memcached_servers:
  - '10.64.0.80:11211:1 "shard01"'
  - '10.64.0.81:11211:1 "shard02"'
  - '10.64.0.82:11211:1 "shard03"'
  - '10.64.0.83:11211:1 "shard04"'
  - '10.64.0.84:11211:1 "shard05"'
  - '10.64.16.107:11211:1 "shard06"'
  - '10.64.16.108:11211:1 "shard07"'
  - '10.64.16.109:11211:1 "shard08"'
  - '10.64.16.110:11211:1 "shard09"'
  - '10.64.32.208:11211:1 "shard10"'
  - '10.64.32.209:11211:1 "shard11"'
  - '10.64.32.210:11211:1 "shard12"'
  - '10.64.32.211:11211:1 "shard13"'
  - '10.64.32.212:11211:1 "shard14"'
  - '10.64.48.155:11211:1 "shard15"'
  - '10.64.48.156:11211:1 "shard16"'
  - '10.64.48.157:11211:1 "shard17"'
  - '10.64.48.158:11211:1 "shard18"'

#
# Ganglia
#
ganglia_aggregators: install1002.wikimedia.org:9649

# Eventlogging
eventlogging_host: 10.64.32.167 # eventlog1001

# Kafka Topic eventlogging-client-side with
# raw eventlogging events has 12 partitions
# in production.  Run 12 processors.
eventlogging_client_side_processors:
    - client-side-00
    - client-side-01
    - client-side-02
    - client-side-03
    - client-side-04
    - client-side-05
    - client-side-06
    - client-side-07
    - client-side-08
    - client-side-09
    - client-side-10
    - client-side-11

# Kafka Topic eventlogging-valid-mixed with
# most eventlogging events has 12 partitions
# in production.  Run 4 mysql consumers
# to insert this data into the log database.
eventlogging_mysql_consumers:
    - mysql-m4-master-00
    # Dropping these consumers because custom replication of
    # eventlogging tables does not work with parallel consumers due to
    # race conditions. We'll bring these back after adding autoincrement ids
    # to eventlogging tables, and having replication script check the ids
    # instead of the time based faulty checking for new events that's happening now.
    # - mysql-m4-master-01
    # - mysql-m4-master-02
    # - mysql-m4-master-03


labs_certmanager_hostname: "labservices1001.wikimedia.org"

#
# Labs
#

profile::openstack::main::cumin_auth_group: bastion_restricted_hosts
labs_nova_controller: &labsnovacontroller "labcontrol1001.wikimedia.org"
# _spare is a duplicate/backup controller.  In theory it has the
#  same state as the main controller.
# WARNING:  Base images are rsynced from the primary host to the spare with
#  --delete.  Make sure to back-up or otherwise keep track
#  of your base images before creating a new empty, primary host here or
#  you'll lose your image backups.
labs_nova_controller_spare: &labsnovacontrollerspare "labcontrol1002.wikimedia.org"

labs_glance_controller: &labsglancecontroller "labcontrol1001.wikimedia.org"
labs_puppet_master: &labspuppetmaster "labs-puppetmaster.wikimedia.org"
labs_keystone_host: &labskeystonehost "labcontrol1001.wikimedia.org"

labs_osm_host: "wikitech.wikimedia.org"
labs_horizon_host: "californium.wikimedia.org"
labs_host_ips: '10.64.20.0/24'

# These are the up-and-coming, better dns servers:
labsdnsconfig:
  host: 'labs-ns0.wikimedia.org'
  host_secondary: 'labs-ns1.wikimedia.org'
  dbserver: 'localhost'
  recursor: 'labs-recursor0.wikimedia.org'
  recursor_secondary: 'labs-recursor1.wikimedia.org'

novaconfig:
  db_host: 'm5-master.eqiad.wmnet'
  glance_host: *labsnovacontroller
  rabbit_host: *labsnovacontroller
  cc_host: *labsnovacontroller
  controller_hostname: *labsnovacontroller
  puppet_host: *labsnovacontroller
  puppet_db_host: *labsnovacontroller
  dhcp_domain: 'eqiad.wmflabs'
  live_migration_uri: 'qemu://%s.eqiad.wmnet/system?pkipath=/var/lib/nova'
  zone: eqiad
  spice_hostname: 'labspice.wikimedia.org'
  scheduler_pool:
    - labvirt1001
    - labvirt1002
    - labvirt1003
    - labvirt1004
    - labvirt1005
    - labvirt1006
    - labvirt1007
    - labvirt1008
    - labvirt1009
    - labvirt1010
    - labvirt1011
    - labvirt1012
    - labvirt1013
    - labvirt1014
    - labvirt1016

wikitech_db_name: 'labswiki'

wikitechstatusconfig:
  host: 'wikitech.wikimedia.org'
  page_prefix: 'Nova_Resource:'

keystoneconfig:
  auth_port: '35357'
  public_port: '5000'
  auth_protocol: 'http'
  auth_host: 208.80.154.92
  admin_project_id: 'admin'
  admin_project_name: 'admin'

designateconfig:
  db_host:  'm5-master.eqiad.wmnet'
  db_name: 'designate'
  pool_manager_db_name: 'designate_pool_manager'
  dhcp_domain: 'eqiad'
  pdns_db_name: 'pdns'
  rabbit_host:  *labsnovacontroller
  controller_hostname: *labsnovacontroller
  puppetmaster_hostname: 'labs-puppetmaster.wikimedia.org'
  domain_id_internal_forward: '114f1333-c2c1-44d3-beb4-ebed1a91742b'
  domain_id_internal_reverse: '8d114f3c-815b-466c-bdd4-9b91f704ea60'
  wmflabsdotorg_project: 'wmflabsdotorg'
  private_tld: 'wmflabs'
  floating_ip_ptr_zone: '128-25.155.80.208.in-addr.arpa.'
  floating_ip_ptr_fqdn_matching_regex: '^(\d{1,3})\.155\.80\.208\.in-addr\.arpa\.'
  floating_ip_ptr_fqdn_replacement_pattern: '\1.128-25.155.80.208.in-addr.arpa.'

labs_baremetal_servers:
  - '10.64.20.12'

labsldapconfig:
  hostname: ldap-labs.eqiad.wikimedia.org
  secondary_hostname: ldap-labs.codfw.wikimedia.org

# Configure regular backups of the analytics-meta MySQL instance
# in the Analytics Cluster to back up via rsync to
# analytics1002.  This works because the analytics1002 has the
# role::analyitcs_cluster::database::meta::backup_dest class
# applied to it.
analytics_cluster_meta_database_backup_rsync_dest: analytics1002.eqiad.wmnet::backup/mysql/analytics-meta

prometheus_nodes:
    - prometheus1003.eqiad.wmnet
    - prometheus1004.eqiad.wmnet

# Which of our redundant unified cert vendors to use in each DC
public_tls_unified_cert_vendor: "globalsign"
