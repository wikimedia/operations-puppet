# Hadoop base configuration is common to multiple profiles, and must be kept
# in sync. Instead of having it repated multiple times it is convenient to
# have a single place in hiera to check/modify.

profile::hadoop::common::zookeeper_cluster_name: main-eqiad
profile::hadoop::common::resourcemanager_hosts:
  - analytics1001.eqiad.wmnet
  - analytics1002.eqiad.wmnet

profile::hadoop::common::cluster_name: analytics-hadoop

profile::hadoop::common::namenode_hosts:
    - analytics1001.eqiad.wmnet
    - analytics1002.eqiad.wmnet

profile::hadoop::common::journalnode_hosts:
    - analytics1052.eqiad.wmnet  # Row A3
    - analytics1028.eqiad.wmnet  # Row C2
    - analytics1035.eqiad.wmnet  # Row D2

# analytics* Dell R720s have mounts on disks sdb - sdm.
# (sda is hardware raid on the 2 2.5 drives in the flex bays.)
profile::hadoop::common::datanode_mounts:
    - /var/lib/hadoop/data/b
    - /var/lib/hadoop/data/c
    - /var/lib/hadoop/data/d
    - /var/lib/hadoop/data/e
    - /var/lib/hadoop/data/f
    - /var/lib/hadoop/data/g
    - /var/lib/hadoop/data/h
    - /var/lib/hadoop/data/i
    - /var/lib/hadoop/data/j
    - /var/lib/hadoop/data/k
    - /var/lib/hadoop/data/l
    - /var/lib/hadoop/data/m

# Increase NameNode heapsize independent from other daemons.
# Note that cdh::hadoop::hadoop_namenode_heapsize is NOT a cdh class parameter.
# It exists so that we can configure icinga alerts properly,
# without having to parse the -Xmx value.  These
# two should be set to the same megabyte value.
profile::hadoop::common::namenode_heapsize: 6144
profile::hadoop::common::hadoop_namenode_opts: "-Xms6144m -Xmx6144m"

profile::hadoop::common::mapreduce_reduce_shuffle_parallelcopies: 10
profile::hadoop::common::mapreduce_task_io_sort_mb: 200
profile::hadoop::common::mapreduce_task_io_sort_factor: 10

# Configure memory based on these recommendations and then adjusted:
# http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.6.0/bk_installing_manually_book/content/rpm-chap1-11.html

# These Map/Reduce and YARN ApplicationMaster master settings are
# settable per job.
# All worker nodes are currently Dell R720s with 64G of RAM.

# Choosing 2G for default application container size.
# This is used as the base for the memory settings below.
# It may be possible to increase since all nodes now have
# 64G instead of 48G.
# See: https://phabricator.wikimedia.org/T118501
#

# Map container size and JVM max heap size (-XmX)
profile::hadoop::common::mapreduce_map_memory_mb: 2048
profile::hadoop::common::mapreduce_map_java_opts: '-Xmx1638m'  # 0.8 * 2G

# Reduce container size and JVM max heap size (-Xmx)
profile::hadoop::common::mapreduce_reduce_memory_mb: 4096         # 2 * 2G
profile::hadoop::common::mapreduce_reduce_java_opts: '-Xmx3276m'  # 0.8 * 2 * 2G

# This is the heap zize of YARN daemons ResourceManager and NodeManagers.
# This setting is used to configure the max heap size for both.
# The default is 1000m, we increase it to 2048m in production.
profile::hadoop::common::yarn_heapsize: 2048
profile::hadoop::common::yarn_nodemanager_opts: '-Xms2048m'
profile::hadoop::common::yarn_resourcemanager_opts: '-Xms2048m'

# Heap size for HDFS datanode.
# The default is 1000m, we increase it to 2048m in production.
profile::hadoop::common::hadoop_heapsize: 2048
profile::hadoop::common::hadoop_datanode_opts: '-Xms2048m'

# Yarn ApplicationMaster container size and  max heap size (-Xmx)
profile::hadoop::common::yarn_app_mapreduce_am_resource_mb: 4096          # 2 * 2G
profile::hadoop::common::yarn_app_mapreduce_am_command_opts: '-Xmx3276m'  # 0.8 * 2 * 2G

# MapReduce History server (running on Hadoop master nodes)
profile::hadoop::common::mapreduce_history_java_opts: '-Xms4096m -Xmx4096m'

# Save 8G for OS and other processes.
# Memory available for use by Hadoop jobs:  64G - 8G == 56G.
# 56G at 2G per container gives nodes with 64G RAM space for
# 28 containers.

# This is the total amount of memory that NodeManagers
# will use for allocation to containers.
profile::hadoop::common::yarn_nodemanager_resource_memory_mb: 57344  # 64G - 8G

# Setting _minimum_allocation_mb to 0 to allow Impala to submit small
# reservation requests.
profile::hadoop::common::yarn_scheduler_minimum_allocation_mb: 0
profile::hadoop::common::yarn_scheduler_maximum_allocation_mb: 57344
# Setting minimum_allocation_vcores to 0 to allow Impala to submit small
# reservation requests.
profile::hadoop::common::yarn_scheduler_minimum_allocation_vcores: 0
profile::hadoop::common::yarn_scheduler_maximum_allocation_vcores: 32
