# Hadoop base configuration is common to multiple profiles, and must be kept
# in sync. Instead of having it repated multiple times it is convenient to
# have a single place in hiera to check/modify.

profile::hadoop::common::zookeeper_cluster_name: main-eqiad
profile::hadoop::common::resourcemanager_hosts:
  - analytics1001.eqiad.wmnet
  - analytics1002.eqiad.wmnet

profile::hadoop::common::cluster_name: analytics-hadoop

profile::hadoop::common::namenode_hosts:
    - analytics1001.eqiad.wmnet
    - analytics1002.eqiad.wmnet

profile::hadoop::common::journalnode_hosts:
    - analytics1052.eqiad.wmnet  # Row A3
    - analytics1028.eqiad.wmnet  # Row C2
    - analytics1035.eqiad.wmnet  # Row D2

# analytics* Dell R720s have mounts on disks sdb - sdm.
# (sda is hardware raid on the 2 2.5 drives in the flex bays.)
profile::hadoop::common::datanode_mounts:
    - /var/lib/hadoop/data/b
    - /var/lib/hadoop/data/c
    - /var/lib/hadoop/data/d
    - /var/lib/hadoop/data/e
    - /var/lib/hadoop/data/f
    - /var/lib/hadoop/data/g
    - /var/lib/hadoop/data/h
    - /var/lib/hadoop/data/i
    - /var/lib/hadoop/data/j
    - /var/lib/hadoop/data/k
    - /var/lib/hadoop/data/l
    - /var/lib/hadoop/data/m

profile::hadoop::common::mapreduce_reduce_shuffle_parallelcopies: 10
profile::hadoop::common::mapreduce_task_io_sort_mb: 200
profile::hadoop::common::mapreduce_task_io_sort_factor: 10

# Configure memory based on these recommendations and then adjusted:
# http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.6.0/bk_installing_manually_book/content/rpm-chap1-11.html

# These Map/Reduce and YARN ApplicationMaster master settings are
# settable per job.
# All worker nodes are currently Dell R720s with 64G of RAM.

# Choosing 2G for default application container size.
# This is used as the base for the memory settings below.
# It may be possible to increase since all nodes now have
# 64G instead of 48G.
# See: https://phabricator.wikimedia.org/T118501
#
# Map container size and JVM max heap size (-XmX)
profile::hadoop::common::mapreduce_map_memory_mb: 2048
profile::hadoop::common::mapreduce_map_java_opts: '-Xmx1638m'  # 0.8 * 2G

# Reduce container size and JVM max heap size (-Xmx)
profile::hadoop::common::mapreduce_reduce_memory_mb: 4096         # 2 * 2G
profile::hadoop::common::mapreduce_reduce_java_opts: '-Xmx3276m'  # 0.8 * 2 * 2G

# Yarn ApplicationMaster container size and  max heap size (-Xmx)
profile::hadoop::common::yarn_app_mapreduce_am_resource_mb: 4096          # 2 * 2G
profile::hadoop::common::yarn_app_mapreduce_am_command_opts: '-Xmx3276m'  # 0.8 * 2 * 2G

# Save 12G for OS and other processes.
# Memory available for use by Hadoop jobs:  64G - 12G == 52G.
# 52G at 2G per container gives nodes with 64G RAM space for
# 26 containers.

# This is the total amount of memory that NodeManagers
# will use for allocation to containers.
profile::hadoop::common::yarn_nodemanager_resource_memory_mb: 53248 # 64G - 12G

# Setting _minimum_allocation_mb to 0 to allow Impala to submit small
# reservation requests.
profile::hadoop::common::yarn_scheduler_minimum_allocation_mb: 0
profile::hadoop::common::yarn_scheduler_maximum_allocation_mb: 53248

# Setting minimum_allocation_vcores to 0 to allow Impala to submit small
# reservation requests.
profile::hadoop::common::yarn_scheduler_minimum_allocation_vcores:   0
profile::hadoop::common::yarn_scheduler_maximum_allocation_vcores:   32
