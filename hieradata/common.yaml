## NOTE namespaced keys (i.e. with ::) will NOT be looked up here
## See also https://phabricator.wikimedia.org/T209265
lookup_options:
  "^profile::pki::(.*)::db_pass$":
    convert_to: 'Sensitive'
  '^profile::mail::mx::[\w_]+_password$':
    convert_to: 'Sensitive'
  profile::pki::client::auth_key:
    convert_to: 'Sensitive'
  profile::cache::varnish::frontend::runtime_params:
    merge: deep
  profile::cache::varnish::frontend::fe_vcl_config:
    merge: hash
  profile::docker::engine::settings:
    merge: hash
  profile::debdeploy::client::filter_services:
    merge: hash
  labsldapconfig:
    merge: hash
  profile::statograph::api_key:
    convert_to: 'Sensitive'
  profile::statograph::page_id:
    convert_to: 'Sensitive'
  profile::ceph::auth::load_all::configuration:
    merge: deep
  profile::ceph::auth::deploy::configuration:
    merge: deep
  mediabackup:
    merge: hash

# General variables that once would have been in realm.pp
cluster: misc
# TODO: move all of theses directly under P:monitoring::host
contactgroups: 'admins'
mgmt_contactgroups: 'admins'
do_paging: true
nagios_group: "%{lookup('cluster')}_%{::site}"
datacenters:
  - eqiad
  - codfw
  - esams
  - ulsfo
  - eqsin
  - drmrs

public_domain: 'wikimedia.org'

# kubernetes clusters by group.
kubernetes_cluster_groups:
  main:
    eqiad:
      dc: eqiad
      master: kubemaster.svc.eqiad.wmnet
    codfw:
      dc: codfw
      master: kubemaster.svc.codfw.wmnet
    # active staging cluster.
    staging:
      dc: eqiad
      master: kubestagemaster.svc.eqiad.wmnet
    "staging-eqiad":
      dc: eqiad
      master: kubestagemaster.svc.eqiad.wmnet
      add_private: false
      imagecatalog: true
    "staging-codfw":
      dc: codfw
      master: kubestagemaster.svc.codfw.wmnet
      add_private: false
      imagecatalog: true
  "ml-serve":
    "ml-serve-eqiad":
      dc: eqiad
      master: ml-ctrl.svc.eqiad.wmnet
    "ml-serve-codfw":
      dc: codfw
      master: ml-ctrl.svc.codfw.wmnet
    "ml-staging-codfw":
      dc: codfw
      master: ml-staging-ctrl.svc.codfw.wmnet
  "dse-k8s":
    "dse-k8s-eqiad":
      dc: eqiad
      master: dse-k8s-ctrl.svc.eqiad.wmnet

# Swift cluster metadata
# The keys here are called "cluster_label" in puppet code
# This is an instance of the Swift::Clusters type
swift_clusters:
  codfw:
    cluster_name: 'codfw-prod'
    ring_manager: 'ms-fe2009.codfw.wmnet'
  eqiad:
    cluster_name: 'eqiad-prod'
    ring_manager: 'ms-fe1009.eqiad.wmnet'
  thanos:
    cluster_name: 'thanos-prod'
    ring_manager: 'thanos-fe1001.eqiad.wmnet'

# Main statsd instance
statsd: statsd.eqiad.wmnet:8125
statsd_exporter_port: 9125

# Debmonitor instance
debmonitor: debmonitor.discovery.wmnet

# List of all prometheus nodes (ops instance)
prometheus_all_nodes:
  - prometheus3001.esams.wmnet
  - prometheus4001.ulsfo.wmnet
  - prometheus5001.eqsin.wmnet
  - prometheus6001.drmrs.wmnet
  - prometheus1005.eqiad.wmnet
  - prometheus1006.eqiad.wmnet
  - prometheus2005.codfw.wmnet
  - prometheus2006.codfw.wmnet

alertmanagers:
  - alert1001.wikimedia.org
  - alert2001.wikimedia.org

# NOTE: Do *not* add new clusters *per site* anymore,
# the site name will automatically be appended now,
# and a different IP prefix will be used.
wikimedia_clusters:
  decommissioned:
    description: "Decommissioned servers"
    sites: {}
  lvs:
    description: "LVS loadbalancers"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  puppet:
    description: "Puppetmasters"
    sites:
      eqiad: []
      codfw: []
  search:
    description: "Search"
    sites: {}
  mysql:
    description: "MySQL"
    sites:
      eqiad: []
      codfw: []
  etcd:
    description: "Etcd"
    sites:
      eqiad: []
      codfw: []
  kafka_main:
    description: "Kafka main cluster"
    sites:
      eqiad: []
      codfw: []
  misc:
    description: "Miscellaneous"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  kubernetes:
    description: "Kubernetes"
    sites:
      eqiad: []
      codfw: []
  appserver:
    description: "Application servers"
    sites:
      eqiad: []
      codfw: []
  api_appserver:
    description: "API application servers"
    sites:
      eqiad: []
      codfw: []
  cache_text:
    description: "Text caches"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  cache_upload:
    description: "Upload caches"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  payments:
    description: "Fundraiser payments"
    sites: {}
  ssl:
    description: "SSL cluster"
    sites: {}
  swift:
    description: "Swift"
    sites:
      eqiad: []
      codfw: []
      esams: []
  labvirt:
    description: "Labs virt hosts"
    sites:
      eqiad: []
  labs:
    description: "Labs services"
    sites:
      eqiad: []
  jobrunner:
    description: "Jobrunners"
    sites:
      eqiad: []
      codfw: []
  analytics:
    description: "Analytics cluster"
    sites:
      eqiad: []
  memcached:
    description: "Memcached"
    sites:
      eqiad: []
      codfw: []
  memcached_gutter:
    description: "Memcached gutter pool"
    sites:
      codfw: []
      eqiad: []
  fundraising:
    description: "Fundraising"
    sites:
      eqiad:
        - pay-lvs1001.frack.eqiad.wmnet
        - pay-lvs1002.frack.eqiad.wmnet
  ceph:           # Not used anymore
    description: "Ceph"
    sites: {}
  parsoid:
    description: "Parsoid"
    sites:
      eqiad: []
      codfw: []
  redis:
    description: "Redis"
    sites:
      eqiad: []
      codfw: []
  labsnfs:
    description: "Labs NFS cluster"
    sites:
      eqiad: []
      codfw: []
  elasticsearch:
    description: "Elasticsearch cluster"
    sites:
      eqiad: []
      codfw: []
  logstash:
    description: "Logstash cluster"
    sites:
      eqiad: []
      codfw: []
  apifeatureusage:
    description: "API feature usage iog ingest cluster"
    sites:
      eqiad: []
      codfw: []
  restbase:
    description: "Restbase"
    sites:
      eqiad: []
      codfw: []
  wdqs:
    description: "Wikidata Query Service - Public cluster"
    sites:
      eqiad: []
      codfw: []
  maps:
    description: "Maps Cluster"
    sites:
      eqiad: []
      codfw: []
  ganeti:
    description: "Ganeti Virt cluster"
    sites:
      eqiad: []
      codfw: []
      ulsfo: []
      esams: []
      eqsin: []
      drmrs: []
  aqs:
    description: "Analytics Query Service"
    sites:
      eqiad: []
      codfw: []
  restbase_test:
    description: "Restbase test"
    sites:
      eqiad: []
      codfw: []
  relforge:
    description: "Elasticsearch relforge cluster"
    sites:
      eqiad: []
  labtestvirt:
    description: "Labtest virt hosts"
    sites:
      codfw: []
  labtest:
    description: "Labtest services"
    sites:
      codfw: []
  restbase_dev:
    description: "Services development test"
    sites:
      eqiad: []
  thumbor:
    description: "Thumbor"
    sites:
      codfw: []
      eqiad: []
  kafka_jumbo:
    description: "Kafka Jumbo Cluster"
    sites:
      eqiad: []
  druid_analytics:
    description: "Druid Analytics Cluster"
    sites:
      eqiad: []
  druid_public:
    description: "Druid Public Cluster"
    sites:
      eqiad: []
  ores:
    description: "ORES Cluster"
    sites:
      eqiad: []
      codfw: []
  wdqs-internal:
    description: "Wikidata Query Service - Internal cluster"
    sites:
      eqiad: []
      codfw: []
  wdqs-test:
    description: "Wikidata Query Service - Test cluster"
    sites:
      eqiad: []
  dnsbox:
    description: "DNS and NTP Combo Infra Boxes"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  spare:
    description: "Spare servers"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  prometheus:
    description: "Prometheus servers"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  graphite:
    description: "Graphite servers"
    sites:
      eqiad: []
      codfw: []
  bastion:
    description: "Bastion servers"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  alerting:
    description: "Icinga"
    sites:
      eqiad: []
      codfw: []
  ci:
    description: "Continuous Integration servers"
    sites:
      eqiad: []
      codfw: []
  management:
    description: "Management servers"
    sites:
      eqiad: []
      codfw: []
  wmcs:
    description: "WMCS servers"
    sites:
      eqiad: []
  webperf:
    description: "Web Performance servers"
    sites:
      eqiad: []
      codfw: []
  poolcounter:
    description: "Poolcounter servers"
    sites:
      eqiad: []
      codfw: []
  dumps:
    description: "Dumps servers"
    sites:
      eqiad: []
  syslog:
    description: "Syslog servers"
    sites:
      eqiad: []
      codfw: []
  druid_test_analytics:
    description: "Druid Analytics Test Cluster"
    sites:
      eqiad: []
  sessionstore:
    description: "Sessionstore cluster"
    sites:
      eqiad: []
      codfw: []
  cloudelastic:
    description: "Elasticsearch cloudelastic cluster"
    sites:
      eqiad: []
  acmechief:
    description: "acme-chief hosts"
    sites:
      eqiad: []
      codfw: []
  eventschemas:
    description: "Event platform schemas"
    sites:
      eqiad: []
      codfw: []
  thanos:
    description: "Prometheus long-term storage"
    sites:
      eqiad: []
      codfw: []
  kafka_test:
    description: "Kafka Test Cluster"
    sites:
      eqiad: []
  zookeeper_test:
    description: "Zookeeper Test Cluster"
    sites:
      eqiad: []
  ml_serve:
    description: "ML Team serving clusters"
    sites:
      eqiad: []
      codfw: []
  ml_etcd:
    description: "ML Team etcd clusters"
    sites:
      eqiad: []
      codfw: []
  pki:
    description: "PKI (cfssl) infrastructure"
    sites:
      eqiad: []
      codfw: []
  ganeti_test:
    description: "Ganeti Virt cluster test environment"
    sites:
      codfw: []
  backup:
    description: "Backup cluster (bacula, databases and media)"
    sites:
      eqiad: []
      codfw: []
  wcqs:
    description: "Wikimedia Commons Query Service - Public cluster"
    sites:
      eqiad: []
      codfw: []
  wikidough:
    description: "DoH and DoT recursive resolver"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  durum:
    description: "Wikidough check service"
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
      drmrs: []
  kubernetes-staging:
    description: "Kubernetes staging"
    sites:
      eqiad: []
      codfw: []
  ml_staging_etcd:
    description: "ML Team staging etcd clusters"
    sites:
      codfw: []
  ml_staging:
    description: "ML Team staging clusters"
    sites:
      codfw: []
  ml_cache:
    description: "ML Team cache and Feature Store clusters"
    sites:
      eqiad: []
      codfw: []
  dse_k8s:
    description: "Kubernetes cluster for Data Science and Engineering (DSE) workloads"
    sites:
      eqiad: []
  dse_k8s_etcd:
    description: "Etcd cluster for the DSE Kubernetes cluster"
    sites:
      eqiad: []
  aux_k8s_etcd:
    description: "etcd cluster for aux kubernetes cluster"
    sites:
      eqiad: []
puppetmaster: "puppet"
puppet_ca_server: puppetmaster1001.eqiad.wmnet
puppet_ca_source: puppet:///modules/profile/puppet/ca.production.pem
manage_puppet_ca_file: true

# This list is mostly maintained for rsync hosts_allow.
# Servers listed here can rsync pull from each other.
statistics_servers:
  - stat1004.eqiad.wmnet
  - stat1005.eqiad.wmnet
  - stat1006.eqiad.wmnet
  - stat1007.eqiad.wmnet
  - stat1008.eqiad.wmnet
  - clouddumps1001.wikimedia.org
  - clouddumps1002.wikimedia.org
  - an-launcher1002.eqiad.wmnet
  - an-test-client1001.eqiad.wmnet

dumps_nfs_clients:
  snapshots:
    - snapshot1008.eqiad.wmnet
    - snapshot1009.eqiad.wmnet
    - snapshot1010.eqiad.wmnet
    - snapshot1011.eqiad.wmnet
    - snapshot1012.eqiad.wmnet
    - snapshot1013.eqiad.wmnet
dumps_datadir_mount_type: nfs
dumps_nfs_server: dumpsdata1003.eqiad.wmnet
dumps_cron_nfs_server: dumpsdata1002.eqiad.wmnet
dumps_managed_subdirs: []
dumps_misc_cronrunner: false

# Dumps distribution servers actively serving NFS traffic
dumps_dist_nfs_servers: [clouddumps1001.wikimedia.org, clouddumps1002.wikimedia.org]

# Dumps distribution server currently serving traffic over NFS to cloud vps instances
dumps_dist_active_vps: clouddumps1001.wikimedia.org
# Dumps distribution server currently serving web and rsync mirror traffic
# Also serves stat* hosts over nfs
dumps_dist_active_web: clouddumps1002.wikimedia.org

# List of all zookeeper clusters in production.
zookeeper_clusters:
  main-eqiad:
    hosts:
      conf1007.eqiad.wmnet: '1107'
      conf1008.eqiad.wmnet: '1108'
      conf1009.eqiad.wmnet: '1109'

  main-codfw:
    hosts:
      conf2004.codfw.wmnet: '2001'
      conf2005.codfw.wmnet: '2002'
      conf2006.codfw.wmnet: '2003'

  # ZK cluster for Druid analytics-eqiad cluster (non public),
  # colocated on druid hosts.
  druid-analytics-eqiad:
    hosts:
      an-druid1001.eqiad.wmnet: '1001'
      an-druid1002.eqiad.wmnet: '1002'
      an-druid1003.eqiad.wmnet: '1003'

  # ZK cluster for Druid public-eqiad cluster, (for AQS, wikistats, etc.)
  # colocated on druid hosts.
  druid-public-eqiad:
    hosts:
      druid1004.eqiad.wmnet: '1004'
      druid1005.eqiad.wmnet: '1005'
      druid1006.eqiad.wmnet: '1006'

  # ZK cluster for Druid analytics-test-eqiad cluster (non public),
  # colocated on druid hosts. This is mainly used as testing
  # environment for the Hadoop Test cluster jobs.
  # Temporarily removed due to hw refresh of the hadoop test cluster.
  druid-analytics-test-eqiad:
    hosts:
      an-test-druid1001.eqiad.wmnet: '1001'

  # ZK Cluster dedicated to the Hadoop cluster (and its satellite systems)
  analytics-eqiad:
    hosts:
      an-conf1001.eqiad.wmnet: '1001'
      an-conf1002.eqiad.wmnet: '1002'
      an-conf1003.eqiad.wmnet: '1003'

  # Test ZK cluster
  test-eqiad:
    hosts:
      zookeeper-test1002.eqiad.wmnet: '1002'

# Used to sync the setting between all Kafka clusters and clients.
kafka_message_max_bytes: 4194304

kafka_clusters:

  main-eqiad:
    zookeeper_cluster_name: main-eqiad
    brokers:
      kafka-main1001.eqiad.wmnet:
        id: 1001
        rack: A
      kafka-main1002.eqiad.wmnet:
        id: 1002
        rack: B
      kafka-main1003.eqiad.wmnet:
        id: 1003
        rack: C
      kafka-main1004.eqiad.wmnet:
        id: 1004
        rack: D
      kafka-main1005.eqiad.wmnet:
        id: 1005
        rack: D

  main-codfw:
    zookeeper_cluster_name: main-codfw
    brokers:
      kafka-main2001.codfw.wmnet:
        id: 2001
        rack: A
      kafka-main2002.codfw.wmnet:
        id: 2002
        rack: B
      kafka-main2003.codfw.wmnet:
        id: 2003
        rack: C
      kafka-main2004.codfw.wmnet:
        id: 2004
        rack: D
      kafka-main2005.codfw.wmnet:
        id: 2005
        rack: D

  # NOTE:  The 'rack' here is used by the confluent kafka module
  # to assign broker.rack for Kafka rack awareness.  We are actually setting
  # the row letter, not the full row-rack number, since each of these brokers
  # are in different racks anyway.  We do awareness at the row level.
  jumbo-eqiad:
    zookeeper_cluster_name: main-eqiad
    brokers:
      kafka-jumbo1001.eqiad.wmnet:
        id: 1001
        rack: A
      kafka-jumbo1002.eqiad.wmnet:
        id: 1002
        rack: A
      kafka-jumbo1003.eqiad.wmnet:
        id: 1003
        rack: B
      kafka-jumbo1004.eqiad.wmnet:
        id: 1004
        rack: C
      kafka-jumbo1005.eqiad.wmnet:
        id: 1005
        rack: C
      kafka-jumbo1006.eqiad.wmnet:
        id: 1006
        rack: D
      kafka-jumbo1007.eqiad.wmnet:
        id: 1007
        rack: C
      kafka-jumbo1008.eqiad.wmnet:
        id: 1008
        rack: D
      kafka-jumbo1009.eqiad.wmnet:
        id: 1009
        rack: D

  # Kafka clusters for logs, see also T206454
  logging-eqiad:
    zookeeper_cluster_name: main-eqiad
    brokers:
      kafka-logging1001.eqiad.wmnet:
        id: 1004
        rack: B
      kafka-logging1002.eqiad.wmnet:
        id: 1005
        rack: C
      kafka-logging1003.eqiad.wmnet:
        id: 1006
        rack: D
    # The logging infra doesn't support ipv6 for now.
    # see https://phabricator.wikimedia.org/T279342#7002887
    ipv6: false

  logging-codfw:
    zookeeper_cluster_name: main-codfw
    brokers:
      kafka-logging2001.codfw.wmnet:
        id: 2001
        rack: A
      kafka-logging2002.codfw.wmnet:
        id: 2002
        rack: C
      kafka-logging2003.codfw.wmnet:
        id: 2003
        rack: D
    # The logging infra doesn't support ipv6 for now.
    # see https://phabricator.wikimedia.org/T279342#7002887
    ipv6: false

  test-eqiad:
    zookeeper_cluster_name: test-eqiad
    brokers:
      kafka-test1006.eqiad.wmnet:
        id: 1006
      kafka-test1007.eqiad.wmnet:
        id: 1007
      kafka-test1008.eqiad.wmnet:
        id: 1008
      kafka-test1009.eqiad.wmnet:
        id: 1009
      kafka-test1010.eqiad.wmnet:
        id: 1010

# Oozie base configuration is common to multiple profiles, and must be kept
# in sync. Instead of having it repated multiple times it is convenient to
# have a single place in hiera to check/modify.
oozie_services:
  analytics-test-oozie:
    oozie_host: an-test-coord1001.eqiad.wmnet

  analytics-oozie:
    oozie_host: an-coord1001.eqiad.wmnet

# Hive base configuration is common to multiple profiles, and must be kept
# in sync. Instead of having it repated multiple times it is convenient to
# have a single place in hiera to check/modify.
hive_services:
  analytics-test-hive:
    server_host: 'analytics-test-hive.eqiad.wmnet'
    server_port: 10000
    # Please note that this value is overridden by the coordinators to force
    # them to use their local Metastore. Check the coordinators' role for more info.
    metastore_host: 'analytics-test-hive.eqiad.wmnet'
    metastore_jdbc_host: 'an-test-coord1001.eqiad.wmnet'
    metastore_sasl_enabled: true
    metastore_kerberos_principal: 'hive/analytics-test-hive.eqiad.wmnet@WIKIMEDIA'
    server_authentication: 'KERBEROS'
    metastore_kerberos_keytab_file: '/etc/security/keytabs/hive/hive.keytab'
    server_authentication_kerberos_principal: 'hive/analytics-test-hive.eqiad.wmnet@WIKIMEDIA'
    server_authentication_kerberos_keytab: '/etc/security/keytabs/hive/hive.keytab'
    hive_metastore_disallow_incompatible_col_type_changes: false
    java_home: '/usr/lib/jvm/java-8-openjdk-amd64/jre'
    metastore_opts: '-Xms4g -Xmx4g -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=1000 -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:9183:/etc/prometheus/hive_metastore_jmx_exporter.yaml'
    server_opts: '-Xms6g -Xmx6g -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=1000 -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:10100:/etc/prometheus/hive_server_jmx_exporter.yaml'

  analytics-hive:
    server_host: analytics-hive.eqiad.wmnet
    server_port: 10000
    # Please note that this value is overridden by the coordinators to force
    # them to use their local Metastore. Check the coordinators' role for more info.
    metastore_host: 'analytics-hive.eqiad.wmnet'
    metastore_jdbc_host: 'an-coord1001.eqiad.wmnet'
    metastore_sasl_enabled: true
    metastore_kerberos_keytab_file: '/etc/security/keytabs/hive/hive.keytab'
    metastore_kerberos_principal: 'hive/analytics-hive.eqiad.wmnet@WIKIMEDIA'
    server_authentication: 'KERBEROS'
    server_authentication_kerberos_principal: 'hive/analytics-hive.eqiad.wmnet@WIKIMEDIA'
    server_authentication_kerberos_keytab: '/etc/security/keytabs/hive/hive.keytab'
    hive_metastore_disallow_incompatible_col_type_changes: false
    java_home: '/usr/lib/jvm/java-8-openjdk-amd64/jre'
    metastore_opts: '-Xms4g -Xmx4g -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=1000 -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:9183:/etc/prometheus/hive_metastore_jmx_exporter.yaml'
    server_opts: '-Xms10g -Xmx10g -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=1000 -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:10100:/etc/prometheus/hive_server_jmx_exporter.yaml'



# Hadoop base configuration is common to multiple profiles, and must be kept
# in sync. Instead of having it repated multiple times it is convenient to
# have a single place in hiera to check/modify.
hadoop_clusters:
  analytics-test-hadoop:
    zookeeper_cluster_name: analytics-eqiad
    resourcemanager_hosts:
      - an-test-master1001.eqiad.wmnet
      - an-test-master1002.eqiad.wmnet
    namenode_hosts:
      - an-test-master1001.eqiad.wmnet
      - an-test-master1002.eqiad.wmnet
    journalnode_hosts:
      - an-test-worker1001.eqiad.wmnet
      - an-test-worker1002.eqiad.wmnet
      - an-test-worker1003.eqiad.wmnet
    net_topology:
      an-test-worker1001.eqiad.wmnet: /eqiad/A/5
      an-test-worker1002.eqiad.wmnet: /eqiad/C/5
      an-test-worker1003.eqiad.wmnet: /eqiad/D/6

    # https://community.hortonworks.com/articles/43839/scaling-the-hdfs-namenode-part-2.html
    # 20 * log2(Cluster Size)
    dfs_namenode_handler_count: 20
    dfs_namenode_service_port: 8040
    dfs_namenode_service_handler_count: 10
    hadoop_var_directory: '/srv/hadoop'

    yarn_scheduler_maximum_allocation_mb: 53248
    yarn_resourcemanager_zk_state_store_parent_path: '/yarn-rmstore/analytics-test-hadoop'
    yarn_resourcemanager_max_completed_applications: 1000
    # yarn_nodemanager_resource_memory_mb is set using the formula:
    # total-memory - yarn_nodemanager_os_reserved_memory_mb
    yarn_nodemanager_os_reserved_memory_mb: 12000

    # Requires the Capacity scheduler to work
    yarn_node_labels_enabled: true

    hadoop_datanode_opts: "-Xms4096m -Xmx4096m -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:51010:/etc/prometheus/hdfs_datanode_jmx_exporter.yaml"
    hadoop_journalnode_opts: "-Xms4096m -Xmx4096m -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:10485:/etc/prometheus/hdfs_journalnode_jmx_exporter.yaml"
    yarn_nodemanager_opts: "-Xms4096m -Xmx4096m -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:8141:/etc/prometheus/yarn_nodemanager_jmx_exporter.yaml"
    hadoop_namenode_opts: "-Xms12288m -Xmx12288m  -XX:+UseG1GC -XX:MaxGCPauseMillis=1000 -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:10080:/etc/prometheus/hdfs_namenode_jmx_exporter.yaml"
    yarn_resourcemanager_opts: "-Xms4096m -Xmx4096m -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:10083:/etc/prometheus/yarn_resourcemanager_jmx_exporter.yaml"
    mapreduce_history_java_opts: "-Xms4096m -Xmx4096m -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:10086:/etc/prometheus/mapreduce_history_jmx_exporter.yaml"

    core_site_extra_properties:
      # User used in the Yarn UI to check job logs/statuses/etc..
      hadoop.http.staticuser.user: 'yarn'

    yarn_site_extra_properties:
      yarn.resourcemanager.scheduler.class: 'org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler'
      yarn.resourcemanager.scheduler.monitor.enable: true
      # Note: the extra space at the beginning is needed.
      # If you enable ACLs the admin list is "everybody" by default, so the rules are not
      # really respected if we don't set a more specific group.
      # The 'hadoop.http.staticuser.user' needs to be among the admins to allow various UIs
      # to see Yarn logs.
      yarn.admin.acl: 'yarn analytics-admins'
      yarn.acl.enable: true

    hdfs_site_extra_properties:
      # Avoid long waits if the Namenodes are not reachable
      # from clients.
      dfs.client.failover.max.attempts: 3
      dfs.namenode.acls.enabled: true
      # Since we have less space available on the test cluster (due to fewer workers),
      # better to use a smaller replication factor (3 is the Hadoop's default).
      dfs.replication: 2

  analytics-hadoop:
    zookeeper_cluster_name: analytics-eqiad
    resourcemanager_hosts:
      - an-master1001.eqiad.wmnet
      - an-master1002.eqiad.wmnet
    namenode_hosts:
      - an-master1001.eqiad.wmnet
      - an-master1002.eqiad.wmnet
    journalnode_hosts:
      - an-worker1080.eqiad.wmnet  # Row A4
      - an-worker1078.eqiad.wmnet  # Row A2
      - analytics1072.eqiad.wmnet  # ROW B2
      - an-worker1090.eqiad.wmnet  # Row C4
      - analytics1069.eqiad.wmnet  # Row D8
    net_topology:
      an-master1001.eqiad.wmnet:  /eqiad/A/5
      an-master1002.eqiad.wmnet:  /eqiad/B/8

      analytics1058.eqiad.wmnet:  /eqiad/A/1
      an-worker1078.eqiad.wmnet:  /eqiad/A/2
      an-worker1079.eqiad.wmnet:  /eqiad/A/2
      an-worker1096.eqiad.wmnet:  /eqiad/A/2
      an-worker1118.eqiad.wmnet:  /eqiad/A/2
      an-worker1119.eqiad.wmnet:  /eqiad/A/2
      an-worker1129.eqiad.wmnet:  /eqiad/A/2
      analytics1059.eqiad.wmnet:  /eqiad/A/3
      analytics1060.eqiad.wmnet:  /eqiad/A/3
      an-worker1080.eqiad.wmnet:  /eqiad/A/4
      an-worker1102.eqiad.wmnet:  /eqiad/A/4
      an-worker1120.eqiad.wmnet:  /eqiad/A/4
      an-worker1121.eqiad.wmnet:  /eqiad/A/4
      an-worker1140.eqiad.wmnet:  /eqiad/A/4
      an-worker1141.eqiad.wmnet:  /eqiad/A/4
      analytics1070.eqiad.wmnet:  /eqiad/A/5
      analytics1071.eqiad.wmnet:  /eqiad/A/5
      an-worker1081.eqiad.wmnet:  /eqiad/A/7
      an-worker1082.eqiad.wmnet:  /eqiad/A/7
      an-worker1103.eqiad.wmnet:  /eqiad/A/7
      an-worker1122.eqiad.wmnet:  /eqiad/A/7
      an-worker1123.eqiad.wmnet:  /eqiad/A/7
      an-worker1139.eqiad.wmnet:  /eqiad/A/7

      an-worker1083.eqiad.wmnet:  /eqiad/B/2
      an-worker1084.eqiad.wmnet:  /eqiad/B/2
      an-worker1124.eqiad.wmnet:  /eqiad/B/2
      an-worker1125.eqiad.wmnet:  /eqiad/B/2
      an-worker1126.eqiad.wmnet:  /eqiad/B/2
      analytics1072.eqiad.wmnet:  /eqiad/B/3
      an-worker1085.eqiad.wmnet:  /eqiad/B/4
      an-worker1097.eqiad.wmnet:  /eqiad/B/4
      an-worker1117.eqiad.wmnet:  /eqiad/B/4
      an-worker1127.eqiad.wmnet:  /eqiad/B/4
      an-worker1128.eqiad.wmnet:  /eqiad/B/4
      analytics1073.eqiad.wmnet:  /eqiad/B/7
      an-worker1086.eqiad.wmnet:  /eqiad/B/7
      an-worker1087.eqiad.wmnet:  /eqiad/B/7
      an-worker1098.eqiad.wmnet:  /eqiad/B/7
      an-worker1130.eqiad.wmnet:  /eqiad/B/7
      analytics1061.eqiad.wmnet:  /eqiad/B/8
      analytics1062.eqiad.wmnet:  /eqiad/B/8
      analytics1063.eqiad.wmnet:  /eqiad/B/8

      an-worker1088.eqiad.wmnet:  /eqiad/C/2
      an-worker1099.eqiad.wmnet:  /eqiad/C/2
      an-worker1104.eqiad.wmnet:  /eqiad/C/2
      an-worker1131.eqiad.wmnet:  /eqiad/C/2
      an-worker1132.eqiad.wmnet:  /eqiad/C/2
      analytics1064.eqiad.wmnet:  /eqiad/C/3
      analytics1065.eqiad.wmnet:  /eqiad/C/3
      analytics1066.eqiad.wmnet:  /eqiad/C/3
      analytics1074.eqiad.wmnet:  /eqiad/C/3
      an-worker1089.eqiad.wmnet:  /eqiad/C/4
      an-worker1090.eqiad.wmnet:  /eqiad/C/4
      an-worker1100.eqiad.wmnet:  /eqiad/C/4
      an-worker1105.eqiad.wmnet:  /eqiad/C/4
      an-worker1106.eqiad.wmnet:  /eqiad/C/4
      an-worker1107.eqiad.wmnet:  /eqiad/C/4
      an-worker1108.eqiad.wmnet:  /eqiad/C/4
      analytics1075.eqiad.wmnet:  /eqiad/C/7
      an-worker1091.eqiad.wmnet:  /eqiad/C/7
      an-worker1109.eqiad.wmnet:  /eqiad/C/7
      an-worker1110.eqiad.wmnet:  /eqiad/C/7
      an-worker1133.eqiad.wmnet:  /eqiad/C/7
      an-worker1111.eqiad.wmnet:  /eqiad/C/8

      analytics1067.eqiad.wmnet:  /eqiad/D/2
      analytics1068.eqiad.wmnet:  /eqiad/D/2
      analytics1076.eqiad.wmnet:  /eqiad/D/2
      an-worker1092.eqiad.wmnet:  /eqiad/D/2
      an-worker1093.eqiad.wmnet:  /eqiad/D/2
      an-worker1112.eqiad.wmnet:  /eqiad/D/2
      an-worker1134.eqiad.wmnet:  /eqiad/D/4
      an-worker1135.eqiad.wmnet:  /eqiad/D/4
      an-worker1136.eqiad.wmnet:  /eqiad/D/4
      an-worker1137.eqiad.wmnet:  /eqiad/D/4
      an-worker1138.eqiad.wmnet:  /eqiad/D/4
      an-worker1113.eqiad.wmnet:  /eqiad/D/5
      an-worker1114.eqiad.wmnet:  /eqiad/D/5
      analytics1077.eqiad.wmnet:  /eqiad/D/7
      an-worker1094.eqiad.wmnet:  /eqiad/D/7
      an-worker1095.eqiad.wmnet:  /eqiad/D/7
      an-worker1101.eqiad.wmnet:  /eqiad/D/7
      an-worker1115.eqiad.wmnet:  /eqiad/D/7
      an-worker1116.eqiad.wmnet:  /eqiad/D/7
      analytics1069.eqiad.wmnet:  /eqiad/D/8

      an-worker1142.eqiad.wmnet:  /eqiad/E/1
      an-worker1143.eqiad.wmnet:  /eqiad/E/2
      an-worker1144.eqiad.wmnet:  /eqiad/F/1
      an-worker1145.eqiad.wmnet:  /eqiad/F/2
      an-worker1146.eqiad.wmnet:  /eqiad/F/3
      an-worker1147.eqiad.wmnet:  /eqiad/E/1
      an-worker1148.eqiad.wmnet:  /eqiad/F/1

    core_site_extra_properties:
      # User used in the Yarn UI to check job logs/statuses/etc..
      hadoop.http.staticuser.user: 'yarn'

    # Requires the Capacity scheduler to work
    yarn_node_labels_enabled: true

    yarn_site_extra_properties:
      yarn.resourcemanager.scheduler.class: 'org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler'
      yarn.resourcemanager.scheduler.monitor.enable: true
      # Note: the extra space at the beginning is needed.
      # If you enable ACLs the admin list is "everybody" by default, so the rules are not
      # really respected if we don't set a more specific group.
      # The 'hadoop.http.staticuser.user' needs to be among the admins to allow various UIs
      # to see Yarn logs.
      yarn.admin.acl: 'yarn analytics-admins'
      yarn.acl.enable: true

    # The datanode daemon by default begins the shutdown procedure as soon as
    # on volume/disk failure is registered. In our use case we want to keep the
    # datanode working in case of one/two (two is very unlikey on the same host)
    # disk failures.
    datanode_volumes_failed_tolerated: 2

    # https://community.hortonworks.com/articles/43839/scaling-the-hdfs-namenode-part-2.html
    # 20 * log2(Cluster Size)
    dfs_namenode_handler_count: 127

    # We have experienced some issues with hdfs saveNamespace with a number of handler
    # threads lower than the number of total datanodes running in the cluster.
    # More specifically, during saveNamespace it seemed as if all service handler
    # threads got blocked on a read lock, apparently held by the thread saving the fsimage
    # file. The idea is to keep the number of threads a little more than the number
    # of datanodes, to allow room for other threads responding to the ZKFC health probes.
    # More info: T283733
    dfs_namenode_service_port: 8040
    dfs_namenode_service_handler_count: 100

    # Allow a job to request up to the smallest value of yarn_nodemanager_resource_memory_mb
    # in the cluster. The smallest value is 52G on the R720s (analytics1069 and below).
    yarn_scheduler_maximum_allocation_mb: 49152
    yarn_resourcemanager_zk_state_store_parent_path: '/yarn-rmstore/analytics-hadoop'

    # yarn_nodemanager_resource_memory_mb is set using the formula:
    # total-memory - yarn_nodemanager_os_reserved_memory_mb
    # Used memory is: datanode + nodemanager + journalnode on some
    #                   8G     +     8G      +        4G
    yarn_nodemanager_os_reserved_memory_mb: 20000

    hadoop_datanode_opts: "-Xms8192m -Xmx8192m -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:51010:/etc/prometheus/hdfs_datanode_jmx_exporter.yaml"
    hadoop_journalnode_opts: "-Xms4096m -Xmx4096m -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:10485:/etc/prometheus/hdfs_journalnode_jmx_exporter.yaml"
    yarn_nodemanager_opts: "-Xms8192m -Xmx8192m -Djava.net.preferIPv4Stack=false -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=1000 -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:8141:/etc/prometheus/yarn_nodemanager_jmx_exporter.yaml"
    # Following https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.3/bk_command-line-installation/content/configuring-namenode-heap-size.html
    # and https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.2/bk_hdfs-administration/content/ch_g1gc_garbage_collector_tech_preview.html
    # Also see T310293 for reasons behind increases to the heap size
    hadoop_namenode_opts: "-Xms86016m -Xmx86016m -Djava.net.preferIPv4Stack=false -XX:+UseG1GC -XX:MaxGCPauseMillis=4000 -XX:ParallelGCThreads=20 -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:10080:/etc/prometheus/hdfs_namenode_jmx_exporter.yaml"
    yarn_resourcemanager_opts: "-Xms4096m -Xmx4096m -Djava.net.preferIPv4Stack=false -javaagent:/usr/share/java/prometheus/jmx_prometheus_javaagent.jar=[::]:10083:/etc/prometheus/yarn_resourcemanager_jmx_exporter.yaml"

deployment_server: deploy1002.eqiad.wmnet

aptrepo_hostname: apt.wikimedia.org
aptrepo_server: apt1001.wikimedia.org
aptrepo_servers_failover:
    - 'apt2001.wikimedia.org'

netmon_server: netmon1003.wikimedia.org
netmon_servers_failover:
    - 'netmon2001.wikimedia.org'
    - 'netmon1002.wikimedia.org'

releases_server: releases1002.eqiad.wmnet
releases_servers_failover:
    - 'releases2002.codfw.wmnet'

# Defines which Phabricator server is the active
# one to open needed firewall holes and decide where
# dumps are created.
# Note there is an additional list of all Phabricator
# servers in the role specific Hiera.
phabricator_server: phab1001.eqiad.wmnet

kerberos_realm_name: WIKIMEDIA
kerberos_kadmin_server_primary: 'krb1001.eqiad.wmnet'
kerberos_kadmin_keytabs_repo: 'puppetmaster1001.eqiad.wmnet'
kerberos_kdc_servers:
    - 'krb1001.eqiad.wmnet'
    - 'krb2001.codfw.wmnet'

# Etcd client global configuration
etcd_client_srv_domain: "conftool.%{::site}.wmnet"
etcd_host: ~
etcd_port: ~

# Conftool global prefix (will be per-dc)
conftool_prefix: "/conftool/v1"


# Logging: logstash, udp2log
logstash_host: "logstash.svc.eqiad.wmnet"
logstash_syslog_port: 10514
logstash_gelf_port: 12201
# TCP json_lines input
logstash_json_lines_port: 11514
# UDP logback/json input
logstash_logback_port: 11514
udp2log_aggregator: "udplog:8420"

tcpircbot_host: 'icinga.wikimedia.org'
tcpircbot_port: 9200

# User for jenkins master-slave connections
jenkins_agent_username: 'jenkins-slave'

# HTTP proxy.
# Provide these as seperate host and ports.
http_proxy_host: webproxy.%{::site}.wmnet
http_proxy_port: 8080
# And as a url (can be used as an env variable).
http_proxy: "http://%{lookup('http_proxy_host')}:%{lookup('http_proxy_port')}"

# This is the "live" authdns server set, which feeds into any other tooling
# that needs to operate on them (including themselves).  It includes redundant
# IP address information so that authdns tooling can operate in the face of
# recdns issues.
authdns_servers:
  'authdns1001.wikimedia.org': 208.80.154.134
  'authdns2001.wikimedia.org': 208.80.153.17
  'dns1001.wikimedia.org': 208.80.154.10
  'dns1002.wikimedia.org': 208.80.155.108
  'dns2001.wikimedia.org': 208.80.153.77
  'dns2002.wikimedia.org': 208.80.153.111
  'dns3001.wikimedia.org': 91.198.174.61
  'dns3002.wikimedia.org': 91.198.174.62
  'dns4003.wikimedia.org': 198.35.26.7
  'dns4004.wikimedia.org': 198.35.26.8
  'dns5001.wikimedia.org': 103.102.166.8
  'dns5002.wikimedia.org': 103.102.166.9
  'dns6001.wikimedia.org': 185.15.58.5
  'dns6002.wikimedia.org': 185.15.58.37

# .. and this is the set of public IPs they provide service on, which is
# consumed by both authdns servers and the icinga monitoring of them.
# XXX - probably a better way to do this without common.yaml, and some
# restructuring to do in general, all of which may fall out when monitoring is
# redone anyways...
authdns_addrs:
  ns0-v4:
    address: '208.80.154.238'
  ns1-v4:
    address: '208.80.153.231'
  ns2-v4:
    address: '91.198.174.239'
  nsa-v4:
    address: '198.35.27.27'
    skip_loopback: true # bird::anycast takes care of this one

# acme-chief active host
acmechief_host: 'acmechief1001.eqiad.wmnet'

mail_smarthost:
- 'mx1001.wikimedia.org'
- 'mx2001.wikimedia.org'

wikimail_smarthost:
- 'wiki-mail-eqiad.wikimedia.org'
- 'wiki-mail-codfw.wikimedia.org'
# These are our servers - they all peer to each other and sync to upstream NTP
# pool servers.
ntp_peers:
    eqiad:
    - 'dns1001.wikimedia.org'
    - 'dns1002.wikimedia.org'
    codfw:
    - 'dns2001.wikimedia.org'
    - 'dns2002.wikimedia.org'
    esams:
    - 'dns3001.wikimedia.org'
    - 'dns3002.wikimedia.org'
    ulsfo:
    - 'dns4003.wikimedia.org'
    - 'dns4004.wikimedia.org'
    eqsin:
    - 'dns5001.wikimedia.org'
    - 'dns5002.wikimedia.org'
    drmrs:
    - 'dns6001.wikimedia.org'
    - 'dns6002.wikimedia.org'

# Url to use for reaching graphite

graphite_host: 'graphite-in.eqiad.wmnet'
graphite_url: "http://%{lookup('graphite_host')}"

cumin_masters:
- 10.64.32.25                 # cumin1001.eqiad.wmnet
- 2620:0:861:103:10:64:32:25  # cumin1001.eqiad.wmnet
- 10.192.32.49                # cumin2002.codfw.wmnet
- 2620:0:860:103:10:192:32:49 # cumin2002.codfw.wmnet

unpriv_cumin_masters:
- 10.64.48.57                 # cuminunpriv1001.eqiad.wmnet
- 2620:0:861:107:10:64:48:57  # cuminunpriv1001.eqiad.wmnet

maintenance_hosts:
- 10.64.16.77                 # mwmaint1002.eqiad.wmnet
- 2620:0:861:102:10:64:16:77  # mwmaint1002.eqiad.wmnet
- 10.192.32.34                # mwmaint2002.codfw.wmnet
- 2620:0:860:103:10:192:32:34 # mwmaint2002.codfw.wmnet

bastion_hosts:
- 208.80.155.110                # bast1003.wikimedia.org
- 2620:0:861:4:208:80:155:110   # bast1003.wikimedia.org
- 208.80.153.54                 # bast2002.wikimedia.org
- 2620:0:860:2:208:80:153:54    # bast2002.wikimedia.org
- 91.198.174.6                  # bast3005.wikimedia.org
- 2620:0:862:1:91:198:174:6     # bast3005.wikimedia.org
- 198.35.26.13                  # bast4003.wikimedia.org
- 2620:0:863:1:198:35:26:13     # bast4003.wikimedia.org
- 103.102.166.6                 # bast5002.wikimedia.org
- 2001:df2:e500:1:103:102:166:6 # bast5002.wikimedia.org
- 185.15.58.6                   # bast6001.wikimedia.org
- 2a02:ec80:600:1:185:15:58:6   # bast6001.wikimedia.org

cache_hosts:
- 10.64.0.130                    # cp1075.eqiad.wmnet
- 2620:0:861:101:10:64:0:130     # cp1075.eqiad.wmnet
- 10.64.0.131                    # cp1076.eqiad.wmnet
- 2620:0:861:101:10:64:0:131     # cp1076.eqiad.wmnet
- 10.64.0.132                    # cp1077.eqiad.wmnet
- 2620:0:861:101:10:64:0:132     # cp1077.eqiad.wmnet
- 10.64.0.133                    # cp1078.eqiad.wmnet
- 2620:0:861:101:10:64:0:133     # cp1078.eqiad.wmnet
- 10.64.16.22                    # cp1079.eqiad.wmnet
- 2620:0:861:102:10:64:16:22     # cp1079.eqiad.wmnet
- 10.64.16.23                    # cp1080.eqiad.wmnet
- 2620:0:861:102:10:64:16:23     # cp1080.eqiad.wmnet
- 10.64.16.24                    # cp1081.eqiad.wmnet
- 2620:0:861:102:10:64:16:24     # cp1081.eqiad.wmnet
- 10.64.16.25                    # cp1082.eqiad.wmnet
- 2620:0:861:102:10:64:16:25     # cp1082.eqiad.wmnet
- 10.64.32.67                    # cp1083.eqiad.wmnet
- 2620:0:861:103:10:64:32:67     # cp1083.eqiad.wmnet
- 10.64.32.68                    # cp1084.eqiad.wmnet
- 2620:0:861:103:10:64:32:68     # cp1084.eqiad.wmnet
- 10.64.32.69                    # cp1085.eqiad.wmnet
- 2620:0:861:103:10:64:32:69     # cp1085.eqiad.wmnet
- 10.64.32.70                    # cp1086.eqiad.wmnet
- 2620:0:861:103:10:64:32:70     # cp1086.eqiad.wmnet
- 10.64.48.101                   # cp1087.eqiad.wmnet
- 2620:0:861:107:10:64:48:101    # cp1087.eqiad.wmnet
- 10.64.48.102                   # cp1088.eqiad.wmnet
- 2620:0:861:107:10:64:48:102    # cp1088.eqiad.wmnet
- 10.64.48.103                   # cp1089.eqiad.wmnet
- 2620:0:861:107:10:64:48:103    # cp1089.eqiad.wmnet
- 10.64.48.104                   # cp1090.eqiad.wmnet
- 2620:0:861:107:10:64:48:104    # cp1090.eqiad.wmnet
- 10.192.0.23                    # cp2027.codfw.wmnet
- 2620:0:860:101:10:192:0:23     # cp2027.codfw.wmnet
- 10.192.0.24                    # cp2028.codfw.wmnet
- 2620:0:860:101:10:192:0:24     # cp2028.codfw.wmnet
- 10.192.0.30                    # cp2029.codfw.wmnet
- 2620:0:860:101:10:192:0:30     # cp2029.codfw.wmnet
- 10.192.0.32                    # cp2030.codfw.wmnet
- 2620:0:860:101:10:192:0:32     # cp2030.codfw.wmnet
- 10.192.16.32                   # cp2031.codfw.wmnet
- 2620:0:860:102:10:192:16:32    # cp2031.codfw.wmnet
- 10.192.16.33                   # cp2032.codfw.wmnet
- 2620:0:860:102:10:192:16:33    # cp2032.codfw.wmnet
- 10.192.16.182                  # cp2033.codfw.wmnet
- 2620:0:860:102:10:192:16:182   # cp2033.codfw.wmnet
- 10.192.16.184                  # cp2034.codfw.wmnet
- 2620:0:860:102:10:192:16:184   # cp2034.codfw.wmnet
- 10.192.32.18                   # cp2035.codfw.wmnet
- 2620:0:860:103:10:192:32:18    # cp2035.codfw.wmnet
- 10.192.32.19                   # cp2036.codfw.wmnet
- 2620:0:860:103:10:192:32:19    # cp2036.codfw.wmnet
- 10.192.32.103                  # cp2037.codfw.wmnet
- 2620:0:860:103:10:192:32:103   # cp2037.codfw.wmnet
- 10.192.32.104                  # cp2038.codfw.wmnet
- 2620:0:860:103:10:192:32:104   # cp2038.codfw.wmnet
- 10.192.48.154                  # cp2039.codfw.wmnet
- 2620:0:860:104:10:192:48:154   # cp2039.codfw.wmnet
- 10.192.48.155                  # cp2040.codfw.wmnet
- 2620:0:860:104:10:192:48:155   # cp2040.codfw.wmnet
- 10.192.48.156                  # cp2041.codfw.wmnet
- 2620:0:860:104:10:192:48:156   # cp2041.codfw.wmnet
- 10.192.48.157                  # cp2042.codfw.wmnet
- 2620:0:860:104:10:192:48:157   # cp2042.codfw.wmnet
- 10.20.0.50                     # cp3050.esams.wmnet
- 2620:0:862:102:10:20:0:50      # cp3050.esams.wmnet
- 10.20.0.51                     # cp3051.esams.wmnet
- 2620:0:862:102:10:20:0:51      # cp3051.esams.wmnet
- 10.20.0.52                     # cp3052.esams.wmnet
- 2620:0:862:102:10:20:0:52      # cp3052.esams.wmnet
- 10.20.0.53                     # cp3053.esams.wmnet
- 2620:0:862:102:10:20:0:53      # cp3053.esams.wmnet
- 10.20.0.54                     # cp3054.esams.wmnet
- 2620:0:862:102:10:20:0:54      # cp3054.esams.wmnet
- 10.20.0.55                     # cp3055.esams.wmnet
- 2620:0:862:102:10:20:0:55      # cp3055.esams.wmnet
- 10.20.0.56                     # cp3056.esams.wmnet
- 2620:0:862:102:10:20:0:56      # cp3056.esams.wmnet
- 10.20.0.57                     # cp3057.esams.wmnet
- 2620:0:862:102:10:20:0:57      # cp3057.esams.wmnet
- 10.20.0.58                     # cp3058.esams.wmnet
- 2620:0:862:102:10:20:0:58      # cp3058.esams.wmnet
- 10.20.0.59                     # cp3059.esams.wmnet
- 2620:0:862:102:10:20:0:59      # cp3059.esams.wmnet
- 10.20.0.60                     # cp3060.esams.wmnet
- 2620:0:862:102:10:20:0:60      # cp3060.esams.wmnet
- 10.20.0.61                     # cp3061.esams.wmnet
- 2620:0:862:102:10:20:0:61      # cp3061.esams.wmnet
- 10.20.0.62                     # cp3062.esams.wmnet
- 2620:0:862:102:10:20:0:62      # cp3062.esams.wmnet
- 10.20.0.63                     # cp3063.esams.wmnet
- 2620:0:862:102:10:20:0:63      # cp3063.esams.wmnet
- 10.20.0.64                     # cp3064.esams.wmnet
- 2620:0:862:102:10:20:0:64      # cp3064.esams.wmnet
- 10.20.0.65                     # cp3065.esams.wmnet
- 2620:0:862:102:10:20:0:65      # cp3065.esams.wmnet
- 10.128.0.19                    # cp4037.ulsfo.wmnet
- 2620:0:863:101:10:128:0:19     # cp4037.ulsfo.wmnet
- 10.128.0.27                    # cp4038.ulsfo.wmnet
- 2620:0:863:101:10:128:0:27     # cp4038.ulsfo.wmnet
- 10.128.0.22                    # cp4039.ulsfo.wmnet
- 2620:0:863:101:10:128:0:22     # cp4039.ulsfo.wmnet
- 10.128.0.26                    # cp4043.ulsfo.wmnet
- 2620:0:863:101:10:128:0:26     # cp4043.ulsfo.wmnet
- 10.128.0.31                    # cp4044.ulsfo.wmnet
- 2620:0:863:101:10:128:0:31     # cp4044.ulsfo.wmnet
- 10.128.0.14                    # cp4045.ulsfo.wmnet
- 2620:0:863:101:10:128:0:14     # cp4045.ulsfo.wmnet
- 10.128.0.35                    # cp4046.ulsfo.wmnet
- 2620:0:863:101:10:128:0:35     # cp4046.ulsfo.wmnet
- 10.128.0.21                    # cp4047.ulsfo.wmnet
- 2620:0:863:101:10:128:0:21     # cp4047.ulsfo.wmnet
- 10.128.0.24                    # cp4049.ulsfo.wmnet
- 2620:0:863:101:10:128:0:24     # cp4049.ulsfo.wmnet
- 10.132.0.102                   # cp5002.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:102 # cp5002.eqsin.wmnet
- 10.132.0.103                   # cp5003.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:103 # cp5003.eqsin.wmnet
- 10.132.0.104                   # cp5004.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:104 # cp5004.eqsin.wmnet
- 10.132.0.105                   # cp5005.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:105 # cp5005.eqsin.wmnet
- 10.132.0.107                   # cp5007.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:107 # cp5007.eqsin.wmnet
- 10.132.0.108                   # cp5008.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:108 # cp5008.eqsin.wmnet
- 10.132.0.109                   # cp5009.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:109 # cp5009.eqsin.wmnet
- 10.132.0.110                   # cp5010.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:110 # cp5010.eqsin.wmnet
- 10.132.0.111                   # cp5011.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:111 # cp5011.eqsin.wmnet
- 10.132.0.112                   # cp5012.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:112 # cp5012.eqsin.wmnet
- 10.132.0.106                   # cp5006.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:106 # cp5006.eqsin.wmnet
- 10.132.0.6                     # cp5013.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:6   # cp5013.eqsin.wmnet
- 10.132.0.7                     # cp5014.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:7   # cp5014.eqsin.wmnet
- 10.132.0.8                     # cp5015.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:8   # cp5015.eqsin.wmnet
- 10.132.0.9                     # cp5016.eqsin.wmnet
- 2001:df2:e500:101:10:132:0:9   # cp5016.eqsin.wmnet
- 10.136.0.6                     # cp6001.drmrs.wmnet
- 2a02:ec80:600:101:10:136:0:6   # cp6001.drmrs.wmnet
- 10.136.1.6                     # cp6002.drmrs.wmnet
- 2a02:ec80:600:102:10:136:1:6   # cp6002.drmrs.wmnet
- 10.136.0.7                     # cp6003.drmrs.wmnet
- 2a02:ec80:600:101:10:136:0:7   # cp6003.drmrs.wmnet
- 10.136.1.7                     # cp6004.drmrs.wmnet
- 2a02:ec80:600:102:10:136:1:7   # cp6004.drmrs.wmnet
- 10.136.0.8                     # cp6005.drmrs.wmnet
- 2a02:ec80:600:101:10:136:0:8   # cp6005.drmrs.wmnet
- 10.136.1.8                     # cp6006.drmrs.wmnet
- 2a02:ec80:600:102:10:136:1:8   # cp6006.drmrs.wmnet
- 10.136.0.9                     # cp6007.drmrs.wmnet
- 2a02:ec80:600:101:10:136:0:9   # cp6007.drmrs.wmnet
- 10.136.1.9                     # cp6008.drmrs.wmnet
- 2a02:ec80:600:102:10:136:1:9   # cp6008.drmrs.wmnet
- 10.136.0.10                    # cp6009.drmrs.wmnet
- 2a02:ec80:600:101:10:136:0:10  # cp6009.drmrs.wmnet
- 10.136.1.10                    # cp6010.drmrs.wmnet
- 2a02:ec80:600:102:10:136:1:10  # cp6010.drmrs.wmnet
- 10.136.0.11                    # cp6011.drmrs.wmnet
- 2a02:ec80:600:101:10:136:0:11  # cp6011.drmrs.wmnet
- 10.136.1.11                    # cp6012.drmrs.wmnet
- 2a02:ec80:600:102:10:136:1:11  # cp6012.drmrs.wmnet
- 10.136.0.12                    # cp6013.drmrs.wmnet
- 2a02:ec80:600:101:10:136:0:12  # cp6013.drmrs.wmnet
- 10.136.1.12                    # cp6014.drmrs.wmnet
- 2a02:ec80:600:102:10:136:1:12  # cp6014.drmrs.wmnet
- 10.136.0.13                    # cp6015.drmrs.wmnet
- 2a02:ec80:600:101:10:136:0:13  # cp6015.drmrs.wmnet
- 10.136.1.13                    # cp6016.drmrs.wmnet
- 2a02:ec80:600:102:10:136:1:13  # cp6016.drmrs.wmnet

kafka_brokers_main:
- 10.64.0.200                  # kafka-main1001.eqiad.wmnet
- 2620:0:861:101:10:64:0:200   # kafka-main1001.eqiad.wmnet
- 10.64.16.37                  # kafka-main1002.eqiad.wmnet
- 2620:0:861:102:10:64:16:37   # kafka-main1002.eqiad.wmnet
- 10.64.32.90                  # kafka-main1003.eqiad.wmnet
- 2620:0:861:103:10:64:32:90   # kafka-main1003.eqiad.wmnet
- 10.64.48.30                  # kafka-main1004.eqiad.wmnet
- 2620:0:861:107:10:64:48:30   # kafka-main1004.eqiad.wmnet
- 10.64.48.31                  # kafka-main1005.eqiad.wmnet
- 2620:0:861:107:10:64:48:31   # kafka-main1005.eqiad.wmnet
- 10.192.0.17                  # kafka-main2001.codfw.wmnet
- 2620:0:860:101:10:192:0:17   # kafka-main2001.codfw.wmnet
- 10.192.16.8                  # kafka-main2002.codfw.wmnet
- 2620:0:860:102:10:192:16:8   # kafka-main2002.codfw.wmnet
- 10.192.32.136                # kafka-main2003.codfw.wmnet
- 2620:0:860:103:10:192:32:136 # kafka-main2003.codfw.wmnet
- 10.192.48.38                 # kafka-main2004.codfw.wmnet
- 2620:0:860:104:10:192:48:38  # kafka-main2004.codfw.wmnet
- 10.192.48.46                 # kafka-main2005.codfw.wmnet
- 2620:0:860:104:10:192:48:46  # kafka-main2005.codfw.wmnet
kafka_brokers_jumbo:
- 10.64.0.175                 # kafka-jumbo1001.eqiad.wmnet
- 2620:0:861:101:10:64:0:175  # kafka-jumbo1001.eqiad.wmnet
- 10.64.0.176                 # kafka-jumbo1002.eqiad.wmnet
- 2620:0:861:101:10:64:0:176  # kafka-jumbo1002.eqiad.wmnet
- 10.64.16.99                 # kafka-jumbo1003.eqiad.wmnet
- 2620:0:861:102:10:64:16:99  # kafka-jumbo1003.eqiad.wmnet
- 10.64.32.159                # kafka-jumbo1004.eqiad.wmnet
- 2620:0:861:103:10:64:32:159 # kafka-jumbo1004.eqiad.wmnet
- 10.64.32.160                # kafka-jumbo1005.eqiad.wmnet
- 2620:0:861:103:10:64:32:160 # kafka-jumbo1005.eqiad.wmnet
- 10.64.48.117                # kafka-jumbo1006.eqiad.wmnet
- 2620:0:861:107:10:64:48:117 # kafka-jumbo1006.eqiad.wmnet
- 10.64.32.106                # kafka-jumbo1007.eqiad.wmnet
- 2620:0:861:103:10:64:32:106 # kafka-jumbo1007.eqiad.wmnet
- 10.64.48.121                # kafka-jumbo1008.eqiad.wmnet
- 2620:0:861:107:10:64:48:121 # kafka-jumbo1008.eqiad.wmnet
- 10.64.48.140                # kafka-jumbo1009.eqiad.wmnet
- 2620:0:861:107:10:64:48:140 # kafka-jumbo1009.eqiad.wmnet
kafka_brokers_logging:
- 10.64.16.205                 # kafka-logging1001.eqiad.wmnet
- 2620:0:861:102:10:64:16:205  # kafka-logging1001.eqiad.wmnet
- 10.64.32.142                 # kafka-logging1002.eqiad.wmnet
- 2620:0:861:103:10:64:32:142  # kafka-logging1002.eqiad.wmnet
- 10.64.48.66                  # kafka-logging1003.eqiad.wmnet
- 2620:0:861:107:10:64:48:66   # kafka-logging1003.eqiad.wmnet
- 10.192.0.94                  # kafka-logging2001.codfw.wmnet
- 2620:0:860:101:10:192:0:94   # kafka-logging2001.codfw.wmnet
- 10.192.16.50                 # kafka-logging2002.codfw.wmnet
- 2620:0:860:102:10:192:16:50  # kafka-logging2002.codfw.wmnet
- 10.192.32.24                 # kafka-logging2003.codfw.wmnet
- 2620:0:860:103:10:192:32:24  # kafka-logging2003.codfw.wmnet
- 10.192.0.112                 # logstash2001.codfw.wmnet
- 2620:0:860:101:10:192:0:112  # logstash2001.codfw.wmnet
- 10.192.32.180                # logstash2002.codfw.wmnet
- 2620:0:860:103:10:192:32:180 # logstash2002.codfw.wmnet
- 10.192.48.131                # logstash2003.codfw.wmnet
- 2620:0:860:104:10:192:48:131 # logstash2003.codfw.wmnet
zookeeper_hosts_main:
- 10.64.0.207                  # conf1007.eqiad.wmnet
- 2620:0:861:101:10:64:0:207   # conf1007.eqiad.wmnet
- 10.64.16.110                 # conf1008.eqiad.wmnet
- 2620:0:861:102:10:64:16:110  # conf1008.eqiad.wmnet
- 10.64.48.154                 # conf1009.eqiad.wmnet
- 2620:0:861:107:10:64:48:154  # conf1009.eqiad.wmnet
- 10.192.16.45                 # conf2004.codfw.wmnet
- 2620:0:860:102:10:192:16:45  # conf2004.codfw.wmnet
- 10.192.32.52                 # conf2005.codfw.wmnet
- 2620:0:860:103:10:192:32:52  # conf2005.codfw.wmnet
- 10.192.48.59                 # conf2006.codfw.wmnet
- 2620:0:860:104:10:192:48:59  # conf2006.codfw.wmnet
druid_public_hosts:
- 10.64.0.35                  # druid1004.eqiad.wmnet
- 2620:0:861:101:10:64:0:35   # druid1004.eqiad.wmnet
- 10.64.16.172                # druid1005.eqiad.wmnet
- 2620:0:861:102:10:64:16:172 # druid1005.eqiad.wmnet
- 10.64.48.171                # druid1006.eqiad.wmnet
- 2620:0:861:107:10:64:48:171 # druid1006.eqiad.wmnet
- 10.64.16.171                # druid1007.eqiad.wmnet
- 2620:0:861:102:10:64:16:171 # druid1007.eqiad.wmnet
- 10.64.48.227                # druid1008.eqiad.wmnet
- 2620:0:861:107:10:64:48:227 # druid1008.eqiad.wmnet

labstore_hosts:
- 208.80.154.142              # clouddumps1001.wikimedia.org
- 2620:0:861:2:208:80:154:142 # clouddumps1001.wikimedia.org
- 208.80.154.71               # clouddumps1002.wikimedia.org
- 2620:0:861:3:208:80:154:71  # clouddumps1002.wikimedia.org

mysql_root_clients:
# ipv6 interfaces are not yet allowed due to mysql grants
# do not put dns names or hostnames here, only ipv4
- 10.64.0.122    # db1115.eqiad.wmnet
- 10.192.48.91   # db2093.codfw.wmnet
- 10.64.32.25    # cumin1001.eqiad.wmnet
- 10.192.32.49   # cumin2002.codfw.wmnet
- 208.80.155.103 # dborch1001.wikimedia.org

monitoring_hosts:
- 208.80.154.88              # alert1001.wikimedia.org
- 2620:0:861:3:208:80:154:88 # alert1001.wikimedia.org
- 208.80.153.84              # alert2001.wikimedia.org
- 2620:0:860:3:208:80:153:84 # alert2001.wikimedia.org

deployment_hosts:
- 10.64.32.28                 # deploy1002.eqiad.wmnet
- 2620:0:861:103:10:64:32:28  # deploy1002.eqiad.wmnet
- 10.192.32.7                 # deploy2002.codfw.wmnet
- 2620:0:860:103:10:192:32:7  # deploy2002.codfw.wmnet

# shared certificates needed by acme-chief and other puppet modules like the ncredir one
# this is only required when a puppet module besides acme-chief needs access to certificate details
# like the list of SNIs
shared_acme_certificates:
  non-canonical-redirect-1:
    CN: 'wikipedia.com'
    SNI:
    - 'wikipedia.com'
    - '*.wikipedia.com'
    - '*.en-wp.com'
    - 'en-wp.com'
    - '*.en-wp.org'
    - 'en-wp.org'
    - '*.mediawiki.com'
    - 'mediawiki.com'
    - '*.voyagewiki.com'
    - 'voyagewiki.com'
    - '*.voyagewiki.org'
    - 'voyagewiki.org'
    - '*.wiikipedia.com'
    - 'wiikipedia.com'
    - '*.wikibook.com'
    - 'wikibook.com'
    - '*.wikibooks.com'
    - 'wikibooks.com'
    - '*.wikiepdia.com'
    - 'wikiepdia.com'
    - '*.wikiepdia.org'
    - 'wikiepdia.org'
    - '*.wikiipedia.org'
    - 'wikiipedia.org'
    - '*.wikijunior.com'
    - 'wikijunior.com'
    - '*.wikijunior.net'
    - 'wikijunior.net'
    - '*.wikijunior.org'
    - 'wikijunior.org'
    staging_time: 604800
    challenge: dns-01
    authorized_regexes:
    - '^ncredir100[12]\.eqiad\.wmnet$'
    - '^ncredir200[12]\.codfw\.wmnet$'
    - '^ncredir300[12]\.esams\.wmnet$'
    - '^ncredir400[12]\.ulsfo\.wmnet$'
    - '^ncredir500[12]\.eqsin\.wmnet$'
    - '^ncredir600[12]\.drmrs\.wmnet$'
    prevalidate: true
    skip_invalid_snis: true
  non-canonical-redirect-2:
    CN: '*.wikimania.com'
    SNI:
    - '*.wikimania.com'
    - 'wikimania.com'
    - '*.wikimania.org'
    - 'wikimania.org'
    - '*.wikimediacommons.co.uk'
    - 'wikimediacommons.co.uk'
    - '*.wikimediacommons.info'
    - 'wikimediacommons.info'
    - '*.wikimediacommons.jp.net'
    - 'wikimediacommons.jp.net'
    - '*.wikimediacommons.mobi'
    - 'wikimediacommons.mobi'
    - '*.wikimediacommons.net'
    - 'wikimediacommons.net'
    - '*.wikimediacommons.org'
    - 'wikimediacommons.org'
    - '*.wikimedia.community'
    - 'wikimedia.community'
    - '*.wikimania.asia'
    - 'wikimania.asia'
    - '*.wikimediafoundation.com'
    - 'wikimediafoundation.com'
    - '*.wikimediafoundation.info'
    - 'wikimediafoundation.info'
    - '*.wikimediafoundation.net'
    - 'wikimediafoundation.net'
    - '*.wikimedia.jp.net'
    - 'wikimedia.jp.net'
    - '*.wikimedia.lt'
    - 'wikimedia.lt'
    - '*.wikimedia.us'
    - 'wikimedia.us'
    - '*.wikinews.com'
    - 'wikinews.com'
    - '*.wikinews.de'
    - 'wikinews.de'
    staging_time: 604800
    challenge: dns-01
    authorized_regexes:
    - '^ncredir100[12]\.eqiad\.wmnet$'
    - '^ncredir200[12]\.codfw\.wmnet$'
    - '^ncredir300[12]\.esams\.wmnet$'
    - '^ncredir400[12]\.ulsfo\.wmnet$'
    - '^ncredir500[12]\.eqsin\.wmnet$'
    - '^ncredir600[12]\.drmrs\.wmnet$'
    prevalidate: true
    skip_invalid_snis: true
  non-canonical-redirect-3:
    CN: '*.wikipedia.bg'
    SNI:
    - '*.wikipedia.bg'
    - 'wikipedia.bg'
    - '*.wikipedia.co.il'
    - 'wikipedia.co.il'
    - '*.wikipedia.co.za'
    - 'wikipedia.co.za'
    - '*.wikipedia.ee'
    - 'wikipedia.ee'
    - '*.wikipedia.gr'
    - 'wikipedia.gr'
    - '*.wikipedia.in'
    - 'wikipedia.in'
    - '*.wikipedia.info'
    - 'wikipedia.info'
    - '*.wikipedia.is'
    - 'wikipedia.is'
    - '*.wikipedia.lt'
    - 'wikipedia.lt'
    - '*.wikipedia.net'
    - 'wikipedia.net'
    - '*.wiki-pedia.org'
    - 'wiki-pedia.org'
    - '*.wikipedia.org.il'
    - 'wikipedia.org.il'
    - '*.wikipediazero.org'
    - 'wikipediazero.org'
    - '*.wikiquote.com'
    - 'wikiquote.com'
    - '*.wikiquote.net'
    - 'wikiquote.net'
    - '*.wikisource.com'
    - 'wikisource.com'
    - '*.wikisource.pl'
    - 'wikisource.pl'
    - '*.wikispecies.com'
    - 'wikispecies.com'
    staging_time: 604800
    challenge: dns-01
    authorized_regexes:
    - '^ncredir100[12]\.eqiad\.wmnet$'
    - '^ncredir200[12]\.codfw\.wmnet$'
    - '^ncredir300[12]\.esams\.wmnet$'
    - '^ncredir400[12]\.ulsfo\.wmnet$'
    - '^ncredir500[12]\.eqsin\.wmnet$'
    - '^ncredir600[12]\.drmrs\.wmnet$'
    prevalidate: true
    skip_invalid_snis: true
  non-canonical-redirect-4:
    CN: '*.wikispecies.net'
    SNI:
    - '*.wikispecies.net'
    - 'wikispecies.net'
    - '*.wikispecies.org'
    - 'wikispecies.org'
    - '*.wikiversity.com'
    - 'wikiversity.com'
    - '*.wikivoyage.com'
    - 'wikivoyage.com'
    - '*.wikivoyage.de'
    - 'wikivoyage.de'
    - '*.wikivoyage.eu'
    - 'wikivoyage.eu'
    - '*.wikivoyage.net'
    - 'wikivoyage.net'
    - '*.wikivoyager.de'
    - 'wikivoyager.de'
    - '*.wikivoyager.org'
    - 'wikivoyager.org'
    - '*.wikpedia.org'
    - 'wikpedia.org'
    - '*.wiktionary.com'
    - 'wiktionary.com'
    - '*.wiktionary.eu'
    - 'wiktionary.eu'
    - 'pywikibot.org'
    - '*.pywikibot.org'
    staging_time: 604800
    challenge: dns-01
    authorized_regexes:
    - '^ncredir100[12]\.eqiad\.wmnet$'
    - '^ncredir200[12]\.codfw\.wmnet$'
    - '^ncredir300[12]\.esams\.wmnet$'
    - '^ncredir400[12]\.ulsfo\.wmnet$'
    - '^ncredir500[12]\.eqsin\.wmnet$'
    - '^ncredir600[12]\.drmrs\.wmnet$'
    prevalidate: true
    skip_invalid_snis: true
  non-canonical-redirect-5:
    CN: 'wikimedia.is'
    SNI:
    - 'wikimedia.is'
    - '*.wikimedia.is'
    - 'wikimedia.com.pt'
    - '*.wikimedia.com.pt'
    - 'indianwikimedia.com'
    - '*.indianwikimedia.com'
    - 'wikimedia.biz'
    - '*.wikimedia.biz'
    - 'wikimedia.xyz'
    - '*.wikimedia.xyz'
    - 'wwwwikimedia.com'
    - '*.wwwwikimedia.com'
    - 'wikipedia.es'
    - '*.wikipedia.es'
    - 'wkipedia.org'
    - '*.wkipedia.org'
    - 'wikkipedia.com'
    - '*.wikkipedia.com'
    - 'wilkipedia.org'
    - '*.wilkipedia.org'
    - 'wilipedia.com'
    - '*.wilipedia.com'
    - 'wikipediya.com'
    - '*.wikipediya.com'
    - 'wikipdia.org'
    - '*.wikipdia.org'
    - 'wikipedia.co.uk'
    - '*.wikipedia.co.uk'
    - 'wikipedia.uk'
    - '*.wikipedia.uk'
    - 'wikipedoa.org'
    - '*.wikipedoa.org'
    - 'wikipeedia.org'
    - '*.wikipeedia.org'
    - 'wiktionary.pl'
    - '*.wiktionary.pl'
    - 'wikipedia.com.ar'
    - '*.wikipedia.com.ar'
    - 'wikipedia.sk'
    - '*.wikipedia.sk'
    staging_time: 604800
    challenge: dns-01
    authorized_regexes:
    - '^ncredir100[12]\.eqiad\.wmnet$'
    - '^ncredir200[12]\.codfw\.wmnet$'
    - '^ncredir300[12]\.esams\.wmnet$'
    - '^ncredir400[12]\.ulsfo\.wmnet$'
    - '^ncredir500[12]\.eqsin\.wmnet$'
    - '^ncredir600[12]\.drmrs\.wmnet$'
    prevalidate: true
    skip_invalid_snis: true
  non-canonical-redirect-6:
    CN: 'wikipedia.fi'
    SNI:
    - 'wikipedia.fi'
    - '*.wikipedia.fi'
    - 'wikisource.pt'
    - '*.wikisource.pt'
    - 'wiktionary.pt'
    - '*.wiktionary.pt'
    - 'wiki.voyage'
    - '*.wiki.voyage'
    - 'wikiversity.pt'
    - '*.wikiversity.pt'
    - 'wikiquote.pt'
    - '*.wikiquote.pt'
    - 'wikiquotes.info'
    - '*.wikiquotes.info'
    - 'wikinews.pt'
    - '*.wikinews.pt'
    - 'wikibooks.pt'
    - '*.wikibooks.pt'
    - 'wikipedia.id'
    - '*.wikipedia.id'
    - 'wikjpedia.org'
    - '*.wikjpedia.org'
    - 'wikipedial.org'
    - '*.wikipedial.org'
    - 'wikidata.es'
    - '*.wikidata.es'
    - 'wikidata.pt'
    - '*.wikidata.pt'
    - 'wikidata.us'
    - '*.wikidata.us'
    - 'wikipediafoundation.org'
    - '*.wikipediafoundation.org'
    - 'wikidpedia.org'
    - '*.wikidpedia.org'
    - 'wekipedia.com'
    - '*.wekipedia.com'
    - 'wikipaedia.net'
    - '*.wikipaedia.net'
    - 'wikepedia.org'
    - '*.wikepedia.org'
    staging_time: 604800
    challenge: dns-01
    authorized_regexes:
    - '^ncredir100[12]\.eqiad\.wmnet$'
    - '^ncredir200[12]\.codfw\.wmnet$'
    - '^ncredir300[12]\.esams\.wmnet$'
    - '^ncredir400[12]\.ulsfo\.wmnet$'
    - '^ncredir500[12]\.eqsin\.wmnet$'
    - '^ncredir600[12]\.drmrs\.wmnet$'
    prevalidate: true
    skip_invalid_snis: true
labsldapconfig: {}
ldap:
  base-dn: 'dc=wikimedia,dc=org'
  groups_cn: 'ou=groups'
  users_cn: 'ou=people'
  proxyagent: 'cn=proxyagent,ou=profile,dc=wikimedia,dc=org'
  proxypass: "%{lookup('labsldapconfig.proxypass')}"
  script_user_dn: 'cn=scriptuser,ou=profile,dc=wikimedia,dc=org'
  script_user_pass: "%{lookup('labsldapconfig.script_user_pass')}"
netbox_api_url: https://netbox.discovery.wmnet/
apereo_cas:
  production:
    base_url: 'https://idp.wikimedia.org/'
    login_url: 'https://idp.wikimedia.org/login'
    validate_url: 'https://idp.wikimedia.org/serviceValidate'
    validate_url_saml: 'https://idp.wikimedia.org/samlValidate'
  staging:
    base_url: 'https://idp-test.wikimedia.org/'
    login_url: 'https://idp-test.wikimedia.org/login'
    validate_url: 'https://idp-test.wikimedia.org/serviceValidate'
    validate_url_saml: 'https://idp-test.wikimedia.org/samlValidate'

ripeatlas_measurements:
  eqiad:  # https://atlas.ripe.net/frames/probes/6092/
    ipv4:
      - '1790945'  # anchors pings
      - '1790944'  # anchors traceroutes
      - '9183717'  # probes pings
      - '9183716'  # probes traceroutes
    ipv6:
      - '1790947'  # anchors pings
      - '1790946'  # anchors traceroutes
      - '9183720'  # probes pings
      - '9183719'  # probes traceroutes
  codfw:  # https://atlas.ripe.net/frames/probes/7038/
    ipv4:
      - '32390538'  # anchors pings
      - '32390537'  # anchors traceroutes
      - '32391305'  # probes pings
      - '32391304'  # probes traceroutes
    ipv6:
      - '32390541'  # anchors pings
      - '32390540'  # anchors traceroutes
      - '32391312'  # probes pings
      - '32391311'  # probes traceroutes
  esams:  # https://atlas.ripe.net/frames/probes/6671/
    ipv4:
      - '23449935'  # anchors pings
      - '23449934'  # anchors traceroutes
      - '23484811'  # probes pings
      - '23484810'  # probes traceroutes
    ipv6:
      - '23449938'  # anchors pings
      - '23449937'  # anchors traceroutes
      - '23484824'  # probes pings
      - '23484823'  # probes traceroutes
  ulsfo:  # https://atlas.ripe.net/frames/probes/6095/
    ipv4:
      - '1791307'  # anchors pings
      - '1791306'  # anchors traceroutes
      - '9183761'  # probes pings
      - '9183760'  # probes traceroutes
    ipv6:
      - '1791309'  # anchors pings
      - '1791308'  # anchors traceroutes
      - '9183764'  # probes pings
      - '9183763'  # probes traceroutes
  eqsin:  # https://atlas.ripe.net/frames/probes/6345/
    ipv4:
      - '11645085'  # anchors pings
      - '11645084'  # anchors traceroutes
      - '11645097'  # probes pings
      - '11645096'  # probes traceroutes
    ipv6:
      - '11645088'  # anchors pings
      - '11645087'  # anchors traceroutes
      - '11645100'  # probes pings
      - '11645099'  # probes traceroutes

asns:
  eqiad: 65001
  codfw: 65002
  esams: 65003
  ulsfo: 65004
  eqsin: 65005
  drmrs: 65006
  eqord: 65020

swift_storage_drives:
  - '/dev/sdc'
  - '/dev/sdd'
  - '/dev/sde'
  - '/dev/sdf'
  - '/dev/sdg'
  - '/dev/sdh'
  - '/dev/sdi'
  - '/dev/sdj'
  - '/dev/sdk'
  - '/dev/sdl'
  - '/dev/sdm'
  - '/dev/sdn'

swift_aux_partitions:
  - '/dev/sda3'
  - '/dev/sda4'
  - '/dev/sdb3'
  - '/dev/sdb4'

mediabackup:
  batchsize: 1000
  storage_path: '/srv/objectstorage'
  storage_port: 9000

# The hash of hostname -> data for infrastructure (e.g. not running Puppet) devices.

# See also Wmflib::Infra::Devices type and related for more information.
# Sites are separated/grouped by newline for easier navigation.
infra_devices:
  cr1-eqiad:
    ipv4: 208.80.154.196
    ipv6: 2620:0:861:ffff::1
    vrrp_peer: cr2-eqiad.wikimedia.org
    alarms: true
    site: eqiad
    role: cr
  cr2-eqiad:
    ipv4: 208.80.154.197
    ipv6: 2620:0:861:ffff::2
    alarms: true
    site: eqiad
    role: cr
  pfw3-eqiad:
    ipv4: 208.80.154.219
    parents:
    - cr1-eqiad
    - cr2-eqiad
    bfd: false
    site: eqiad
    role: pfw
  mr1-eqiad:
    ipv4: '208.80.154.199'
    ipv6: '2620:0:861:ffff::6'
    parents:
    - asw2-a-eqiad
    site: eqiad
    role: mr
  msw1-eqiad:
    ipv4: '10.65.0.10'
    parents:
    - mr1-eqiad
    site: eqiad
    role: msw
  msw2-eqiad:
    ipv4: '10.65.0.5'
    parents:
    - msw1-eqiad
    site: eqiad
    role: msw
  asw2-a-eqiad:
    ipv4: 10.65.0.21
    parents:
    - cr1-eqiad
    - cr2-eqiad
    site: eqiad
    role: l2sw
  asw2-b-eqiad:
    ipv4: 10.65.0.25
    parents:
    - cr1-eqiad
    - cr2-eqiad
    site: eqiad
    role: l2sw
  asw2-c-eqiad:
    ipv4: 10.65.0.26
    parents:
    - cr1-eqiad
    - cr2-eqiad
    site: eqiad
    role: l2sw
  asw2-d-eqiad:
    ipv4: 10.65.0.27
    parents:
    - cr1-eqiad
    - cr2-eqiad
    site: eqiad
    role: l2sw
  fasw-c-eqiad:
    ipv4: 10.65.0.30
    parents:
    - pfw3-eqiad
    site: eqiad
    role: l2sw
  cloudsw2-c8-eqiad:
    ipv4: 10.64.146.250
    ipv6: 2620:0:861:11b::250
    parents:
    - cloudsw1-c8-eqiad
    site: eqiad
    role: l3sw
  cloudsw2-d5-eqiad:
    ipv4: 10.64.146.251
    ipv6: 2620:0:861:11b::251
    parents:
    - cloudsw1-d5-eqiad
    site: eqiad
    role: l3sw
  ripe-atlas-eqiad:
    ipv4: '208.80.155.69'
    ipv6: '2620:0:861:202:208:80:155:69'
    site: eqiad
    role: atlas
    parents:
    - asw2-b-eqiad
  scs-a8-eqiad:
    ipv4: 10.65.0.11
    parents:
    - msw1-eqiad
    site: eqiad
    role: scs
  scs-c1-eqiad:
    ipv4: 10.65.0.22
    parents:
    - msw1-eqiad
    site: eqiad
    role: scs
  scs-f8-eqiad:
    ipv4: 10.65.0.4
    parents:
    - msw2-eqiad
    site: eqiad
    role: scs
  cloudsw1-c8-eqiad:
    ipv4: 10.64.146.252
    ipv6: 2620:0:861:11b::252
    parents:
    - cr1-eqiad
    - cr2-eqiad
    site: eqiad
    role: l3sw
  cloudsw1-d5-eqiad:
    ipv4: 10.64.146.253
    ipv6: 2620:0:861:11b::253
    parents:
    - cr1-eqiad
    - cr2-eqiad
    site: eqiad
    role: l3sw
  cloudsw1-e4-eqiad:
    ipv4: 10.64.146.254
    ipv6: 2620:0:861:11b::254
    parents:
    - cloudsw1-c8-eqiad
    - cloudsw1-d5-eqiad
    site: eqiad
    role: l3sw
  cloudsw1-f4-eqiad:
    ipv4: 10.64.146.255
    ipv6: 2620:0:861:11b::255
    parents:
    - cloudsw1-c8-eqiad
    - cloudsw1-d5-eqiad
    site: eqiad
    role: l3sw
  lsw1-e1-eqiad:
    ipv4: 10.64.146.3
    ipv6: 2620:0:861:11b::3
    parents:
    - cr1-eqiad
    - cr2-eqiad
    site: eqiad
    role: l3sw
  lsw1-e2-eqiad:
    ipv4: 10.64.146.4
    ipv6: 2620:0:861:11b::4
    parents:
    - lsw1-e1-eqiad
    - lsw1-f1-eqiad
    site: eqiad
    role: l3sw
  lsw1-e3-eqiad:
    ipv4: 10.64.146.5
    ipv6: 2620:0:861:11b::5
    parents:
    - lsw1-e1-eqiad
    - lsw1-f1-eqiad
    site: eqiad
    role: l3sw
  lsw1-f1-eqiad:
    ipv4: 10.64.146.7
    ipv6: 2620:0:861:11b::7
    parents:
    - cr1-eqiad
    - cr2-eqiad
    site: eqiad
    role: l3sw
  lsw1-f2-eqiad:
    ipv4: 10.64.146.8
    ipv6: 2620:0:861:11b::8
    parents:
    - lsw1-e1-eqiad
    - lsw1-f1-eqiad
    site: eqiad
    role: l3sw
  lsw1-f3-eqiad:
    ipv4: 10.64.146.9
    ipv6: 2620:0:861:11b::9
    parents:
    - lsw1-e1-eqiad
    - lsw1-f1-eqiad
    site: eqiad
    role: l3sw

  cr2-eqord:
    ipv4: 208.80.154.198
    ipv6: 2620:0:861:ffff::5
    site: eqord
    role: cr

  cr1-codfw:
    ipv4: 208.80.153.192
    ipv6: 2620:0:860:ffff::1
    vrrp_peer: cr2-codfw.wikimedia.org
    alarms: true
    site: codfw
    role: cr
  cr2-codfw:
    ipv4: 208.80.153.193
    ipv6: 2620:0:860:ffff::2
    alarms: true
    site: codfw
    role: cr
  pfw3-codfw:
    ipv4: 208.80.153.197
    parents:
    - cr1-codfw
    - cr2-codfw
    bfd: false
    site: codfw
    role: pfw
  mr1-codfw:
    ipv4: '208.80.153.196'
    ipv6: '2620:0:860:ffff::6'
    parents:
    - asw-a-codfw
    site: codfw
    role: mr
  msw1-codfw:
    ipv4: '10.193.0.3'
    parents:
    - mr1-codfw
    site: codfw
    role: msw
  asw-a-codfw:
    ipv4: 10.193.0.16
    parents:
    - cr1-codfw
    - cr2-codfw
    site: codfw
    role: l2sw
  asw-b-codfw:
    ipv4: 10.193.0.17
    parents:
    - cr1-codfw
    - cr2-codfw
    site: codfw
    role: l2sw
  asw-c-codfw:
    ipv4: 10.193.0.18
    parents:
    - cr1-codfw
    - cr2-codfw
    site: codfw
    role: l2sw
  asw-d-codfw:
    ipv4: 10.193.0.19
    parents:
    - cr1-codfw
    - cr2-codfw
    site: codfw
    role: l2sw
  fasw-c-codfw:
    ipv4: 10.193.0.57
    parents:
    - pfw3-codfw
    site: codfw
    role: l2sw
  ripe-atlas-codfw:
    ipv4: 208.80.152.244
    ipv6: 2620:0:860:201:208:80:152:244
    parents:
    - asw-a-codfw
    site: codfw
    role: atlas
  scs-a1-codfw:
    ipv4: 10.193.0.14
    parents:
    - msw1-codfw
    site: codfw
    role: scs
  scs-c1-codfw:
    ipv4: 10.193.0.15
    parents:
    - msw1-codfw
    site: codfw
    role: scs

  cr2-eqdfw:
    ipv4: 208.80.153.198
    ipv6: 2620:0:860:ffff::5
    site: eqdfw
    role: cr

  cr3-esams:
    ipv4: 91.198.174.245
    ipv6: 2620:0:862:ffff::5
    vrrp_peer: cr2-esams.wikimedia.org
    alarms: true
    site: esams
    role: cr
  cr2-esams:
    ipv4: 91.198.174.244
    ipv6: 2620:0:862:ffff::3
    alarms: true
    site: esams
    role: cr
  mr1-esams:
    ipv4: '91.198.174.247'
    ipv6: '2620:0:862:ffff::1'
    parents:
    - asw2-esams
    site: esams
    role: mr
  asw2-esams:
    ipv4: 10.21.0.8
    parents:
    - cr3-esams
    - cr2-esams
    site: esams
    role: l2sw
  ripe-atlas-esams:
    ipv4: 91.198.174.132
    ipv6: 2620:0:862:201:91:198:174:132
    parents:
    - asw2-esams
    site: esams
    role: atlas
  scs-oe16-esams:
    ipv4: 10.21.0.9
    parents:
    - mr1-esams
    site: esams
    role: scs

  cr3-knams:
    ipv4: 91.198.174.246
    ipv6: 2620:0:862:ffff::4
    site: knams
    role: cr

  cr3-ulsfo:
    ipv4: 198.35.26.192
    ipv6: 2620:0:863:ffff::1
    vrrp_peer: cr4-ulsfo.wikimedia.org
    site: ulsfo
    role: cr
  cr4-ulsfo:
    ipv4: 198.35.26.193
    ipv6: 2620:0:863:ffff::2
    site: ulsfo
    role: cr
  mr1-ulsfo:
    ipv4: '198.35.26.194'
    ipv6: '2620:0:863:ffff::6'
    parents:
    - asw2-ulsfo
    site: ulsfo
    role: mr
  asw2-ulsfo:
    ipv4: 10.128.128.7
    parents:
    - cr3-ulsfo
    - cr4-ulsfo
    site: ulsfo
    role: l2sw
  ripe-atlas-ulsfo:
    ipv4: 198.35.26.244
    ipv6: 2620:0:863:201:198:35:26:244
    parents:
    - asw2-ulsfo
    site: ulsfo
    role: atlas
  scs-ulsfo:
    ipv4: 10.128.128.11
    parents:
    - mr1-ulsfo
    site: ulsfo
    role: scs

  cr2-eqsin:
    ipv4: 103.102.166.130
    ipv6: 2001:df2:e500:ffff::3
    site: eqsin
    role: cr
  cr3-eqsin:
    ipv4: 103.102.166.131
    ipv6: 2001:df2:e500:ffff::4
    vrrp_peer: cr2-eqsin.wikimedia.org
    site: eqsin
    role: cr
  mr1-eqsin:
    ipv4: '103.102.166.128'
    ipv6: '2001:df2:e500:ffff::1'
    parents:
    - asw1-eqsin
    site: eqsin
    role: mr
  asw1-eqsin:
    ipv4: 10.132.128.4
    parents:
    - cr2-eqsin
    - cr3-eqsin
    site: eqsin
    role: l2sw
  ripe-atlas-eqsin:
    ipv4: 103.102.166.20
    ipv6: 2001:df2:e500:201:103:102:166:20
    parents:
    - asw1-eqsin
    site: eqsin
    role: atlas
  scs-eqsin:
    ipv4: 10.132.128.5
    parents:
    - mr1-eqsin
    site: eqsin
    role: scs

  cr1-drmrs:
    ipv4: 185.15.58.128
    ipv6: 2a02:ec80:600:ffff::1
    site: drmrs
    role: cr
  cr2-drmrs:
    ipv4: 185.15.58.129
    ipv6: 2a02:ec80:600:ffff::2
    site: drmrs
    role: cr
  mr1-drmrs:
    ipv4: '185.15.58.130'
    ipv6: '2a02:ec80:600:ffff::3'
    parents:
    - asw1-b12-drmrs
    - asw1-b13-drmrs
    site: drmrs
    role: mr
  scs-drmrs:
    ipv4: 10.136.128.5
    parents:
    - mr1-drmrs
    site: drmrs
    role: scs
  asw1-b12-drmrs:
    ipv4: 185.15.58.131
    ipv6: 2a02:ec80:600:ffff::4
    parents:
    - cr1-drmrs
    - cr2-drmrs
    site: drmrs
    role: l3sw
  asw1-b13-drmrs:
    ipv4: 185.15.58.132
    ipv6: 2a02:ec80:600:ffff::5
    parents:
    - cr1-drmrs
    - cr2-drmrs
    site: drmrs
    role: l3sw


# A map of hostname -> metadata for Prometheus blackbox exporter to ping.
# The list is ported from Smokeping and manually maintained as of Jul
# 2022. Going forward the list must come from Netbox instead.

# See also Wmflib::Infra::Devices type and related for more information.
# Sites are separated/grouped by newline for easier navigation.
blackbox_smoke_hosts:
  frbast-eqiad.wikimedia.org:
    realm: frack
    site: eqiad
    rack: C1
  frpig1001.frack.eqiad.wmnet:
    realm: frack
    site: eqiad
    rack: C1
  alert1001.wikimedia.org:
    realm: production
    site: eqiad
    rack: C6
  dns1001.wikimedia.org:
    realm: production
    site: eqiad
    rack: A1
  dns1002.wikimedia.org:
    realm: production
    site: eqiad
    rack: D1
  gerrit1001.wikimedia.org:
    realm: production
    site: eqiad
    rack: B8
  authdns1001.wikimedia.org:
    realm: production
    site: eqiad
    rack: B1

  frbast-codfw.wikimedia.org:
    realm: frack
    site: codfw
    rack: C8
  frpig2001.frack.codfw.wmnet:
    realm: frack
    site: codfw
    rack: C8
  authdns2001.wikimedia.org:
    realm: production
    site: codfw
    rack: A2
  bast2002.wikimedia.org:
    realm: production
    site: codfw
    rack: B5
  dns2001.wikimedia.org:
    realm: production
    site: codfw
    rack: C5
  dns2002.wikimedia.org:
    realm: production
    site: codfw
    rack: D5
  alert2001.wikimedia.org:
    realm: production
    site: codfw
    rack: C5
  puppetmaster2002.codfw.wmnet:
    realm: production
    site: codfw
    rack: D5

  ganeti3001.esams.wmnet:
    realm: production
    site: esams
    rack: OE14
  ganeti3002.esams.wmnet:
    realm: production
    site: esams
    rack: OE15
  ganeti3003.esams.wmnet:
    realm: production
    site: esams
    rack: OE16

  ganeti4005.ulsfo.wmnet:
    realm: production
    site: ulsfo
    rack: "103.02.22"
  ganeti4008.ulsfo.wmnet:
    realm: production
    site: ulsfo
    rack: "103.02.23"

  ganeti5001.eqsin.wmnet:
    realm: production
    site: eqsin
    rack: "603"
  ganeti5002.eqsin.wmnet:
    realm: production
    site: eqsin
    rack: "604"

  dns6001.wikimedia.org:
    realm: production
    site: drmrs
    rack: B12
  ganeti6002.drmrs.wmnet:
    realm: production
    site: drmrs
    rack: B13
