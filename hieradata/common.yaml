## NOTE namespaced keys (i.e. with ::) will NOT be looked up here
## See also https://phabricator.wikimedia.org/T209265

# General variables that once would have been in realm.pp
datacenters:
  - eqiad
  - codfw
  - esams
  - ulsfo
  - eqsin

# Main statsd instance
statsd: statsd.eqiad.wmnet:8125
statsd_exporter_port: 9125

# Debmonitor instance
debmonitor: debmonitor.discovery.wmnet

# NOTE: Do *not* add new clusters *per site* anymore,
# the site name will automatically be appended now,
# and a different IP prefix will be used.
wikimedia_clusters:
  decommissioned:
    description: "Decommissioned servers"
    id: 1
    sites: {}
  lvs:
    description: "LVS loadbalancers"
    id: 2
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
  puppet:
    description: "Puppetmasters"
    id: 3
    sites:
      eqiad: []
      codfw: []
  search:
    description: "Search"
    id: 4
    sites: {}
  mysql:
    description: "MySQL"
    id: 5
    sites:
      eqiad: []
      codfw: []
  etcd:
    description: "Etcd"
    id: 6
    sites:
      eqiad: []
      codfw: []
  eventbus:
    description: "Eventbus"
    id: 7
    sites:
      eqiad: []
  misc:
    description: "Miscellaneous"
    id: 8
    sites:
      eqiad: []
      codfw: []
      esams: []
  kubernetes:
    description: "Kubernetes"
    id: 9
    sites:
      eqiad: []
      codfw: []
  appserver:
    description: "Application servers"
    id: 11
    sites:
      eqiad: []
      codfw: []
  api_appserver:
    description: "API application servers"
    id: 13
    sites:
      eqiad: []
      codfw: []
  cache_text:
    description: "Text caches"
    id: 20
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
  cache_canary:
    description: "Canary caches"
    id: 21
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
  cache_upload:
    description: "Upload caches"
    id: 22
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
  payments:
    description: "Fundraiser payments"
    id: 23
    sites: {}
  ssl:
    description: "SSL cluster"
    id: 26
    sites: {}
  swift:
    description: "Swift"
    id: 27
    sites:
      eqiad: []
      codfw: []
      esams: []
  labvirt:
    description: "Labs virt hosts"
    id: 29
    sites:
      eqiad: []
  labs:
    description: "Labs services"
    id: 30
    sites:
      eqiad: []
  jobrunner:
    description: "Jobrunners"
    id: 31
    sites:
      eqiad: []
      codfw: []
  analytics:
    description: "Analytics cluster"
    id: 32
    sites:
      eqiad: []
  memcached:
    description: "Memcached"
    id: 33
    sites:
      eqiad: []
      codfw: []
  fundraising:
    description: "Fundraising"
    id: 35
    sites:
      eqiad:
        - pay-lvs1001.frack.eqiad.wmnet
        - pay-lvs1002.frack.eqiad.wmnet
  ceph:           # Not used anymore
    description: "Ceph"
    id: 36
    sites: {}
  parsoid:
    description: "Parsoid"
    id: 37
    sites:
      eqiad: []
      codfw: []
  redis:
    description: "Redis"
    id: 39
    sites:
      eqiad: []
      codfw: []
  labsnfs:
    description: "Labs NFS cluster"
    id: 40
    sites:
      eqiad: []
      codfw: []
  elasticsearch:
    description: "Elasticsearch cluster"
    id: 42
    sites:
      eqiad: []
      codfw: []
  logstash:
    description: "Logstash cluster"
    id: 43
    sites:
      eqiad: []
  analytics_kafka:
    description: "Analytics Kafka cluster"
    id: 45
    sites:
      eqiad: []
  restbase:
    description: "Restbase"
    id: 48
    sites:
      eqiad: []
      codfw: []
  wdqs:
    description: "Wikidata Query Service - Public cluster"
    id: 49
    sites:
      eqiad: []
  maps:
    description: "Maps Cluster"
    id: 50
    sites:
      eqiad: []
      codfw: []
  ganeti:
    description: "Ganeti Virt cluster"
    id: 52
    sites:
      eqiad: []
      codfw: []
  scb:
    description: "Service Cluster B"
    id: 53
    sites:
      eqiad: []
      codfw: []
  aqs:
    description: "Analytics Query Service"
    id: 54
    sites:
      eqiad: []
  restbase_test:
    description: "Restbase test"
    id: 55
    sites:
      eqiad: []
      codfw: []
  relforge:
    description: "Elasticsearch relforge cluster"
    id: 56
    sites:
      eqiad: []
  labtestvirt:
    description: "Labtest virt hosts"
    id: 57
    sites:
      codfw: []
  labtest:
    description: "Labtest services"
    id: 58
    sites:
      codfw: []
  restbase_dev:
    description: "Services development test"
    id: 59
    sites:
      eqiad: []
  thumbor:
    description: "Thumbor"
    id: 61
    sites:
      codfw: []
      eqiad: []
  kafka_jumbo:
    description: "Kafka Jumbo Cluster"
    id: 62
    sites:
      eqiad: []
  druid_analytics:
    description: "Druid Analytics Cluster"
    id: 63
    sites:
      eqiad: []
  druid_public:
    description: "Druid Public Cluster"
    id: 64
    sites:
      eqiad: []
  ores:
    description: "ORES Cluster"
    id: 65
    sites:
      eqiad: []
      codfw: []
  wdqs-internal:
    description: "Wikidata Query Service - Internal cluster"
    id: 66
    sites:
      eqiad: []
      codfw: []
  wdqs-test:
    description: "Wikidata Query Service - Test cluster"
    id: 67
    sites:
      eqiad: []
  proton:
    description: "Proton PDF rendering service"
    id: 68
    sites:
      eqiad: []
      codfw: []
  recursor:
    description: "DNS Recursor and NTP Servers"
    id: 69
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
  spare:
    description: "Spare servers"
    id: 70
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
  prometheus:
    description: "Prometheus servers"
    id: 71
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
  graphite:
    description: "Graphite servers"
    id: 72
    sites:
      eqiad: []
      codfw: []
  bastion:
    description: "Bastion servers"
    id: 73
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
  cache_ats:
    description: "Apache Traffic Server (ATS) servers"
    id: 74
    sites:
      eqiad: []
      codfw: []
      esams: []
      ulsfo: []
      eqsin: []
  alerting:
    description: "Icinga"
    id: 75
    sites:
      eqiad: []
      codfw: []
  ci:
    description: "Continuous Integration servers"
    id: 76
    sites:
      eqiad: []
      codfw: []
  management:
    description: "Management servers"
    id: 77
    sites:
      eqiad: []
      codfw: []
  wmcs:
    description: "WMCS servers"
    id: 79
    sites:
      eqiad: []
  webperf:
    description: "Web Performance servers"
    id: 80
    sites:
      eqiad: []
      codfw: []
  poolcounter:
    description: "Poolcounter servers"
    id: 81
    sites:
      eqiad: []
      codfw: []
  dumps:
    description: "Dumps servers"
    id: 82
    sites:
      eqiad: []
  syslog:
    description: "Syslog servers"
    id: 83
    sites:
      eqiad: []
      codfw: []
  druid_test_analytics:
    description: "Druid Analytics Test Cluster"
    id: 84
    sites:
      eqiad: []

puppetmaster: "puppet"

statistics_servers:
  - stat1004.eqiad.wmnet
  - stat1006.eqiad.wmnet
  - stat1007.eqiad.wmnet
  - labstore1006.wikimedia.org
  - labstore1007.wikimedia.org
labstore_servers:
  - labstore1006.wikimedia.org
  - labstore1007.wikimedia.org
dumps_nfs_clients:
  snapshots:
    - snapshot1005.eqiad.wmnet
    - snapshot1006.eqiad.wmnet
    - snapshot1007.eqiad.wmnet
    - snapshot1008.eqiad.wmnet
    - snapshot1009.eqiad.wmnet
dumps_datadir_mount_type: nfs
dumps_nfs_server: dumpsdata1001.eqiad.wmnet
dumps_managed_subdirs: []

# Dumps distribution servers actively serving NFS traffic
dumps_dist_nfs_servers: [labstore1006.wikimedia.org, labstore1007.wikimedia.org]

# Dumps distribution server currently serving traffic over NFS to cloud vps instances
dumps_dist_active_vps: labstore1007.wikimedia.org
# Dumps distribution server currently serving web and rsync mirror traffic
# Also serves stat* hosts over nfs
dumps_dist_active_web: labstore1006.wikimedia.org

# Cloud Services <
#
# Cumin
labs_tld: "wmflabs"
profile::openstack::main::cumin::auth_group: cumin_masters
profile::openstack::main::cumin::project_masters: []
profile::openstack::main::cumin::project_pub_key: undef
profile::openstack::base::keystone::auth_protocol: http
profile::openstack::base::keystone::public_port: 5000
profile::openstack::main::nova::dhcp_domain: 'eqiad.wmflabs'

# /> Cloud Services
#

# List of all zookeeper clusters in production.
zookeeper_clusters:
  main-eqiad:
    hosts:
      conf1004.eqiad.wmnet: '1104'
      conf1005.eqiad.wmnet: '1105'
      conf1006.eqiad.wmnet: '1106'

  main-codfw:
    hosts:
      conf2001.codfw.wmnet: '2001'
      conf2002.codfw.wmnet: '2002'
      conf2003.codfw.wmnet: '2003'

  # ZK cluster for Druid analytics-eqiad cluster (non public),
  # colocated on druid hosts.
  druid-analytics-eqiad:
    hosts:
      druid1001.eqiad.wmnet: '1001'
      druid1002.eqiad.wmnet: '1002'
      druid1003.eqiad.wmnet: '1003'

  # ZK cluster for Druid public-eqiad cluster, (for AQS, wikistats, etc.)
  # colocated on druid hosts.
  druid-public-eqiad:
    hosts:
      druid1004.eqiad.wmnet: '1004'
      druid1005.eqiad.wmnet: '1005'
      druid1006.eqiad.wmnet: '1006'

  # ZK cluster for Druid analytics-test-eqiad cluster (non public),
  # colocated on druid hosts. This is mainly used as testing
  # environment for the Hadoop Test cluster jobs.
  druid-analytics-test-eqiad:
    hosts:
      analytics1041.eqiad.wmnet: '1001'

# Used to sync the setting between all Kafka clusters and clients.
kafka_message_max_bytes: 4194304

kafka_clusters:
  # This is the analytics Kafka cluster, named just 'eqiad' for
  # historical reasons.
  eqiad:
    # Optional api_version indicates the Kafka API version the
    # brokers are running.  Clients can use this to override
    # version discovery for versions of Kafka where the version
    # request API doesn't exist (< 0.10).  Once all brokers
    # are on 0.10, this shouldn't be needed.
    # Note: kafka1023 holds the '18' broker id after T181518
    # since kafka1018 is not available anymore (hw failures).
    api_version: 0.9
    zookeeper_cluster_name: main-eqiad
    brokers:
      kafka1012.eqiad.wmnet:
        id: 12  # Row A
      kafka1013.eqiad.wmnet:
        id: 13  # Row A
      kafka1014.eqiad.wmnet:
        id: 14  # Row C
      kafka1023.eqiad.wmnet:
        id: 18  # Row D
      kafka1020.eqiad.wmnet:
        id: 20  # Row D
      kafka1022.eqiad.wmnet:
        id: 22  # Row C

  main-eqiad:
    zookeeper_cluster_name: main-eqiad
    brokers:
      kafka1001.eqiad.wmnet:
        id: 1001
        rack: A
      kafka1002.eqiad.wmnet:
        id: 1002
        rack: B
      kafka1003.eqiad.wmnet:
        id: 1003
        rack: C

  main-codfw:
    zookeeper_cluster_name: main-codfw
    brokers:
      kafka2001.codfw.wmnet:
        id: 2001
        rack: A
      kafka2002.codfw.wmnet:
        id: 2002
        rack: B
      kafka2003.codfw.wmnet:
        id: 2003
        rack: C

  # NOTE:  The 'rack' here is used by the confluent kafka module
  # to assign broker.rack for Kafka rack awareness.  We are actually setting
  # the row letter, not the full row-rack number, since each of these brokers
  # are in different racks anyway.  We do awareness at the row level.
  jumbo-eqiad:
    zookeeper_cluster_name: main-eqiad
    brokers:
      kafka-jumbo1001.eqiad.wmnet:
        id: 1001
        rack: A
      kafka-jumbo1002.eqiad.wmnet:
        id: 1002
        rack: A
      kafka-jumbo1003.eqiad.wmnet:
        id: 1003
        rack: B
      kafka-jumbo1004.eqiad.wmnet:
        id: 1004
        rack: C
      kafka-jumbo1005.eqiad.wmnet:
        id: 1005
        rack: C
      kafka-jumbo1006.eqiad.wmnet:
        id: 1006
        rack: D

  # Kafka clusters for logs, see also T206454
  logging-eqiad:
    zookeeper_cluster_name: main-eqiad
    brokers:
      logstash1004.eqiad.wmnet:
        id: 1004
        rack: A
      logstash1005.eqiad.wmnet:
        id: 1005
        rack: B
      logstash1006.eqiad.wmnet:
        id: 1006
        rack: D

  logging-codfw:
    zookeeper_cluster_name: main-codfw
    brokers:
      logstash2001.codfw.wmnet:
        id: 2001
        rack: A
      logstash2002.codfw.wmnet:
        id: 2002
        rack: C
      logstash2003.codfw.wmnet:
        id: 2003
        rack: D

# Hadoop base configuration is common to multiple profiles, and must be kept
# in sync. Instead of having it repated multiple times it is convenient to
# have a single place in hiera to check/modify.
hadoop_clusters:
  analytics-test-hadoop:
    zookeeper_cluster_name: main-eqiad
    resourcemanager_hosts:
      - analytics1028.eqiad.wmnet
      - analytics1029.eqiad.wmnet
    namenode_hosts:
      - analytics1028.eqiad.wmnet
      - analytics1029.eqiad.wmnet
    journalnode_hosts:
      - analytics1031.eqiad.wmnet
      - analytics1034.eqiad.wmnet
      - analytics1041.eqiad.wmnet
    # analytics* Dell R720s have mounts on disks sdb - sdm.
    # (sda is hardware raid on the 2 2.5 drives in the flex bays.)
    datanode_mounts:
      - /var/lib/hadoop/data/b
      - /var/lib/hadoop/data/c
      - /var/lib/hadoop/data/d
      - /var/lib/hadoop/data/e
      - /var/lib/hadoop/data/f
      - /var/lib/hadoop/data/g
      - /var/lib/hadoop/data/h
      - /var/lib/hadoop/data/i
      - /var/lib/hadoop/data/j
      - /var/lib/hadoop/data/k
      - /var/lib/hadoop/data/l
      - /var/lib/hadoop/data/m
    net_topology:
      analytics1028.eqiad.wmnet:  /eqiad/C/2
      analytics1029.eqiad.wmnet:  /eqiad/C/2
      analytics1031.eqiad.wmnet:  /eqiad/C/2
      analytics1032.eqiad.wmnet:  /eqiad/C/3
      analytics1033.eqiad.wmnet:  /eqiad/C/3
      analytics1034.eqiad.wmnet:  /eqiad/C/3
      analytics1035.eqiad.wmnet:  /eqiad/D/8
      analytics1036.eqiad.wmnet:  /eqiad/D/8
      analytics1037.eqiad.wmnet:  /eqiad/D/8
      analytics1038.eqiad.wmnet:  /eqiad/D/4
      analytics1040.eqiad.wmnet:  /eqiad/D/4
      analytics1041.eqiad.wmnet:  /eqiad/D/4

    datanode_volumes_failed_tolerated: 2
    mapreduce_reduce_shuffle_parallelcopies: 10
    mapreduce_task_io_sort_mb: 200
    mapreduce_task_io_sort_factor: 10

    mapreduce_map_memory_mb: 2048
    mapreduce_map_java_opts: '-Xmx1638m'  # 0.8 * 2G

    mapreduce_reduce_memory_mb: 4096         # 2 * 2G
    mapreduce_reduce_java_opts: '-Xmx3276m'  # 0.8 * 2 * 2G

    yarn_app_mapreduce_am_resource_mb: 4096          # 2 * 2G
    yarn_app_mapreduce_am_command_opts: '-Xmx3276m'  # 0.8 * 2 * 2G

    yarn_scheduler_minimum_allocation_mb: 0

    yarn_scheduler_maximum_allocation_mb: 53248
    yarn_scheduler_minimum_allocation_vcores:   0
    yarn_scheduler_maximum_allocation_vcores:   32
    yarn_resourcemanager_zk_timeout_ms: 20000

    # Yarn rmstore state stored on HDFS (rather than zookeeper)
    # T216952
    yarn_resourcemanager_fs_state_store_uri: '/user/yarn/rmstore'

    # Allow superset to impersonate users.
    # This is what allows superset to access files in
    # HDFS as users running Hive queries.
    # We only want to allow access from analytics-tool1003 (where superset runs)
    # and for users in specific groups.
    #core_site_extra_properties:
    #  hadoop.proxyuser.superset.hosts: analytics-tool1003.eqiad.wmnet
    #  hadoop.proxyuser.superset.groups: analytics-users,analytics-privatedata-users

  analytics-hadoop:
    zookeeper_cluster_name: main-eqiad
    resourcemanager_hosts:
      - an-master1001.eqiad.wmnet
      - an-master1002.eqiad.wmnet
    namenode_hosts:
      - an-master1001.eqiad.wmnet
      - an-master1002.eqiad.wmnet
    journalnode_hosts:
      - analytics1052.eqiad.wmnet  # Row A3
      - an-worker1078.eqiad.wmnet  # Row A2
      - analytics1072.eqiad.wmnet  # ROW B2
      - an-worker1090.eqiad.wmnet  # Row C4
      - analytics1069.eqiad.wmnet  # Row D8
    # analytics* Dell R720s have mounts on disks sdb - sdm.
    # (sda is hardware raid on the 2 2.5 drives in the flex bays.)
    datanode_mounts:
      - /var/lib/hadoop/data/b
      - /var/lib/hadoop/data/c
      - /var/lib/hadoop/data/d
      - /var/lib/hadoop/data/e
      - /var/lib/hadoop/data/f
      - /var/lib/hadoop/data/g
      - /var/lib/hadoop/data/h
      - /var/lib/hadoop/data/i
      - /var/lib/hadoop/data/j
      - /var/lib/hadoop/data/k
      - /var/lib/hadoop/data/l
      - /var/lib/hadoop/data/m
    net_topology:
      an-master1001.eqiad.wmnet:  /eqiad/A/5
      an-master1002.eqiad.wmnet:  /eqiad/B/8

      analytics1058.eqiad.wmnet:  /eqiad/A/1
      an-worker1078.eqiad.wmnet:  /eqiad/A/2
      an-worker1079.eqiad.wmnet:  /eqiad/A/2
      an-worker1080.eqiad.wmnet:  /eqiad/A/4
      analytics1052.eqiad.wmnet:  /eqiad/A/3
      analytics1053.eqiad.wmnet:  /eqiad/A/3
      analytics1054.eqiad.wmnet:  /eqiad/A/3
      analytics1055.eqiad.wmnet:  /eqiad/A/3
      analytics1056.eqiad.wmnet:  /eqiad/A/3
      analytics1057.eqiad.wmnet:  /eqiad/A/3
      analytics1059.eqiad.wmnet:  /eqiad/A/3
      analytics1060.eqiad.wmnet:  /eqiad/A/3
      analytics1070.eqiad.wmnet:  /eqiad/A/4
      analytics1071.eqiad.wmnet:  /eqiad/A/7
      an-worker1081.eqiad.wmnet:  /eqiad/A/7
      an-worker1082.eqiad.wmnet:  /eqiad/A/7

      analytics1072.eqiad.wmnet:  /eqiad/B/2
      an-worker1083.eqiad.wmnet:  /eqiad/B/2
      an-worker1084.eqiad.wmnet:  /eqiad/B/2
      analytics1046.eqiad.wmnet:  /eqiad/B/3
      analytics1047.eqiad.wmnet:  /eqiad/B/3
      analytics1048.eqiad.wmnet:  /eqiad/B/3
      analytics1049.eqiad.wmnet:  /eqiad/B/3
      analytics1050.eqiad.wmnet:  /eqiad/B/3
      analytics1051.eqiad.wmnet:  /eqiad/B/3
      an-worker1085.eqiad.wmnet:  /eqiad/B/4
      analytics1073.eqiad.wmnet:  /eqiad/B/7
      an-worker1086.eqiad.wmnet:  /eqiad/B/7
      an-worker1087.eqiad.wmnet:  /eqiad/B/7
      analytics1061.eqiad.wmnet:  /eqiad/B/8
      analytics1062.eqiad.wmnet:  /eqiad/B/8
      analytics1062.eqiad.wmnet:  /eqiad/B/8
      analytics1063.eqiad.wmnet:  /eqiad/B/8

      analytics1064.eqiad.wmnet:  /eqiad/C/2
      analytics1065.eqiad.wmnet:  /eqiad/C/2
      analytics1066.eqiad.wmnet:  /eqiad/C/2
      analytics1074.eqiad.wmnet:  /eqiad/C/2
      an-worker1089.eqiad.wmnet:  /eqiad/C/4
      an-worker1090.eqiad.wmnet:  /eqiad/C/4
      analytics1075.eqiad.wmnet:  /eqiad/C/7
      an-worker1091.eqiad.wmnet:  /eqiad/C/7

      analytics1042.eqiad.wmnet:  /eqiad/D/2
      analytics1043.eqiad.wmnet:  /eqiad/D/2
      analytics1044.eqiad.wmnet:  /eqiad/D/2
      analytics1045.eqiad.wmnet:  /eqiad/D/2
      analytics1067.eqiad.wmnet:  /eqiad/D/2
      analytics1068.eqiad.wmnet:  /eqiad/D/2
      analytics1076.eqiad.wmnet:  /eqiad/D/2
      an-worker1092.eqiad.wmnet:  /eqiad/D/2
      an-worker1093.eqiad.wmnet:  /eqiad/D/2
      analytics1077.eqiad.wmnet:  /eqiad/D/7
      an-worker1094.eqiad.wmnet:  /eqiad/D/7
      an-worker1095.eqiad.wmnet:  /eqiad/D/7
      analytics1069.eqiad.wmnet:  /eqiad/D/8

    # The datanode daemon by default begins the shutdown procedure as soon as
    # on volume/disk failure is registered. In our use case we want to keep the
    # datanode working in case of one/two (two is very unlikey on the same host)
    # disk failures.
    datanode_volumes_failed_tolerated: 2
    mapreduce_reduce_shuffle_parallelcopies: 10
    mapreduce_task_io_sort_mb: 200
    mapreduce_task_io_sort_factor: 10

    # Configure memory based on these recommendations and then adjusted:
    # http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.6.0/bk_installing_manually_book/content/rpm-chap1-11.html

    # These Map/Reduce and YARN ApplicationMaster master settings are
    # settable per job.
    # All worker nodes are currently Dell R720s with 64G of RAM and 24 cores
    # or R730dxs with 128G of RAM and 48 cores.

    # Choosing 2G for default application container size.
    # Map container size and JVM max heap size (-XmX)
    mapreduce_map_memory_mb: 2048
    mapreduce_map_java_opts: '-Xmx1638m'  # 0.8 * 2G

    # Reduce container size and JVM max heap size (-Xmx)
    mapreduce_reduce_memory_mb: 4096         # 2 * 2G
    mapreduce_reduce_java_opts: '-Xmx3276m'  # 0.8 * 2 * 2G

    # Yarn ApplicationMaster container size and  max heap size (-Xmx)
    yarn_app_mapreduce_am_resource_mb: 4096          # 2 * 2G
    yarn_app_mapreduce_am_command_opts: '-Xmx3276m'  # 0.8 * 2 * 2G

    # Save 12G for OS and other processes.
    # R720s will have 52G for YARN with room for 26 2G containers.
    # R730dxs will have 116G for YARN with room for 52 2G containers.
    # The value of profile::hadoop::common::yarn_nodemanager_resource_memory_mb is set in regex.yaml.

    # Setting minimum_allocations to 0 to allow Impala to submit small reservation requests.
    # TODO: do we need to set this anymore?  We don't use Impala.
    yarn_scheduler_minimum_allocation_mb: 0
    # Allow a job to request up to the smallest value of yarn_nodemanager_resource_memory_mb
    # in the cluster. The smallest value is 52G on the R720s (analytics1069 and below).
    yarn_scheduler_maximum_allocation_mb: 53248
    yarn_scheduler_minimum_allocation_vcores:   0
    yarn_scheduler_maximum_allocation_vcores:   32
    # Raised for T206943
    yarn_resourcemanager_zk_timeout_ms: 20000

    # The Analytics Hadoop cluster shares zookeeper with
    # a testing cluster, but it was created way before we realized
    # that the /rmstore zookeeper path needed to be separate for
    # each cluster (the HA logic for example, share the same root zk znode
    # but uses the cluster name to separate concerns). In order to avoid
    # any issue with the running cluster we keep the 'old' convention
    # (overriding the default of /rmstore-nameofthecluster).
    yarn_resourcemanager_zk_state_store_parent_path: '/rmstore'

    # Allow superset to impersonate users.
    # This is what allows superset to access files in
    # HDFS as users running Hive queries.
    # We only want to allow access from analytics-tool1003 (where superset runs)
    # and for users in specific groups.
    core_site_extra_properties:
      hadoop.proxyuser.superset.hosts: analytics-tool1003.eqiad.wmnet
      hadoop.proxyuser.superset.groups: analytics-users,analytics-privatedata-users

deployment_server: deploy1001.eqiad.wmnet

install_server: install1002.wikimedia.org
install_server_failover: install2002.wikimedia.org

netmon_server: netmon1002.wikimedia.org
netmon_server_failover: netmon2001.wikimedia.org

profile::netbox::netbox_server: netmon1002.wikimedia.org

releases_server: releases1001.eqiad.wmnet
releases_server_failover: releases2001.codfw.wmnet

phabricator_server: 'phab1001.eqiad.wmnet'
phabricator_server_failover: 'phab1002.eqiad.wmnet'

# Etcd client global configuration
etcd_client_srv_domain: "conftool.%{::site}.wmnet"
etcd_host: ''
etcd_port: ''

# Conftool global prefix (will be per-dc)
conftool_prefix: "/conftool/v1"


# Logging: logstash, udp2log
logstash_host: "logstash.svc.eqiad.wmnet"
logstash_syslog_port: 10514
logstash_gelf_port: 12201
# TCP json_lines input
logstash_json_lines_port: 11514
# UDP logback/json input
logstash_logback_port: 11514
udp2log_aggregator: "udplog:8420"

tcpircbot_host: 'icinga.wikimedia.org'
tcpircbot_port: 9200

# User for jenkins master-slave connections
jenkins_agent_username: 'jenkins-slave'

# HTTP proxy (can be used as an env variable)
http_proxy: "http://webproxy.%{::site}.wmnet:8080"

# This is the "live" authdns server set, which feeds into any other tooling
# that needs to operate on them (including themselves)
authdns_servers:
  - 'authdns1001.wikimedia.org'
  - 'authdns2001.wikimedia.org'
  - 'multatuli.wikimedia.org'

# acme-chief active host
acmechief_host: 'acmechief1001.eqiad.wmnet'

mail_smarthost:
- 'mx1001.wikimedia.org'
- 'mx2001.wikimedia.org'

wikimail_smarthost:
- 'wiki-mail-eqiad.wikimedia.org'
- 'wiki-mail-codfw.wikimedia.org'

# These are our servers - they all peer to each other and sync to upstream NTP
# pool servers.
ntp_peers:
    eqiad:
    - 'dns1001.wikimedia.org'
    - 'dns1002.wikimedia.org'
    codfw:
    - 'dns2001.wikimedia.org'
    - 'dns2002.wikimedia.org'
    esams:
    - 'nescio.wikimedia.org'    # esams recdns
    - 'maerlant.wikimedia.org'  # esams recdns
    ulsfo:
    - 'dns4001.wikimedia.org'
    - 'dns4002.wikimedia.org'
    eqsin:
    - 'dns5001.wikimedia.org'
    - 'dns5002.wikimedia.org'

# specified in the private repo
#k8s_infrastructure_users:
#  client-infrastructure:
#    type: infrastructure-readwrite
#    token: dummydummy
#    id: 1
#  kubelet:
#    type: system:node
#    token: dummydummydummy
#    id: 2
