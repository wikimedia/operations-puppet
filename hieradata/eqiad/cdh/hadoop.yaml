#
# Analytics Cluster Hadoop Configuration
#

cdh::hadoop::cluster_name: analytics-hadoop

cdh::hadoop::namenode_hosts:
    - analytics1001.eqiad.wmnet
    - analytics1002.eqiad.wmnet

cdh::hadoop::journalnode_hosts:
    - analytics1052.eqiad.wmnet  # Row A3
    - analytics1028.eqiad.wmnet  # Row C2
    - analytics1035.eqiad.wmnet  # Row D2

# analytics* Dell R720s have mounts on disks sdb - sdm.
# (sda is hardware raid on the 2 2.5 drives in the flex bays.)
cdh::hadoop::datanode_mounts:
    - /var/lib/hadoop/data/b
    - /var/lib/hadoop/data/c
    - /var/lib/hadoop/data/d
    - /var/lib/hadoop/data/e
    - /var/lib/hadoop/data/f
    - /var/lib/hadoop/data/g
    - /var/lib/hadoop/data/h
    - /var/lib/hadoop/data/i
    - /var/lib/hadoop/data/j
    - /var/lib/hadoop/data/k
    - /var/lib/hadoop/data/l
    - /var/lib/hadoop/data/m

cdh::hadoop::net_topology_script_template: 'role/analytics_cluster/hadoop/net-topology.py.erb'

# Increase NameNode heapsize independent from other daemons
cdh::hadoop::hadoop_namenode_opts: "-Xmx4096m"

cdh::hadoop::mapreduce_reduce_shuffle_parallelcopies: 10
cdh::hadoop::mapreduce_task_io_sort_mb: 200
cdh::hadoop::mapreduce_task_io_sort_factor: 10

# Configure memory based on these recommendations and then adjusted:
# http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.6.0/bk_installing_manually_book/content/rpm-chap1-11.html

# These Map/Reduce and YARN ApplicationMaster master settings are
# settable per job.
# All worker nodes are currently Dell R720s with 64G of RAM.

# Choosing 2G for default application container size.
# This is used as the base for the memory settings below.
# It may be possible to increase since all nodes now have
# 64G instead of 48G.
# See: https://phabricator.wikimedia.org/T118501
#

# Map container size and JVM max heap size (-XmX)
cdh::hadoop::mapreduce_map_memory_mb:                    2048
cdh::hadoop::mapreduce_map_java_opts:                    -Xmx1638m  # 0.8 * 2G

# Reduce container size and JVM max heap size (-Xmx)
cdh::hadoop::mapreduce_reduce_memory_mb:                 4096       # 2 * 2G
cdh::hadoop::mapreduce_reduce_java_opts:                 -Xmx3276m  # 0.8 * 2 * 2G

# This is the heap zize of YARN daemons ResourceManager and NodeManagers.
# This setting is used to configure the max heap size for both.
# The default is 1000m, we increase it to 2048m in production.
cdh::hadoop::yarn_heapsize:                              2048

# Heap size for HDFS datanode.
# The default is 1000m, we increase it to 2048m in production.
cdh::hadoop::hadoop_heapsize:                            2048

# Yarn ApplicationMaster container size and  max heap size (-Xmx)
cdh::hadoop::yarn_app_mapreduce_am_resource_mb:          4096      # 2 * 2G
cdh::hadoop::yarn_app_mapreduce_am_command_opts:         -Xmx3276m  # 0.8 * 2 * 2G

# Save 8G for OS and other processes.
# Memory available for use by Hadoop jobs:  64G - 8G == 56G.
# 56G at 2G per container gives nodes with 64G RAM space for
# 28 containers.

# This is the total amount of memory that NodeManagers
# will use for allocation to containers.
cdh::hadoop::yarn_nodemanager_resource_memory_mb:        57344      # 64G - 8G

# Setting _minimum_allocation_mb to 0 to allow Impala to submit small
# reservation requests.
cdh::hadoop::yarn_scheduler_minimum_allocation_mb:       0
cdh::hadoop::yarn_scheduler_maximum_allocation_mb:       57344
# Setting minimum_allocation_vcores to 0 to allow Impala to submit small
# reservation requests.
cdh::hadoop::yarn_scheduler_minimum_allocation_vcores:   0
