# Notify the Data Platform SRE team about services on these hosts
contactgroups: 'admins,team-data-platform'

nagios_group: analytics_eqiad
cluster: analytics
profile::admin::groups:
  - analytics-admins

profile::hadoop::common::hadoop_cluster_name: 'analytics-test-hadoop'

profile::hive::client::hive_service_name: 'analytics-test-hive'
profile::hive::client::deploy_jdbc_settings: true
profile::hive::client::hive_metastore_host: 'an-test-coord1001.eqiad.wmnet'

# Set the hive-site.xml file with group ownership 'analytics' so systemd timers
# can read the file.
profile::hive::client::config_files_group_ownership: 'analytics'

profile::analytics::cluster::hdfs_mount::monitoring_user: 'analytics'

# Hive Server Settings
profile::hive::server::ferm_srange: '$ANALYTICS_NETWORKS'
profile::hive::metastore::ferm_srange: '(($ANALYTICS_NETWORKS $DSE_KUBEPODS_NETWORKS))'

# Kerberos settings
profile::kerberos::client::prefer_tcp: true

# Presto Server settings
profile::presto::monitoring_enabled: true
profile::presto::cluster_name: analytics-test-presto
profile::presto::discovery_uri: https://analytics-test-presto.eqiad.wmnet:8281
profile::presto::server::heap_max: 4G

profile::presto::server::config_properties:
  coordinator: true
  # since we only run presto as a single node on the coordinator
  # on the test cluster, set this to allow the coordinator to
  # also handle queries.
  node-scheduler.include-coordinator: false
  discovery-server.enabled: true
  # Set network-topology to legacy since we are not (yet?) running
  # presto co-located with HDFS nodes. If we were, we would
  # set this to 'flat'.
  node-scheduler.network-topology: legacy
  query.max-memory: 2GB
  http-server.authentication.type: 'KERBEROS'
  http.server.authentication.krb5.service-name: 'presto'
  http.server.authentication.krb5.keytab: '/etc/security/keytabs/presto/presto.keytab'
  http.authentication.krb5.config: '/etc/krb5.conf'
  join-distribution-type: AUTOMATIC

profile::presto::server::catalogs:

  # Each catalog hash should contain a single properties has that will
  # end up being passed to the presto::catalog define.  This will render
  # a properties file at /etc/presto/catalog/$name.properties.
  analytics_test_hive:
    properties:
      connector.name: hive-hadoop2

      hive.metastore.uri: thrift://analytics-test-hive.eqiad.wmnet:9083
      hive.metastore.username: presto
      hive.metastore.authentication.type: 'KERBEROS'
      hive.metastore.client.keytab: '/etc/security/keytabs/presto/presto.keytab'
      hive.metastore.client.principal: 'presto/%{::fqdn}@WIKIMEDIA'
      hive.metastore.service.principal: 'hive/analytics-test-hive.eqiad.wmnet@WIKIMEDIA'

      # Allow presto to impersonate the user running the process to HDFS.
      hive.hdfs.impersonation.enabled: true
      hive.hdfs.authentication.type: 'KERBEROS'
      hive.hdfs.presto.principal: 'presto/%{::fqdn}@WIKIMEDIA'
      hive.hdfs.presto.keytab: '/etc/security/keytabs/presto/presto.keytab'
      hive.hdfs.wire-encryption.enabled: true

      # We do not (yet) colocate workers with Hadoop DataNodes.
      # When we tested 0.246 the option parser rejected setting the option to its
      # default, so we removed it but we left this comment for documentation purposes.
      #hive.force-local-scheduling: false
      # Add Hadoop config files so Hive connector can work with HA Hadoop NameNodes.
      hive.config.resources: /etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
      hive.security: read-only
      # TODO: do we want to disable non managed tables?
      hive.non-managed-table-writes-enabled: true
      hive.non-managed-table-creates-enabled: true
      # Attempt to enforce some sort of partition predicate,
      # by making the maximum partitions per scan the number of hours in 5 weeks.
      # This way at least for hourly data a month filter will need to be applied.
      hive.max-partitions-per-scan: 840
      hive.storage-format: PARQUET
      hive.compression-codec: SNAPPY
      # Use the parquet column names instead of the ordinals to order of query results. T321960
      hive.parquet.use-column-names: true
      # The following hive.copy-on-first-write-configuration-enabled option is a workaround for a bug affecting kerberized
      # Hive and HDFS. We may remove it when https://github.com/prestodb/presto/issues/18474 is fixed. See also #T337335
      hive.copy-on-first-write-configuration-enabled: false

      # Enable the use of the built-in alluxio cache for this catalog.
      # See https://prestodb.io/docs/current/cache/local.html and #T266641
      hive.node-selection-strategy: SOFT_AFFINITY
      cache.enabled: true
      cache.type: ALLUXIO
      cache.alluxio.max-cache-size: 5GB
      cache.base-directory: /tmp/alluxio-cache

  analytics_test_iceberg:
    properties:
      connector.name: iceberg
      # Discover iceberg tables in via Hive metastore.
      iceberg.catalog.type: hive

      hive.metastore.uri: thrift://analytics-test-hive.eqiad.wmnet:9083
      hive.metastore.username: presto
      hive.metastore.authentication.type: KERBEROS
      hive.metastore.client.keytab: /etc/security/keytabs/presto/presto.keytab
      hive.metastore.client.principal: 'presto/%{::fqdn}@WIKIMEDIA'
      hive.metastore.service.principal: 'hive/analytics-test-hive.eqiad.wmnet@WIKIMEDIA'

      # Allow presto to impersonate the user running the process to HDFS.
      hive.hdfs.impersonation.enabled: true
      hive.hdfs.authentication.type: 'KERBEROS'
      hive.hdfs.presto.keytab: '/etc/security/keytabs/presto/presto.keytab'
      hive.hdfs.presto.principal: 'presto/%{::fqdn}@WIKIMEDIA'
      hive.hdfs.wire-encryption.enabled: true

      # Add Hadoop config files so Iceberg hive connector can work with HA Hadoop NameNodes.
      hive.config.resources: /etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
      iceberg.file-format: PARQUET
      iceberg.compression-codec: SNAPPY

      # The following hive.copy-on-first-write-configuration-enabled option is a workaround for a bug affecting kerberized
      # Hive and HDFS. We may remove it when https://github.com/prestodb/presto/issues/18474 is fixed. See also #T337335
      hive.copy-on-first-write-configuration-enabled: false

profile::presto::server::ferm_srange: $ANALYTICS_NETWORKS

profile::presto::server::generate_certificate: true
profile::presto::server::ssl_certnames:
  - analytics-test-presto.eqiad.wmnet

profile::analytics::database::meta::monitoring_enabled: true
profile::analytics::database::meta::ferm_srange: '(($DRUID_PUBLIC_HOSTS $ANALYTICS_NETWORKS $STAGING_KUBEPODS_NETWORKS $DSE_KUBEPODS_NETWORKS))'
profile::analytics::database::meta::datadir: '/srv/mysql'
profile::analytics::refinery::job::data_purge::public_druid_host: 'an-test-druid1001.eqiad.wmnet'
profile::analytics::refinery::ensure_hdfs_dirs: true

# Upload the spark2-assembly.zip file to HDFS
profile::hadoop::spark2::install_assembly: true

# Http proxy to get project-namespace infos from API using python
profile::analytics::refinery::job::project_namespace_map::http_proxy: 'http://webproxy.eqiad.wmnet:8080'

# Kerberos config
profile::kerberos::keytabs::keytabs_metadata:
  - role: 'hive'
    owner: 'hive'
    group: 'hive'
    filename: 'hive.keytab'
  - role: 'analytics'
    owner: 'analytics'
    group: 'analytics'
    filename: 'analytics.keytab'
  - role: 'presto'
    owner: 'presto'
    group: 'presto'
    filename: 'presto.keytab'
  - role: 'hadoop'
    owner: 'hdfs'
    group: 'hdfs'
    filename: 'hdfs.keytab'
    parent_dir_grp: 'hadoop'

profile::monitoring::notifications_enabled: false

profile::java::java_packages:
  - version: '8'
    variant: 'jdk'
profile::java::extra_args:
  JAVA_TOOL_OPTIONS: "-Dfile.encoding=UTF-8"
profile::java::trust_puppet_ca: true

profile::debdeploy::client::exclude_mounts:
  - /mnt/hdfs

profile::hadoop::common::use_puppet_ssl_certs: true
profile::contacts::role_contacts: ['Data Platform']
profile::base::production::role_description: 'Analytics Cluster host running various Hadoop services (Hive and Presto) MariaDB instance'

profile::base::certificates::include_bundle_jks: true

# Context for the flag in https://phabricator.wikimedia.org/T276906
# The Hive daemons, now running a more up-to-date version of log4j,
# should not load the extra log4jv1 jars to avoid classpath loading issues
# (see https://issues.apache.org/jira/browse/BIGTOP-3619).
profile::hadoop::common::config_override:
  enable_log4j_extras: false

# Versioned gobblin-wmf shaded to use for gobblin ingestion jobs.
# Bump the version in the jar path here after deploying new versions of gobblin-wmf-core shaded jar.
profile::analytics::refinery::job::test::gobblin::gobblin_jar_file: /srv/deployment/analytics/refinery/artifacts/org/wikimedia/gobblin-wmf/gobblin-wmf-core-1.0.1-jar-with-dependencies.jar

# This parameter can be used to disable ingestion via gobblin temporarily,
# such as when Hadoop maintenance work is required
profile::analytics::refinery::job::test::gobblin::ensure_timers: present

# We need to prevent the removal of the python2 packages because of hive and hive-hcatalog
profile::base::remove_python2_on_bullseye: false

profile::puppet::agent::force_puppet7: true
acmechief_host: acmechief2002.codfw.wmnet

# Store historical data about spark jobs in HDFS
profile::hadoop::spark3::event_log_dir: hdfs:///var/log/spark
profile::hadoop::spark3::spark_yarn_history_address: spark-history-test.svc.eqiad.wmnet:30443

mariadb::package: wmf-mariadb106
