# Notify the Data Platform SRE team about services on these hosts
contactgroups: 'admins,team-data-platform'

profile::admin::groups:
  - analytics-admins
  - analytics-privatedata-users
profile::admin::managelingering: true
cluster: analytics

profile::hadoop::common::hadoop_cluster_name: 'analytics-test-hadoop'

profile::hive::client::hive_service_name: 'analytics-test-hive'

profile::analytics::cluster::hdfs_mount::monitoring_user: 'analytics'

# Kerberos config
profile::kerberos::keytabs::keytabs_metadata:
  - role: 'analytics'
    owner: 'analytics'
    group: 'analytics'
    filename: 'analytics.keytab'
  - role: 'analytics-search'
    owner: 'analytics-search'
    group: 'analytics-search'
    filename: 'analytics-search.keytab'
profile::kerberos::client::show_krb_ticket_info: true

# Context https://phabricator.wikimedia.org/T278353#6976509
profile::kerberos::client::dns_canonicalize_hostname: false

profile::kerberos::client::enable_autorenew: true

profile::debdeploy::client::exclude_mounts:
  - /mnt/hdfs

profile::java::java_packages:
  - version: '8'
    variant: 'jdk'
profile::java::extra_args:
  JAVA_TOOL_OPTIONS: "-Dfile.encoding=UTF-8"

profile::monitoring::notifications_enabled: false

profile::presto::cluster_name: analytics-test-presto
profile::presto::discovery_uri: https://analytics-test-presto.eqiad.wmnet:8281

#Airflow version override
profile::airflow::airflow_version: '2.10.2-py3.10-20241002'

# Set up airflow instances.
profile::airflow::instances:
  # airflow@analytics_test instance.
  analytics_test:
    # Since we set security: kerberos a keytab must be deployed for the service_user.
    service_user: analytics
    service_group: analytics
    monitoring_enabled: false
    statsd_monitoring_enabled: true
    airflow_config:
      datahub:
        enabled: False
        conn_id: datahub_kafka_test
        cluster: test
      metrics:
        statsd_on: True
    connections:
      analytics-test-hive:
        conn_type: hive_metastore
        host: analytics-test-hive.eqiad.wmnet
        port: 9083
        # Rename authMechanism to auth_mechanism
        extra_dejson: {'auth_mechanism': 'GSSAPI'}
      datahub_kafka_test:
        conn_type: datahub_kafka
        host: kafka-test1006.eqiad.wmnet:9092
        extra_dejson: {"connection": {"schema_registry_url": "https://datahub-gms-next.discovery.wmnet:30443/schema-registry/api/"}}

profile::airflow::database_host_default: an-db1001.eqiad.wmnet

profile::contacts::role_contacts: ['Data Platform']
profile::base::production::role_description: 'Analytics Hadoop test client'

profile::analytics::conda_analytics::remove_conda_env_pkgs_dir: false

# We need to prevent the removal of the python2 packages because of hive and hive-hcatalog
profile::base::remove_python2_on_bullseye: false

profile::puppet::agent::force_puppet7: true

# Store historical data about spark jobs in HDFS
profile::hadoop::spark3::event_log_dir: hdfs:///var/log/spark
profile::hadoop::spark3::spark_yarn_history_address: spark-history-test.svc.eqiad.wmnet:30443

profile::base::certificates::include_bundle_jks: true
