# Notify the Data Platform SRE team about services on these hosts
contactgroups: 'admins,team-data-platform'

nagios_group: analytics_eqiad
cluster: analytics

# enable a hardened Java security profile.
# This is needed here because we deploy TLS keys by setting ensure_ssl_config: true.
profile::java::hardened_tls: true

profile::admin::groups:
  - analytics-admins
profile::admin::groups_no_ssh:
  - analytics-privatedata-users
  # elasticsearch::analytics creates the analytics-search user and group
  # that analytics-search-users are allowed to sudo to.  This is used
  # for deploying files to HDFS.
  - analytics-search-users

# Testing Apache BigTop in T244499
profile::bigtop::apt::component: 'bigtop15'
profile::analytics::cluster::packages::common::use_bigtop_settings: true

profile::hadoop::common::hadoop_cluster_name: 'analytics-test-hadoop'

profile::hive::client::hive_service_name: 'analytics-test-hive'

profile::hadoop::common::datanode_mounts_prefix: '/srv/hadoop'

# Deploy TLS keys and xml configuration files
profile::hadoop::common::ensure_ssl_config: true

profile::hadoop::worker::monitoring_enabled: true

profile::hadoop::worker::ferm_srange: '(($ANALYTICS_NETWORKS $DRUID_PUBLIC_HOSTS $DSE_KUBEPODS_NETWORKS))'

profile::kerberos::keytabs::keytabs_metadata:
  - role: 'hadoop'
    owner: 'hdfs'
    group: 'hdfs'
    filename: 'hdfs.keytab'
    parent_dir_grp: 'hadoop'
  - role: 'hadoop'
    owner: 'yarn'
    group: 'yarn'
    filename: 'yarn.keytab'
    parent_dir_grp: 'hadoop'
  - role: 'hadoop'
    owner: 'hdfs'
    group: 'hdfs'
    filename: 'HTTP.keytab'
    parent_dir_grp: 'hadoop'

# Context https://phabricator.wikimedia.org/T278353#6976509
profile::kerberos::client::dns_canonicalize_hostname: false

profile::java::java_packages:
  - version: '8'
    variant: 'jdk'
profile::java::extra_args:
  JAVA_TOOL_OPTIONS: "-Dfile.encoding=UTF-8"

prometheus::node_exporter::collectors_extra:
  - meminfo_numa

profile::monitoring::notifications_enabled: false

profile::hadoop::common::use_puppet_ssl_certs: true

profile::hadoop::worker::check_mountpoints_disk_space: false
profile::contacts::role_contacts: ['Data Platform']
profile::base::production::role_description: 'Hadoop Worker (DataNode & NodeManager)'

profile::base::certificates::include_bundle_jks: true

# We need to prevent the removal of the python2 packages because of hive and hive-hcatalog
profile::base::remove_python2_on_bullseye: false

# Use the spark3 shuffler on the test_cluster
profile::hadoop::spark2::install_yarn_shuffle_jar: false
profile::hadoop::spark3::install_yarn_shuffle_jar: false

profile::puppet::agent::force_puppet7: true
acmechief_host: acmechief2002.codfw.wmnet

# Store historical data about spark jobs in HDFS
profile::hadoop::spark3::event_log_dir: hdfs:///var/log/spark
profile::hadoop::spark3::spark_yarn_history_address: spark-history-test.svc.eqiad.wmnet:30443
