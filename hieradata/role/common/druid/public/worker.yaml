# Notify the Data Engineering team about services on these hosts
contactgroups: 'admins,analytics'
mgmt_contactgroups: 'admins,analytics'

profile::admin::groups:
  - druid-admins

cluster: 'druid_public'

prometheus::node_exporter::collectors_extra:
  - meminfo_numa

# Avoid an explicit pin to the bigtop apt component that can
# force packages shared between the bigtop and debian upstream distribution
# to prefer the bigtop version (we do it only for Hadoop nodes).
profile::bigtop::apt::pin_release: false

profile::hadoop::common::hadoop_cluster_name: 'analytics-hadoop'

# Druid nodes get their own Zookeeper cluster to isolate them
# from the production ones.
# Configure the zookeeper profile.
profile::zookeeper::cluster_name: druid-public-eqiad
# To avoid version conflics with Cloudera zookeeper package, this
# class manually specifies which debian package version should be installed.
profile::zookeeper::zookeeper_version: '3.4.13-2'
profile::zookeeper::override_java_home: '/usr/lib/jvm/java-11-openjdk-amd64'

profile::zookeeper::firewall::srange: '$DRUID_PUBLIC_HOSTS'
profile::zookeeper::prometheus_instance: 'analytics'
profile::zookeeper::monitoring_enabled: true

profile::kerberos::keytabs::keytabs_metadata:
  - role: 'druid'
    owner: 'druid'
    group: 'druid'
    filename: 'druid.keytab'

profile::prometheus::druid_exporter::druid_version: '0.19.0'

# -- Druid common configuration

# The logical name of this druid cluster
profile::druid::common::druid_cluster_name: public-eqiad
# The logical name of the zookeeper cluster that druid should use
profile::druid::common::zookeeper_cluster_name: druid-public-eqiad

# Use this as the metadata storage database name in MySQL.
profile::druid::common::metadata_storage_database_name: 'druid_public_eqiad'

profile::druid::daemons_autoreload: false
profile::druid::ferm_srange: '$DRUID_PUBLIC_HOSTS'

profile::druid::common::properties:
  druid.hadoop.security.kerberos.principal: "druid/%{::fqdn}@WIKIMEDIA"
  druid.hadoop.security.kerberos.keytab: '/etc/security/keytabs/druid/druid.keytab'
  druid.metadata.storage.type: mysql
  druid.metadata.storage.connector.host: an-coord1001.eqiad.wmnet
  # druid.metadata.storage.connector.password is set in the private repo.
  druid.metadata.mysql.ssl.useSSL: true
  druid.metadata.mysql.ssl.enabledTLSProtocols: ["TLSv1.2"]
  druid.storage.type: hdfs
  druid.request.logging.type: slf4j
  druid.request.logging.dir: /var/log/druid
  # We need to use a special deep storage directory in HDFS so
  # we don't conflict with other (e.g. analytics-eqiad) druid
  # cluster deep storage.
  # NOTE: This directory is ensured to exist by usage of the
  # druid::bigtop::hadoop::deep_storage define included in the
  # role::analytics_cluster::hadoop::master class.
  druid.storage.storageDirectory: /user/druid/deep-storage-public-eqiad
  druid.extensions.loadList:
    - 'druid-datasketches'
    - 'druid-hdfs-storage'
    - 'druid-histogram'
    - 'druid-lookups-cached-global'
    - 'mysql-metadata-storage'
    - 'druid-parquet-extensions'
    - 'druid-avro-extensions'

# -- Druid worker service configurations

# --- Druid Broker

# Broker gets a special ferm_srange since it is the frontend query interface to Druid.
profile::druid::broker::monitoring_enabled: true
profile::druid::broker::ferm_srange: '$DOMAIN_NETWORKS'
profile::druid::broker::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://127.0.0.1:8000/"
  druid.processing.buffer.sizeBytes: 268435456 # 1024 * 1024 * 256
  # Set numMergeBuffers to use v2 groupBy engine
  druid.processing.numMergeBuffers: 10
  # https://druid.apache.org/docs/latest/operations/basic-cluster-tuning.html#connection-pool-sizing-1
  # "druid.server.http.numThreads on the Broker should be set to a value slightly
  # higher than druid.broker.http.numConnections on the same Broker."
  druid.server.http.numThreads: 25
  druid.broker.http.numConnections: 20
  # https://github.com/apache/druid/issues/325#issuecomment-32744317
  # [num of brokers] * broker:druid.processing.numThreads > historical:druid.server.http.numThreads
  druid.processing.numThreads: 25
  # To avoid excessive queueing on the Historicals,
  # the Broker fails fast if it doesn't receive any data
  # from the Historical after 5s (as opposed to minutes
  # like set in the defaults) or if a query takes more than
  # 5s to complete.
  druid.broker.http.readTimeout: PT5S
  druid.server.http.defaultQueryTimeout: 5000
  # https://druid.apache.org/docs/latest/operations/basic-cluster-tuning.html#broker-backpressure
  druid.broker.http.maxQueuedBytes: 10485760 # 5 * 2MB
  # https://druid.apache.org/docs/latest/querying/caching.html#query-caching-on-brokers
  # For clusters of 5+ nodes (and I suspect also big datasets with a lot of segments)
  # upstream suggests to avoid segment caching for brokers, since it will force Historicals
  # to skip merge/pre-processing before returning data to brokers. We had some performance
  # issues in the past due to historicals stopping/slowing-down for maintenance and brokers
  # piling up requests becoming unresponsive. See T270173
  # If you change any of these, please also remember to adjust (if needed) the related
  # settings for Historicals.
  druid.broker.cache.useCache: false
  druid.broker.cache.populateCache: false
  druid.broker.cache.useResultLevelCache: true
  druid.broker.cache.populateResultLevelCache: true
  # Increase druid broker query cache size to 2G.
  # TBD: Perhaps we should also try using memcached?
  druid.cache.sizeInBytes: 2147483648
  druid.sql.enable: true
  druid.monitoring.monitors: ["org.apache.druid.client.cache.CacheMonitor", "org.apache.druid.server.metrics.QueryCountStatsMonitor"]
profile::druid::broker::env:
  DRUID_HEAP_OPTS: "-Xmx10g -Xms5g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=4g -XX:MaxNewSize=4g -XX:MaxDirectMemorySize=12g -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager -Djava.io.tmpdir=/srv/druid/tmp"


# --- Druid Coordinator
profile::druid::coordinator::monitoring_enabled: true
profile::druid::coordinator::ferm_srange: '$DOMAIN_NETWORKS'
profile::druid::coordinator::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://127.0.0.1:8000/"
profile::druid::coordinator::env:
  DRUID_HEAP_OPTS: "-Xmx2g -Xms2g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=512m -XX:MaxNewSize=512m -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"


# --- Druid Historical
profile::druid::historical::monitoring_enabled: true
profile::druid::historical::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://127.0.0.1:8000/"
  druid.processing.buffer.sizeBytes: 268435456 # 1024 * 1024 * 256
  # Set numMergeBuffers to use v2 groupBy engine
  druid.processing.numMergeBuffers: 10
  # 20 Druid http client threads * 5 brokers + 20
  # The value 20 is the lowest among all the node hw types.
  # We need to pick the lowest since each historical needs to be able
  # to sustain, at worst, the maximum amount of concurrent reqs from all
  # brokers plus a safe margin.
  # If you change this, please also change the Broker's druid.processing.numThreads
  # setting as well.
  druid.server.http.numThreads: 120
  druid.server.maxSize: 2748779069440 # 2.5 TB
  druid.segmentCache.locations: '[{"path":"/srv/druid/segment-cache","maxSize"\:2748779069440}]'
  # https://druid.apache.org/docs/latest/querying/caching.html#query-caching-on-historicals
  # For clusters of 5+ nodes it is recommended to enable segment query caching on Historicals
  # and not on brokers.
  # If you change any of these, please also remember to adjust (if needed) the related
  # settings for Brokers.
  druid.historical.cache.useCache: true
  druid.historical.cache.populateCache: true
  druid.cache.sizeInBytes: 2147483648
  # Sane value to abort a query that takes more than 5s to complete
  druid.server.http.defaultQueryTimeout: 5000
  druid.monitoring.monitors: ["org.apache.druid.server.metrics.HistoricalMetricsMonitor", "org.apache.druid.server.metrics.QueryCountStatsMonitor", "org.apache.druid.client.cache.CacheMonitor"]
profile::druid::historical::env:
  DRUID_HEAP_OPTS: "-Xmx8g -Xms4g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=2g -XX:MaxNewSize=2g -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager -Djava.io.tmpdir=/srv/druid/tmp"


# --- Druid MiddleManager
profile::druid::middlemanager::monitoring_enabled: true
profile::druid::middlemanager::properties:
  druid.worker.ip: "%{::fqdn}"
  druid.worker.capacity: 12
  druid.processing.numThreads: 3
  druid.processing.buffer.sizeBytes: 356515840
  druid.server.http.numThreads: 20
  druid.indexer.runner.javaOpts: "-server -Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dhadoop.mapreduce.job.classloader=true"
profile::druid::middlemanager::env:
  DRUID_HEAP_OPTS: "-Xmx64m -Xms64m"
  DRUID_EXTRA_JVM_OPTS: "-XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Djava.io.tmpdir=/srv/druid/tmp"


# --- Druid Overlord

# Overlord will accept indexing jobs from Hadoop nodes in the ANALYTICS_NETWORKS
profile::druid::overlord::monitoring_enabled: true
profile::druid::overlord::ferm_srange: '(($DRUID_PUBLIC_HOSTS $ANALYTICS_NETWORKS))'
profile::druid::overlord::properties:
  druid.indexer.runner.type: remote
  druid.indexer.storage.type: metadata
profile::druid::overlord::env:
  DRUID_HEAP_OPTS: "-Xmx1g -Xms1g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=256m -XX:MaxNewSize=256m -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Djava.io.tmpdir=/srv/druid/tmp"

profile::java::java_packages:
  - version: '8'
    variant: 'jdk'
  - version: '11'
    variant: 'jdk'
profile::java::extra_args:
  JAVA_TOOL_OPTIONS: "-Dfile.encoding=UTF-8"
profile::contacts::role_contacts: ['Data Engineering']
