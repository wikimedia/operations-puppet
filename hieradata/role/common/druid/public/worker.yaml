admin::groups:
  - druid-admins

cluster: 'druid_public'

prometheus::node_exporter::collectors_extra:
  - meminfo_numa

# Avoid an explicit pin to the cloudera apt component that can
# force packages shared between the cdh and debian upstream distribution
# to prefer the cdh version (we do it only for Hadoop nodes).
profile::cdh::apt::pin_release: false

# Druid nodes get their own Zookeeper cluster to isolate them
# from the production ones.
# Configure the zookeeper profile.
profile::zookeeper::cluster_name: druid-public-eqiad
# To avoid version conflics with Cloudera zookeeper package, this
# class manually specifies which debian package version should be installed.
profile::zookeeper::zookeeper_version: '3.4.5+dfsg-2+deb8u2'
profile::zookeeper::firewall::srange: '$DRUID_PUBLIC_HOSTS'
profile::zookeeper::prometheus_instance: 'analytics'
profile::zookeeper::monitoring_enabled: true

# -- Druid common configuration

# The logical name of this druid cluster
profile::druid::common::druid_cluster_name: public-eqiad
# The logical name of the zookeeper cluster that druid should use
profile::druid::common::zookeeper_cluster_name: druid-public-eqiad

# Make druid build an extension composed of CDH jars.
profile::druid::common::use_cdh: true

profile::druid::common::java_home: '/usr/lib/jvm/java-1.8.0-openjdk-amd64'

# Use this as the metadata storage database name in MySQL.
profile::druid::common::metadata_storage_database_name: 'druid_public_eqiad'

profile::druid::daemons_autoreload: false
profile::druid::ferm_srange: '$DRUID_PUBLIC_HOSTS'

profile::druid::common::properties:
  druid.metadata.storage.type: mysql
  druid.metadata.storage.connector.host: analytics1003.eqiad.wmnet
  # druid.metadata.storage.connector.password is set in the private repo.
  druid.storage.type: hdfs
  druid.request.logging.type: file
  druid.request.logging.dir: /var/log/druid
  # We need to use a special deep storage directory in HDFS so
  # we don't conflict with other (e.g. analytics-eqiad) druid
  # cluster deep storage.
  # NOTE: This directory is ensured to exist by usage of the
  # druid::cdh::hadoop::deep_storage define included in the
  # role::analytics_cluster::hadoop::master class.
  druid.storage.storageDirectory: /user/druid/deep-storage-public-eqiad


# -- Druid worker service configurations

# --- Druid Broker

# Broker gets a special ferm_srange since it is the frontend query interface to Druid.
profile::druid::broker::monitoring_enabled: true
profile::druid::broker::ferm_srange: '$DOMAIN_NETWORKS'
profile::druid::broker::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://localhost:8000/"
  druid.processing.numThreads: 10
  druid.processing.buffer.sizeBytes: 268435456 # 1024 * 1024 * 256
  # Set numMergeBuffers to use v2 groupBy engine
  druid.processing.numMergeBuffers: 10
  druid.server.http.numThreads: 20
  druid.broker.http.numConnections: 20
  druid.broker.http.readTimeout: PT5M
  # Increase druid broker query cache size to 2G.
  # TBD: Perhaps we should also try using memcached?
  druid.cache.sizeInBytes: 2147483648
profile::druid::broker::env:
  DRUID_HEAP_OPTS: "-Xmx10g -Xms5g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=4g -XX:MaxNewSize=4g -XX:MaxDirectMemorySize=12g -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"


# --- Druid Coordinator
profile::druid::coordinator::monitoring_enabled: true
profile::druid::coordinator::ferm_srange: '$DOMAIN_NETWORKS'
profile::druid::coordinator::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://localhost:8000/"
profile::druid::coordinator::env:
  DRUID_HEAP_OPTS: "-Xmx4g -Xms2g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=512m -XX:MaxNewSize=512m -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"


# --- Druid Historical
profile::druid::historical::monitoring_enabled: true
profile::druid::historical::properties:
  druid.emitter: composing
  druid.emitter.composing.emitters: ["logging","http"]
  druid.emitter.http.recipientBaseUrl: "http://localhost:8000/"
  druid.processing.numThreads: 10
  druid.processing.buffer.sizeBytes: 268435456 # 1024 * 1024 * 256
  # Set numMergeBuffers to use v2 groupBy engine
  druid.processing.numMergeBuffers: 10
  druid.server.http.numThreads: 20
  druid.server.maxSize: 2748779069440 # 2.5 TB
  druid.segmentCache.locations: '[{"path":"/var/lib/druid/segment-cache","maxSize"\:2748779069440}]'
  # For small clusters it is reccomended to only enable caching on brokers
  # See: http://druid.io/docs/latest/querying/caching.html
  druid.historical.cache.useCache: false
  druid.historical.cache.populateCache: false
  druid.monitoring.monitors: ["io.druid.server.metrics.HistoricalMetricsMonitor"]
profile::druid::historical::env:
  DRUID_HEAP_OPTS: "-Xmx8g -Xms4g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=2g -XX:MaxNewSize=2g -XX:MaxDirectMemorySize=10g -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"


# --- Druid MiddleManager
profile::druid::middlemanager::monitoring_enabled: true
profile::druid::middlemanager::properties:
  druid.worker.ip: "%{::fqdn}"
  druid.worker.capacity: 12
  druid.processing.numThreads: 3
  druid.processing.buffer.sizeBytes: 356515840
  druid.server.http.numThreads: 20
  druid.indexer.runner.javaOpts: "-server -Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dhadoop.mapreduce.job.user.classpath.first=true"
  druid.indexer.task.defaultHadoopCoordinates: ["org.apache.hadoop:hadoop-client:cdh"]
profile::druid::middlemanager::env:
  DRUID_HEAP_OPTS: "-Xmx64m -Xms64m"
  DRUID_EXTRA_JVM_OPTS: "-XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"


# --- Druid Overlord

# Overlord will accept indexing jobs from Hadoop nodes in the ANALYTICS_NETWORKS
profile::druid::overlord::monitoring_enabled: true
profile::druid::overlord::ferm_srange: '(($DRUID_PUBLIC_HOSTS $ANALYTICS_NETWORKS))'
profile::druid::overlord::properties:
  druid.indexer.runner.type: remote
  druid.indexer.storage.type: metadata
profile::druid::overlord::env:
  DRUID_HEAP_OPTS: "-Xmx4g -Xms2g"
  DRUID_EXTRA_JVM_OPTS: "-XX:NewSize=256m -XX:MaxNewSize=256m -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"
