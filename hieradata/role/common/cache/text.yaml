cluster: cache_text
cache::cluster: text
cache::lua_support: true
admin::groups:
  - perf-roots
  - varnish-log-readers
prometheus::node_exporter::collectors_extra:
  - qdisc
  - meminfo_numa
mtail::ensure: 'stopped'
# The contents of this hash control our DC->DC routing for varnish backend
# daemons.  There should be a key for every cache datacenter.  The values must
# be a core datacenter (eqiad or codfw), or at least must lead indirectly to
# a core datacenter when traversing the table recursively.  A loop between
# the two core datacenters is expected and normal here.  The only reason to
# edit this is to remove a datacenter from active service (due to fault or
# maintenance) by routing around it from the edge sites.
#
cache::route_table:
  eqiad: 'codfw'
  codfw: 'eqiad'
  ulsfo: 'codfw'
  esams: 'eqiad'
cache::app_def_be_opts:
  port: 80
  connect_timeout: '3s'
  first_byte_timeout: '63s'
  between_bytes_timeout: '31s'
  max_connections: 1000
cache::app_directors:
  appservers:
    backends:
      eqiad: 'appservers.svc.eqiad.wmnet'
    #   codfw: 'appservers.svc.codfw.wmnet'
  api:
    backends:
      eqiad: 'api.svc.eqiad.wmnet'
    #   codfw: 'api.svc.codfw.wmnet'
  rendering:
    backends:
      eqiad: 'rendering.svc.eqiad.wmnet'
    #   codfw: 'rendering.svc.codfw.wmnet'
  security_audit:
    backends:
      eqiad: 'appservers.svc.eqiad.wmnet'
      # codfw: 'appservers.svc.codfw.wmnet'
  appservers_debug:
    be_opts:
      max_connections: 20
    backends:
      # eqiad: 'hassium.eqiad.wmnet'
      codfw: 'hassaleh.codfw.wmnet'
  restbase_backend:
    be_opts:
      port: 7231
      max_connections: 5000
    backends:
      eqiad: 'restbase.svc.eqiad.wmnet'
      #codfw: 'restbase.svc.codfw.wmnet'
  cxserver_backend:
    be_opts:
      port: 8080
    backends:
      eqiad: 'cxserver.svc.eqiad.wmnet'
      codfw: 'cxserver.svc.codfw.wmnet'
cache::req_handling:
  cxserver.wikimedia.org:
    director: 'cxserver_backend'
    caching: 'pass'
  default:
    director: 'appservers'
    debug_director: 'appservers_debug'
    subpaths:
      '^/api/rest_v1/':
        director: 'restbase_backend'
      '^/w/api\.php':
        director: 'api'
        debug_director: 'appservers_debug'
      '^/w/thumb(_handler)?\.php':
        director: 'rendering'
        debug_director: 'appservers_debug'
cache::fe_transient_gb: 5
cache::be_transient_gb: 2
# Profile::cache::base
profile::cache::base::zero_site: 'https://zero.wikimedia.org'
profile::cache::base::purge_host_only_upload_re: '^(upload|maps)\.wikimedia\.org$'
profile::cache::base::purge_host_not_upload_re: '^(?!(upload|maps)\.wikimedia\.org)'
profile::cache::base::storage_parts: ['sda3', 'sdb3']
profile::cache::base::purge_host_regex: ''
profile::cache::base::purge_multicasts: ['239.128.0.112']
profile::cache::base::purge_varnishes: ['127.0.0.1:3128', '127.0.0.1:3127']
profile::cache::base::fe_runtime_params:
    - default_ttl=86400
profile::cache::base::be_runtime_params:
    - default_ttl=86400
    - timeout_idle=120
    - nuke_limit=1000
    - lru_interval=31
# Profile::cache::ssl::unified
profile::cache::ssl::unified::monitoring: true
profile::cache::ssl::unified::letsencrypt: false

# Enable varnishkafka-webrequest instance monitoring.
profile::cache::kafka::webrequest::monitoring_enabled: true

# This should match an entry in the kafka_clusters hash (defined in common.yaml).
# We use the fully qualified kafka cluster name (with datacenter), because we want
# to route all statsv -> statsd traffic to the datacenter that hosts the master
# statsd instance.  If the active statsd instance changes to codfw (for an extended period of time)
# should probably change this to main-codfw.  If you don't things will probably be fine,
# but statsv will have to send messages over UDP cross-DC to the active statsd instance.
profile::cache::kafka::statsv::kafka_cluster_name: main-eqiad

# Monitor varnishkafka-eventlogging process.
profile::cache::kafka::eventlogging::monitoring_enabled: true
