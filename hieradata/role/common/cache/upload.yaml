cluster: cache_upload
cache::cluster: upload
cache::lua_support: true
base::microcode: true
admin::groups:
  - perf-roots
  - varnish-log-readers
prometheus::node_exporter::collectors_extra:
  - qdisc
  - meminfo_numa
mtail::ensure: 'stopped'
cache::tune_for_media: true
# The contents of this hash control our DC->DC routing for varnish backend
# daemons.  There should be a key for every cache datacenter.  The values must
# be a core datacenter (eqiad or codfw), or at least must lead indirectly to
# a core datacenter when traversing the table recursively.  A loop between
# the two core datacenters is expected and normal here.  The only reason to
# edit this is to remove a datacenter from active service (due to fault or
# maintenance) by routing around it from the edge sites.
#
cache::route_table:
  eqiad: 'codfw'
  codfw: 'eqiad'
  ulsfo: 'codfw'
  esams: 'eqiad'
  eqsin: 'codfw'
cache::app_def_be_opts:
  port: 80
  connect_timeout: '5s'
  first_byte_timeout: '35s'
  between_bytes_timeout: '60s'
  max_connections: 10000
cache::app_directors:
  swift:
    backends:
      eqiad: 'ms-fe.svc.eqiad.wmnet'
      # codfw: 'ms-fe.svc.codfw.wmnet'
  swift_thumbs:
    backends:
      eqiad: 'ms-fe-thumbs.svc.eqiad.wmnet'
      # codfw: 'ms-fe-thumbs.svc.codfw.wmnet'
  kartotherian:
    be_opts:
      port:               6533
      max_connections:    1000
    backends:
      eqiad: 'kartotherian.svc.eqiad.wmnet'
      codfw: 'kartotherian.svc.codfw.wmnet'
cache::req_handling:
  upload.wikimedia.org:
    director: 'swift'
    subpaths:
      '^/+[^/]+/[^/]+/thumb/':
        director: 'swift_thumbs'
  maps.wikimedia.org:
    director: 'kartotherian'
# Profile parameters
# Profile::cache::base
profile::cache::base::zero_site: 'https://zero.wikimedia.org'
profile::cache::base::purge_host_only_upload_re: '^(upload|maps)\.wikimedia\.org$'
profile::cache::base::purge_host_not_upload_re: '^(?!(upload|maps)\.wikimedia\.org)'
profile::cache::base::storage_parts: ['sda3', 'sdb3']
profile::cache::base::purge_host_regex: '[um][pa][lp][os]'
profile::cache::base::purge_multicasts: ['239.128.0.112', '239.128.0.113', '239.128.0.114']
profile::cache::base::purge_varnishes: ['127.0.0.1:3128', '127.0.0.1:3127']
profile::cache::base::fe_runtime_params:
    - default_ttl=86400
    - gethdr_extrachance=0
profile::cache::base::be_runtime_params:
    - default_ttl=86400
    - timeout_idle=120
    - nuke_limit=1000
    - lru_interval=31
    - gethdr_extrachance=0
# Profile::cache::ssl::unified
profile::cache::ssl::unified::monitoring: true
profile::cache::ssl::unified::letsencrypt: false

# Enable varnishkafka-webrequest instance monitoring.
profile::cache::kafka::webrequest::monitoring_enabled: true
profile::cache::kafka::webrequest::kafka_cluster_name: jumbo-eqiad
profile::cache::kafka::webrequest::ssl_enabled: true

profile::cache::base::varnish_version: 5
