# General configs
cluster: elasticsearch
standard::has_ganglia: false
admin::groups:
  - elasticsearch-roots

# More than 30G isn't very useful
profile::elasticsearch::heap_memory: '30G'

# Don't run if more than 1 master missing
profile::elasticsearch::minimum_master_nodes: 2

# wait that long for all nodes to restart. If not all nodes are present after
# `recover_after_time`, recover anyway, as long as at least
# `recover_after_nodes` are present.
profile::elasticsearch::recover_after_time: '5m'

# T130329
profile::base::check_disk_options: -w 18% -c 12% -W 6% -K 3% -l -e -A -i "/srv/sd[a-b][1-3]" --exclude-type=tracefs

profile::elasticsearch::awareness_attributes: 'row'

profile::elasticsearch::ferm_srange: '$DOMAIN_NETWORKS'

# mwgrep queries one copy of each shard in the cluster, which is currently just
# over 3k shards. For it to work we need to increase the limit from default 1k
profile::elasticsearch::search_shard_count_limit: 5000

# By default elasticsearch sets this to ((# of available_processors * 3) / 2) + 1,
# which works out to 49 for our servers. When search is stalled on IO it makes sense
# to have more threads than cores, but we never see our servers stalled out on IO.
# We have seen (T169498) servers completely exhaust their CPU though, so setting
# this to match cpu counts to prevent overcommit of CPU resources.
profile::elasticsearch::search_thread_pool_executors: 32

# Increase the per-node cache for compiled LTR models from default 10MB
profile::elasticsearch::ltr_cache_size: '100mb'
