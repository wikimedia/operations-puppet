# Notify the Data Platform SRE team about services on these hosts
contactgroups: 'admins,team-data-platform'

nagios_group: analytics_eqiad
cluster: analytics
profile::admin::groups:
  - analytics-admins
  # https://phabricator.wikimedia.org/T326827
  - analytics-platform-eng-admins

profile::hadoop::common::hadoop_cluster_name: 'analytics-hadoop'

profile::hive::client::hive_service_name: 'analytics-hive'

# Set the hive-site.xml file with group ownership 'analytics' so systemd timers
# can read the file.
profile::hive::client::config_files_group_ownership: 'analytics'

profile::analytics::cluster::hdfs_mount::monitoring_user: 'analytics'

profile::analytics::refinery::job::project_namespace_map::http_proxy: 'http://webproxy.eqiad.wmnet:8080'

profile::analytics::refinery::job::data_purge::public_druid_host: 'druid1007.eqiad.wmnet'

profile::statistics::base::enable_stat_host_addons: false
profile::statistics::base::mysql_credentials_group: 'analytics'

profile::debdeploy::client::exclude_mounts:
  - /mnt/hdfs

profile::kerberos::keytabs::keytabs_metadata:
  - role: 'analytics'
    owner: 'analytics'
    group: 'analytics'
    filename: 'analytics.keytab'
  - role: 'hadoop'
    owner: 'hdfs'
    group: 'hdfs'
    filename: 'hdfs.keytab'
    parent_dir_grp: 'hadoop'
  # https://phabricator.wikimedia.org/T326827
  - role: 'analytics-platform-eng'
    owner: 'analytics-platform-eng'
    group: 'analytics-platform-eng'
    filename: 'analytics-platform-eng.keytab'

# Context https://phabricator.wikimedia.org/T278353#6976509
profile::kerberos::client::dns_canonicalize_hostname: false

profile::java::java_packages:
  - version: '8'
    variant: 'jdk'
profile::java::extra_args:
  JAVA_TOOL_OPTIONS: "-Dfile.encoding=UTF-8"

profile::analytics::refinery::ensure_hdfs_dirs: true

profile::analytics::refinery::job::hdfs_cleaner::ensure_timer: present

# This parameter can be used to disable ingestion via gobblin temporarily,
# such as when Hadoop maintenance work is required
#profile::analytics::refinery::job::gobblin::ensure_timers: absent

# These parameters can be used to disable the refinery jobs temporarily,
# such as when Hadoop maintenance work is required
#profile::analytics::refinery::job::canary_events::ensure_timers: absent
#profile::analytics::refinery::job::refine::ensure_timers: absent
#profile::analytics::refinery::job::refine_sanitize::ensure_timers: absent
#profile::analytics::refinery::job::sqoop_mediawiki::ensure_timers: absent
#profile::analytics::refinery::job::import_wikidata_entities_dumps::ensure_timers: absent
#profile::analytics::refinery::job::import_mediawiki_dumps::ensure_timers: absent
#profile::analytics::refinery::job::import_commons_mediainfo_dumps::ensure_timers: absent
#profile::analytics::refinery::job::data_check::ensure_timers: absent
#profile::analytics::refinery::job::data_purge::ensure_timers: absent

profile::airflow::database_host_default: an-db1001.eqiad.wmnet

#Airflow version override
#profile::airflow::airflow_version: '2.9.3-py3.10-20240916'

# Set up airflow instances.
profile::airflow::instances:
  # airflow@analytics instance.
  analytics:
    firewall_srange: ANALYTICS_NETWORKS
    # Since we set security: kerberos a keytab must be deployed for the service_user.
    service_user: analytics
    service_group: analytics
    statsd_monitoring_enabled: true
    monitoring_enabled: true
    # Drop retention time from 90 to 60 days due to disk space issues. See #T370437
    clean_logs_older_than_days: 60
    airflow_config:
      datahub:
        enabled: False
        conn_id: datahub_kafka_jumbo
        cluster: prod
      metrics:
        statsd_on: True
    connections:
      analytics-hive:
        conn_type: hive_metastore
        host: analytics-hive.eqiad.wmnet
        port: 9083
        extra_dejson: {'authMechanism': 'GSSAPI'}
      datahub_kafka_jumbo:
        conn_type: datahub_kafka
        host: kafka-jumbo1007.eqiad.wmnet:9092
        extra_dejson: {"connection": {"schema_registry_url": "https://datahub-gms.discovery.wmnet:30443/schema-registry/api/"}}

profile::contacts::role_contacts: ['Data Platform']

profile::base::certificates::include_bundle_jks: true

profile::analytics::conda_analytics::remove_conda_env_pkgs_dir: false

# We need to retain python2 on bullseye because of hive and hive-hcatalog
profile::base::remove_python2_on_bullseye: false

profile::puppet::agent::force_puppet7: true
