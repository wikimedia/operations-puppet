# SPDX-License-Identifier: Apache-2.0
groups:
  - name: service_slis
    rules:
      - record: cluster_site:sli_etcd_http_error_ratio:increase90d
        expr: 100 * sum by (cluster, site) (increase(etcd_http_failed_total{code=~"5.."}[90d])) /
                    sum by (cluster, site) (increase(etcd_http_received_total[90d]))

      - record: cluster_site:sli_etcd_http_error_ratio:increase91d
        expr: 100 * sum by (cluster, site) (increase(etcd_http_failed_total{code=~"5.."}[91d])) /
                    sum by (cluster, site) (increase(etcd_http_received_total[91d]))

      - record: cluster_site:sli_etcd_http_error_ratio:increase92d
        expr: 100 * sum by (cluster, site) (increase(etcd_http_failed_total{code=~"5.."}[92d])) /
                    sum by (cluster, site) (increase(etcd_http_received_total[92d]))

  - name: service_slo_targets
    rules:
      - record: service:error_slo:fraction
        labels:
          service: "etcd"
        expr: 0.001

      # Deprecated: Use service:error_slo:fraction instead. Retained for continuity.
      - record: service:error_slo:percent
        labels:
          service: "etcd"
        expr: 0.1

  - name: traffic
    rules:
      - record: site_cluster:haproxy_requests:avail2m
        expr: sum by(site, cluster) (cluster_code:haproxy_frontend_http_responses_total:rate2m{code="5xx"})
          / sum by(site, cluster) (cluster_code:haproxy_frontend_http_responses_total:rate2m{code=~"[12345]xx"})

      - record: global_job:haproxy_requests:avail2m
        expr: sum by(cluster) (cluster_code:haproxy_frontend_http_responses_total:rate2m{code="5xx"})
          / sum by(cluster) (cluster_code:haproxy_frontend_http_responses_total:rate2m{code=~"[12345]xx"})

      - record: global_job:varnish_requests:avail2m
        expr: sum by(job) (job_method_status:varnish_requests:rate2m{status=~"5.."})
          / sum by(job) (job_method_status:varnish_requests:rate2m{status=~"[12345].."})

  # Aggregated exporter metrics.
  #
  # The "<service>_up" metrics are exported by Prometheus sidecar exporter (e.g. mysql-exporter,
  # when the service itself doesn't support Prometheus natively).
  #
  # Such metrics are used to catch conditions such as the following:
  # * the exporter itself is up and able to export metrics to Prometheus (therefore the "up" metric is 1)
  # * the exporter can't talk to the underlying service (e.g. mysql is down, thus "mysql_up" is 0)
  #
  # The label_replace function is used to attach a new "exporter" label to the aggregated metric.
  #
  # The following query will list all the "up style" metrics:
  #   count by (__name__) ({__name__=~".*_up$"})

  - name: exporters
    rules:
      - record: exporter:up:avail
        expr: label_replace(
          sum(mysql_up) without (instance) / count(mysql_up) without (instance),
          "exporter", "mysql",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(apache_up) without (instance) / count(apache_up) without (instance),
          "exporter", "apache",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(etherpad_up) without (instance) / count(etherpad_up) without (instance),
          "exporter", "etherpad",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(haproxy_up) without (instance) / count(haproxy_up) without (instance),
          "exporter", "haproxy",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(pg_up) without (instance) / count(pg_up) without (instance),
          "exporter", "postgres",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(varnish_up) without (instance) / count(varnish_up) without (instance),
          "exporter", "varnish",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(openldap_up) without (instance) / count(openldap_up) without (instance),
          "exporter", "openldap",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(memcached_up) without (instance) / count(memcached_up) without (instance),
          "exporter", "memcached",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(mcrouter_up) without (instance) / count(mcrouter_up) without (instance),
          "exporter", "mcrouter",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(phpfpm_up) without (instance) / count(phpfpm_up) without (instance),
          "exporter", "phpfpm",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(nutcracker_up) without (instance) / count(nutcracker_up) without (instance),
          "exporter", "nutcracker",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(redis_up) without (instance) / count(redis_up) without (instance),
          "exporter", "redis",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(elasticsearch_node_stats_up) without (instance) / count(elasticsearch_node_stats_up) without (instance),
          "exporter", "elasticsearch",
          "", ""
          )

      - record: exporter:up:avail
        expr: label_replace(
          sum(squid_up) without (instance) / count(squid_up) without (instance),
          "exporter", "squid",
          "", ""
          )
  - name: istio_slos
    rules:
      # istio_request_duration_milliseconds_{bucket,count} and istio_requests_total
      # are used for SLO dashboards for services behind an Istio Gateway.
      # Due to the number of metrics and the big number of labels provided by upstream
      # (see https://istio.io/latest/docs/reference/config/metrics/#labels)
      # the computation of recording rule can easily become slow and cause holes
      # in the final time series. For this reason we heavily limit the range of
      # values for specific labels.
      #
      # SLx: Buckets for latency
      - record: istio_sli_latency_request_duration_milliseconds_bucket:increase5m
        expr: sum by (destination_canonical_service, destination_service_namespace, le, response_code, site, prometheus) (increase(istio_request_duration_milliseconds_bucket{kubernetes_namespace="istio-system", le=~"(50|100|250|500|1000|2500|5000|10000|30000|\\+Inf)"}[5m]))
      # SLx: Total requests for latency.
      - record: istio_sli_latency_request_duration_milliseconds_count:increase5m
        expr: sum by (destination_canonical_service, destination_service_namespace, response_code, site, prometheus) (increase(istio_request_duration_milliseconds_count{kubernetes_namespace="istio-system"}[5m]))
      # SLx: Request counts with status
      - record: istio_sli_availability_requests_total:increase5m
        expr: sum by (destination_canonical_service, destination_service_namespace, response_code, site, prometheus) (increase(istio_requests_total{kubernetes_namespace="istio-system"}[5m]))
  - name: logstash_slos
    rules:
      # record boolean logstash availabilty metric for pyrra logstash-availability bool_gauge SLO
      - record: logstash_sli_availability:bool
        expr: increase(logstash_node_pipeline_events_filtered_total[1m]) > bool 1
