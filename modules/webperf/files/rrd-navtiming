#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
  rrd-navtiming
  ~~~~~~~~~~~~~

  This script provides simple, minimal and robust storage for a small
  set of time-series metrics. The idea is not to replace a full-fledged
  time-series database but to implement the bare minimum subset of
  features required to power a site like <https://status.github.com/>.

  rrd-navtiming subscribes to NavigationTiming events via EventLogging
  and it updates a pair of RRD files in its working directory: mobile.rrd
  and desktop.rrd. If the files do not exist, they are created.


  Usage:

    rrd-navtiming [--endpoint ENDPOINT] [--log-path LOG_PATH] RRD_FILE

      --endpoint ENDPOINT   EventLogging endpoint URI
                            (default: tcp://eventlogging.eqiad.wmnet:8600)

      --log-path LOG_PATH   Path to use for log files
                            (default: log to stderr only)

  Example:

    rrd-navtiming --endpoint tcp://eventlog1001.eqiad.wmnet:8600 \
                  /var/lib/rrd-navtiming/rrd-navtiming.rrd


  Requirements:

  * eventlogging
      https://github.com/wikimedia/mediawiki-extensions-EventLogging/
      (the Python module is in server/).

  * rrdtool
      http://oss.oetiker.ch/rrdtool/prog/rrdpython.en.html


  Futher reading:

  * https://meta.wikimedia.org/wiki/Schema:NavigationTiming
  * https://www.mediawiki.org/wiki/Extension:NavigationTiming


  Copyright 2015 Ori Livneh <ori@wikimedia.org>

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

"""
from __future__ import division

import sys
reload(sys)
sys.setdefaultencoding('utf-8')

import argparse
import bisect
import heapq
import logging
import os
import threading
import time

import eventlogging
import rrdtool


ap = argparse.ArgumentParser(description='Navigation Timing RRD logger')
ap.add_argument(
    'rrd',
    help='RRD file path',
    type=os.path.abspath
)
ap.add_argument(
    '--endpoint',
    help='Endpoint URI (default: tcp://eventlogging.eqiad.wmnet:8600)',
    default='tcp://eventlogging.eqiad.wmnet:8600'
)
ap.add_argument(
    '--log-path',
    help='Path to use for log files (default: log to stderr only).',
    type=os.path.abspath
)
args = ap.parse_args()

# Setup logging
log_handlers = [logging.StreamHandler(stream=sys.stderr)]
if args.log_path:
    log_handlers.append(logging.handlers.RotatingFileHandler(
        os.path.join(args.log_path, 'rrd-navtiming.log'), backupCount=10,
        maxBytes=(50 * 1000 * 1000)))  # Rotate when >50MB; retain 10 archives

log = logging.getLogger('rrd-navtiming')
formatter = logging.Formatter('[%(asctime)s] %(message)s')
log.setLevel(logging.INFO)
for log_handler in log_handlers:
    log_handler.setFormatter(formatter)
    log_handler.setLevel(logging.INFO)
    log.addHandler(log_handler)

log.info('Starting up...')

# Lock for operations that access shared dynamic data (the `heap` list).
# We could probably get by without locks, because only the main thread
# mutates the sample heap -- the worker thread only reads it.
lock = threading.Lock()

# A min-heap of of (timestamp, event) tuples.
heap = []

METRICS = (
    'responseStart',  # Time to user agent receiving first byte
    'firstPaint',     # Time to initial render
    'domComplete',    # Time to DOM Comlete event
    'loadEventEnd',   # Time to load event completion
)

# Size of sliding window, in seconds.
WINDOW_SIZE = 300

# Only push an update if we have at least this many samples.
SAMPLE_THRESHOLD = 500

# Aggregation intervals.
INTERVALS = (
    60 * 60,                # Hour
    60 * 60 * 24,           # Day
    60 * 60 * 24 * 7,       # Week
    60 * 60 * 24 * 30,      # Month
    60 * 60 * 24 * 365.25,  # Year
)

# Store 120 values at each resolution. This makes graphing simpler,
# because we're always working with a fixed number of points.
ROWS = 120

# We will push an aggregate value as often as we need in order to have
# ROWS many values at the smallest INTERVAL.
STEP = INTERVALS[0] / ROWS

# STEP should be a whole number that divides each INTERVAL.
assert STEP.is_integer()
for interval in INTERVALS:
    assert (interval / ROWS / STEP).is_integer()

# Set the maximum acceptable interval between samples ("heartbeat") to a
# full day. This means RRD will record an estimate for missing samples as
# long as it has at least one sample from the last 24h to go by. If we go
# longer than 24h without reporting a measurement, RRD will record a value
# of UNKNOWN instead.
HEARTBEAT = 60 * 60 * 24

# The expected range for measurements is 0 - 60,000 milliseconds.
MIN, MAX = 0, 60 * 1000

# List of data source ('DS') definitions in the format expected by
# rrdcreate (<http://oss.oetiker.ch/rrdtool/doc/rrdcreate.en.html>).
SOURCES = ['DS:%s:GAUGE:%d:%d:%d' % (metric, HEARTBEAT, MIN, MAX)
           for metric in METRICS]

# List of round-robin archive ('RRA') definitions in rrdcreate format.
ARCHIVES = ['RRA:AVERAGE:0.5:%d:%d' % (interval / ROWS / STEP, ROWS)
            for interval in INTERVALS]


def prune_heap(event_heap):
    # Prune old entries.
    cutoff = time.time() - WINDOW_SIZE
    with lock:
        # Python's heapq is a min heap, meaning heap[0] is always
        # the oldest entry.
        while event_heap and event_heap[0][0] < cutoff:
            heapq.heappop(event_heap)


def update_rrd(rrd_file, event_heap):
    """Push updates to RRD."""
    prune_heap(event_heap)
    if len(event_heap) < SAMPLE_THRESHOLD:
        log.warn('Fewer than SAMPLE_THRESHOLD (%s) events; '
                 'skipping update.', SAMPLE_THRESHOLD)
        return

    with lock:
        data = accumulate(event_heap)

    if not all(metric in data for metric in METRICS):
        # We have to give RRD a full update or nothing, so move on.
        log.info('Skipping update due to empty metrics: %s',
                 [m for m in METRICS if m not in data])
        return

    # We have to output values in the order we declared them.
    update = 'N:' + ':'.join(str(data[m]) for m in METRICS)
    log.info('%s: %s', rrd_file, update)
    rrdtool.update(rrd_file, update)


def median(sorted_list):
    """Compute the median of a sorted list."""
    if not sorted_list:
        raise ValueError('Cannot compute median of empty list.')
    length = len(sorted_list)
    index = (length - 1) // 2
    if length % 2:
        return sorted_list[index]
    sum_of_terms = sorted_list[index] + sorted_list[index + 1]
    return sum_of_terms / 2.0


def accumulate(event_heap):
    """Group samples by metric and compute medians."""
    data = {metric: [] for metric in METRICS}
    for timestamp, event in event_heap:
        for metric in METRICS:
            value = event.get(metric)
            if value:
                bisect.insort(data[metric], value)
    return {m: median(vs) for m, vs in data.items() if vs}


def create_rrd(rrd_file):
    args = [
        rrd_file,
        '--no-overwrite',
        '--step', '%d' % STEP,
        '--start', 'N'
    ] + SOURCES + ARCHIVES
    try:
        rrdtool.create(*args)
        log.info('Created %s', rrd_file)
    except Exception as e:
        if 'File exists' not in e.message:
            raise
        log.info('RRD file %s already exists.', rrd_file)


create_rrd(args.rrd)
log.info('Connecting to <%s>...', args.endpoint)
events = eventlogging.connect(args.endpoint)

worker = eventlogging.PeriodicThread(interval=STEP, target=update_rrd,
                                     args=(args.rrd, heap))
worker.daemon = True
worker.start()

for meta in events.filter(schema='NavigationTiming'):
    sample = meta['timestamp'], meta['event']
    with lock:
        heapq.heappush(heap, sample)
