# NOTE: This file is managed by Puppet.

############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=<%= @id %>

# Always require a static broker id.
broker.id.generation.enable=false

<% if @rack -%>
broker.rack=<%= @rack %>

<% end -%>
<% if @listeners -%>
listeners=<%= Array(@listeners).join(',') %>
<% else -%>
listeners=PLAINTEXT://:9092

<% end -%>
<% if @inter_broker_protocol_version -%>
# Specify which version of the inter-broker protocol will be used. This is
# typically bumped after all brokers were upgraded to a new version.
inter.broker.protocol.version=<%= @inter_broker_protocol_version %>

<% end -%>
<% if @log_message_format_version -%>
# Specify the message format version the broker will use to append messages to the logs.
# By setting a particular message format version, the user is certifying that all the
# existing messages on disk are smaller or equal than the specified version.
log.message.format.version=<%= @log_message_format_version %>

<% end -%>
<% if @log_message_timestamp_type -%>
# Define whether the timestamp in the message is message create time or log append time.
# The value should be either `CreateTime` or `LogAppendTime`
log.message.timestamp.type=<%= @log_message_timestamp_type %>

<% end -%>
<% if @message_max_bytes -%>
message.max.bytes=<%= @message_max_bytes %>
replica.fetch.max.bytes=<%= @message_max_bytes %>
<% else -%>
<% if @replica_fetch_max_bytes -%>
replica.fetch.max.bytes=<%= @replica_fetch_max_bytes %>
<% end -%>
<% end -%>
<%if @authorizer_class_name -%>
######################### ACL handling ##################################
authorizer.class.name=<%= @authorizer_class_name %>
<%if @allow_everyone_if_no_acl_found -%>
allow.everyone.if.no.acl.found=<%= @allow_everyone_if_no_acl_found %>
<% end -%>

<% if @super_users -%>
super.users=<%= Array(@super_users).join(';') %>
<% end -%>

<% end -%>

######################### Socket Server Settings ########################
<% if @security_inter_broker_protocol -%>
security.inter.broker.protocol=<%= @security_inter_broker_protocol %>

<% end -%>
<% if @ssl_keystore_location -%>
ssl.keystore.location=<%= @ssl_keystore_location %>
<% end -%>
<% if @ssl_keystore_password -%>
ssl.keystore.password=<%= @ssl_keystore_password %>
<% end -%>
<% if @ssl_key_password -%>
ssl.key.password=<%= @ssl_key_password %>
<% end -%>
<% if @ssl_truststore_location -%>
ssl.truststore.location=<%= @ssl_truststore_location %>
<% end -%>
<% if @ssl_truststore_password -%>
ssl.truststore.password=<%= @ssl_truststore_password %>
<% end -%>

<% if @ssl_client_auth -%>
ssl.client.auth=<%= @ssl_client_auth %>

<% end -%>
<% if @num_network_threads -%>
# The number of threads handling network requests
num.network.threads=<%= @num_network_threads %>

<% end -%>
<% if @num_io_threads -%>
# The number of threads doing disk I/O
num.io.threads=<%= @num_io_threads %>

<% end -%>
<% if @socket_send_buffer_bytes -%>
# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=<%= @socket_send_buffer_bytes %>

<% end -%>
<% if @socket_receive_buffer_bytes -%>
# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=<%= @socket_receive_buffer_bytes %>

<% end -%>
<% if @socket_request_max_bytes -%>
# The maximum size of a request that the socket server will accept
# (protection against OOM)
socket.request.max.bytes=<%= @socket_request_max_bytes %>

<% end -%>
############################# Log Basics #############################

# A comma seperated list of directories under which to store log files
log.dirs=<%= Array(@log_dirs).join(',') %>

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=<%= @num_partitions %>

# The default replication factor for automatically created topics.
# Default to the number of brokers in this cluster.
default.replication.factor=<%= @default_replication_factor %>

<% if @delete_topic_enable -%>
# Enables topic deletion
delete.topic.enable=<%= @delete_topic_enable %>
<% end -%>

<% if @offsets_topic_replication_factor -%>
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
offsets.topic.replication.factor=<%= @offsets_topic_replication_factor %>

<% end -%>
# When a producer sets acks to "all" (or "-1"), min.insync.replicas specifies the minimum number of
# replicas that must acknowledge a write for the write to be considered successful. If this minimum
# cannot be met, then the producer will raise an exception (either NotEnoughReplicas or
# NotEnoughReplicasAfterAppend). When used together, min.insync.replicas and acks allow you to
# enforce greater durability guarantees. A typical scenario would be to create a topic with a
# replication factor of 3, set min.insync.replicas to 2, and produce with acks of "all". This will
# ensure that the producer raises an exception if a majority of replicas do not receive a write.
min.insync.replicas=<%= @min_insync_replicas %>

# Enable auto creation of topic on the server. If this is set to true
# then attempts to produce, consume, or fetch metadata for a non-existent
# topic will automatically create it with the default replication factor
# and number of partitions.
auto.create.topics.enable=<%= @auto_create_topics_enable %>

# If this is enabled the controller will automatically try to balance
# leadership for partitions among the brokers by periodically returning
# leadership to the "preferred" replica for each partition if it is available.
auto.leader.rebalance.enable=<%= @auto_leader_rebalance_enable %>

<% if @replica_lag_time_max_ms -%>
# If a follower hasn't sent any fetch requests for this window of time,
# the leader will remove the follower from ISR and treat it as dead.
replica.lag.time.max.ms=<%= @replica_lag_time_max_ms %>

<% end -%>
<% if @num_recovery_threads_per_data_dir -%>
# The number of threads per data directory to be used for log recovery at
# startup and flushing at shutdown. This value is recommended to be increased
# for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=<%= @num_recovery_threads_per_data_dir %>

<% end -%>
<% if @replica_socket_timeout_ms -%>
# The socket timeout for network requests to the leader for replicating data.
replica.socket.timeout.ms=<%= @replica_socket_timeout_ms %>

<% end -%>
<% if @replica_socket_receive_buffer_bytes -%>
# The socket receive buffer for network requests to the leader for replicating data.
replica.socket.receive.buffer.bytes=<%= @replica_socket_receive_buffer_bytes %>

<% end -%>
# Number of threads used to replicate messages from leaders. Increasing this
# value can increase the degree of I/O parallelism in the follower broker.
# This is useful to temporarily increase if you have a broker that needs
# to catch up on messages to get back into the ISR.
num.replica.fetchers=<%= @num_replica_fetchers %>

<% if @log_flush_interval_messages or @log_flush_interval_ms -%>
############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

<% if @log_flush_interval_messages -%>
# The number of messages to accept before forcing a flush of data to disk
log.flush.interval.messages=<%= @log_flush_interval_messages %>

<% end -%>
<% if @log_flush_interval_ms -%>
# The maximum time in ms that a message in any topic is kept in memory before
# flushed to disk. If not set, the value in log.flush.scheduler.interval.ms
# is used.
log.flush.interval.ms=<%= @log_flush_interval_ms %>

<% end -%>
<% end # if any Log Flush Policy settings -%>
<% if (@log_retention_hours or
       @log_retention_bytes or
       @log_segment_bytes or
       @log_retention_check_interval_ms or
       @log_cleanup_policy or
       @offsets_retention_minutes) -%>
############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy
# can be set to delete segments after a period of time, or after a given size
# has accumulated. A segment will be deleted whenever *either* of these
# criteria are met. Deletion always happens from the end of the log.

<% if @log_retention_hours -%>
# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=<%= @log_retention_hours %>

<% end -%>
<% if @log_retention_bytes -%>
# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
# segments don't drop below log.retention.bytes. Functions independently of log.retention.hours.
log.retention.bytes=<%= @log_retention_bytes %>

<% end -%>
<% if @log_segment_bytes -%>
# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=<%= @log_segment_bytes %>

<% end -%>
<% if @log_retention_check_interval_ms -%>
# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=<%= @log_retention_check_interval_ms %>

<% end -%>
<% if @log_cleanup_policy -%>
# The default cleanup policy for segments beyond the retention window,
# must be either "delete" or "compact"
log.cleanup.policy=<%= @log_cleanup_policy %>

<% end -%>
<% if @offsets_retention_minutes -%>
# Log retention window in minutes for offsets topic.  If an offset
# commit for a consumer group has not been recieved in this amount of
# time, Kafka will drop the offset commit and consumers in the group
# will have to start a new.  This can be overridden in an offset commit
# request.
offsets.retention.minutes=<%= @offsets_retention_minutes %>

<% end -%>
<% end # if any Log Retention Policy settings -%>
############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=<%= @zookeeper_connect %>

<% if @zookeeper_connection_timeout_ms -%>
# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=<%= @zookeeper_connection_timeout_ms %>

<% end -%>
<% if @zookeeper_session_timeout_ms -%>
# Zookeeper session timeout. If the server fails to heartbeat to Zookeeper
# within this period of time it is considered dead. If you set this too low
# the server may be falsely considered dead; if you set it too high it may
# take too long to recognize a truly dead server.
zookeeper.session.timeout.ms=<%= @zookeeper_session_timeout_ms %>

<% end -%>
##################### Confluent Proactive Support ######################
# If set to true, and confluent-support-metrics package is installed
# then the feature to collect and report support metrics
confluent.support.metrics.enable=false

# The customer ID under which support metrics will be collected and
# reported.
#
# When the customer ID is set to "anonymous" (the default), then only a
# reduced set of metrics is being collected and reported.
#
# Confluent customers
# -------------------
# If you are a Confluent customer, then you should replace the default
# value with your actual Confluent customer ID.  Doing so will ensure
# that additional support metrics will be collected and reported.
#
confluent.support.customer.id=anonymous
