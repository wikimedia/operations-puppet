# == Class: eventlogging::monitoring::graphite
#
# Provisions a Graphite check for sudden fluctuations in the volume
# of incoming events.
# == Parameters:
# $kafka_brokers_graphite_wildcard - graphite wildcard to match kafka brokers
#                                    from metrics generated by jmxtrans from
#                                    the kafka puppet moduleÂ class
#                                    kafka::server::jmxtrans
#
class eventlogging::monitoring::graphite($kafka_brokers_graphite_wildcard) {
    $raw_events_rate_metric   = "sumSeries(kafka.${kafka_brokers_graphite_wildcard}.kafka.server.BrokerTopicMetrics.MessagesInPerSec.{eventlogging-client-side,eventlogging-server-side}.OneMinuteRate)"
    $valid_events_rate_metric = "sumSeries(kafka.${kafka_brokers_graphite_wildcard}.kafka.server.BrokerTopicMetrics.MessagesInPerSec.eventlogging_*.OneMinuteRate)"

    # Warn if 15% of overall event throughput goes beyond 500 events/s
    # in a 15 min period
    # These thresholds are somewhat arbtirary at this point, but it
    # was seen that the current setup can handle 500 events/s.
    # Better thresholds are pending (see T86244).
    monitoring::graphite_threshold { 'eventlogging_throughput':
        description   => 'Throughput of event logging events',
        metric        => $raw_events_rate_metric,
        warning       => 500,
        critical      => 600,
        percentage    => 15, # At least 3 of the 15 readings
        from          => '15min',
        contact_group => 'analytics'
    }

    # Alarms if 15% of Navigation Timing event throughput goes under 1 req/sec
    # in a 15 min period
    # https://meta.wikimedia.org/wiki/Schema:NavigationTiming
    monitoring::graphite_threshold { 'eventlogging_NavigationTiming_throughput':
        description   => 'Throughput of event logging NavigationTiming events',
        metric        => "kafka.${kafka_brokers_graphite_wildcard}.kafka.server.BrokerTopicMetrics.MessagesInPerSec.eventlogging_NavigationTiming.OneMinuteRate",
        warning       => 1,
        critical      => 0,
        percentage    => 15, # At least 3 of the 15 readings
        from          => '15min',
        contact_group => 'analytics',
        under         => true
    }

    # Warn/Alert if the difference between raw and valid EventLogging
    # alerts gets too big.  We put a 10 minute lag because of metrics
    # not being correct in graphite before.
    # If the difference gets too big, either the validation step is
    # overloaded, or high volume schemas are failing validation.
    monitoring::graphite_threshold { 'eventlogging_difference_raw_validated':
        description   => 'Difference between raw and validated EventLogging overall message rates',
        metric        => "absolute(diffSeries(${raw_events_rate_metric},${valid_events_rate_metric}))",
        warning       => 20,
        critical      => 30,
        percentage    => 20, # At least 3 of the (25 - 10) = 15 readings
        from          => '25min',
        until         => '10min',
        contact_group => 'analytics',
    }


    # Warn/Alert if the db inserts of EventLogging data have dropped dramatically
    # Since the MySQL consumer is at the bottom of the pipeline
    # this metric is a good proxy to make sure events are flowing through the
    # kafka pipeline
    monitoring::graphite_threshold { 'eventlogging_overall_inserted_rate':
        description   => 'Overall insertion rate from MySQL consumer',
        metric        => 'eventlogging.overall.inserted.rate',
        warning       => 100,
        critical      => 10,
        percentage    => 20, # At least 3 of the (25 - 10) = 15 readings
        from          => '25min',
        until         => '10min',
        contact_group => 'analytics',
        under         => true
    }
}
