# SPDX-License-Identifier: Apache-2.0
This document outlines what is needed to test a new nfs share for dumps when it is made available as a dumps spare.

PREPARATION

* Make sure the new share has the role dumps::generation::server::spare, and check that the nfsd and lockd ports it is using are correct
* Go to the snapshot testbed, become the <%= @user -%> user, and cd into the nfs_testing directory. Presumably you
  are there since you are reading this file.
* As root, mount the nfs share. Sample command:
  mount -t nfs -o bg,hard,rw,nfsvers=3,rsize=8192,wsize=8192,proto=tcp,actimeo=0 dumpsdata10XX.eqiad.wmnet:/data <%= @mountpoint %>
* After the mount is successful, manually create <%= @mountpoint -%>/nfstest with mkdir -p <%= @mountpoint -%>/nfstest and 
  run chown <%= @user %>:<%= @group %> <%= @mountpoint %>/nfstest 
  The reason we do this is because the <%= @mountpoint -%> dir is owned by root and the <%= @mountpoint -%>/nfstest should 
  be owned by the <%= @user %> user since this is where the dumps subdir will be created by the <%= @user %> user
* As the <%= @user %> user, run the script nfs_testing_create_output_dirs.sh located in this directory.
  This will create the directories for dumps output files, if they do not already exist.

TESTING SQL/XML DUMPS

All actions are done as the <%= @user -%> user.

* cd into the /srv/deployment/dumps/dumps/xmldumps-backup directory
* run the worker.py script on olowiki:
  python3 /srv/deployment/dumps/dumps/xmldumps-backup/worker.py --configfile <%= @confsdir -%>/xmldumps.conf --log --skipdone --exclusive olowiki
* check that the expected output files in <%= @publicdir -%> are present for each step, that they have reasonable content,
  and that the dumps log in <%= @privatedir -%>/olowiki/dumps.log doesn't have
  exceptions in it, etc.

* mkdir <%= @publicdir -%>/igwiki
* on the dumpsdata host with the nfs share you want to test, as the dumpsgen user, rsync the files from the last igwiki run to the test mount point:
  (substitute in the right YYYYMMDD and the right dumpsdata host values below, fix up the target path to be right)
  rsync -av --bwlimit=100000 dumpsdataXXX.eqiad.wmnet::data/public/igwiki/202XMMDD /some/path/nfstest/xmldatadumps/public/igwiki
* run the worker script on igwiki, testing prefetch:
  python3 /srv/deployment/dumps/dumps/xmldumps-backup/worker.py --configfile <%= @confsdir -%>/xmldumps.conf --log --skipdone --exclusive igwiki
* check that the expected output files in <%= @publicdir -%> are present for each step, that they have reasonable content,
  and that the dumps log in <%= @privatedir -%>/olowiki/dumps.log doesn't have
  exceptions in it, etc.

* run the dumps monitor:
  python3 /srv/deployment/dumps/dumps/xmldumps-backup/monitor --configfile <%= @confsdir -%>/xmldumps.conf --basedir /srv/deployment/dumps/dumps/xmldumps-backup
* in another screen window or another browser tab, check the output: the index.html file should be
  generated at <%= @publicdir -%>/backup-index.html and you can inspect it manually to make sure the two wikis (igwiki, olowiki)
  have reasonable entries generated for them
* stop the monitor

This should be sufficient testing for the SQL/XML dumps for a new nfs share.

TESTING OTHER DUMPS

We will test only the following: the rdf weekly category dumps, the adds-changes dumps,
the pagetitles dump, and wikidata entity dumps for a small range of entities.
 
Test the categories rdf dump:
/usr/bin/php /srv/mediawiki/multiversion/MWScript.php maintenance/dumpCategoriesAsRdf.php --wiki=olowiki --format=ttl 2> <%= @homedir -%>/nfs_testing/categories_rdf.log | /usr/bin/gzip > <%= @otherdumpsdir -%>/categoriesrdf/testrun
Check the output to see that it looks reasonable.

Test the wikidata entity dumps:
Remove any files from a previous run.
rm -f <%= @homedir -%>/nfs_testing/wikidata_entities.log
rm -f <%= @homedir -%>/nfs_testing/wikidata_entities.log | gzip -9 > <%= @otherdumpsdir -%>/wikidata-entity-dumps-test.gz
/usr/bin/php /srv/mediawiki/multiversion/MWScript.php extensions/Wikibase/repo/maintenance/dumpRdf.php --wiki wikidatawiki --shard 1 --sharding-factor 2 --batch-size 250 --first-page-id 100880001 --last-page-id 100880501 --flavor full-dump --entity-type item --entity-type property --dbgroupdefault dump --part-id 1 2>> <%= @homedir -%>/nfs_testing/wikidata_entities.log | gzip -9 > <%= @otherdumpsdir -%>/wikidata-entity-dumps-test.gz
Check the output to see that it looks reasonable.

Test the adds-changed dumps:
Make output directories for for igwiki, snwiki, olowiki.
mkdir -p <%= @otherdumpsdir -%>/incr/olowiki <%= @otherdumpsdir -%>/incr/igwiki <%= @otherdumpsdir -%>/incr/snwiki
On the dumpsdata host with the nfs share you want to test, as the dumpsgen user, rsync the files from the igwiki
incr run from a week ago to the test mount point. We choose a week rather than a day or a month because these are
small wikis and we want to make sure there are a few but not tons of revisions that will be dumped in our test.
(substitute in the right YYYYMMDD and the right dumpsdata host values below, fix up the target path to be right)
rsync -av --bwlimit=100000 dumpsdataXXX.eqiad.wmnet::data/otherdumps/incr/olowiki/202XMMDD /some/path/nfstest/otherdumps/incr/olowiki
Repeat this for snwiki, igwiki.
Back on the snapshot testbed, for each of olowiki, snwiki, igwiki, rename the subdir to have yesterday's date (YYYYMMDD always):
mv <%= @otherdumpsdir -%>/incr/olowiki/old-date <%= @otherdumpsdir -%>/incr/olowiki/yesterday-date
mv <%= @otherdumpsdir -%>/incr/igwiki/old-date <%= @otherdumpsdir -%>/incr/igwiki/yesterday-date
mv <%= @otherdumpsdir -%>/incr/snwiki/old-date <%= @otherdumpsdir -%>/incr/snwiki/yesterday-date
Now run the adds-changes dumps:
/usr/bin/python3 /srv/deployment/dumps/dumps/xmldumps-backup/generatemiscdumps.py --configfile <%= @confsdir -%>/addschanges.conf --dumptype incrdumps
Check the output to see that it looks reasonable.

