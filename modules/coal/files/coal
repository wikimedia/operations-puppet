#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
  coal
  ~~~~
  Coal logs Navigation Timing metrics to Whisper files.

  More specifically, coal aggregates all of the samples for a given NavTiming
  metric that are collected within a period of time, and it writes the median
  of those values to Whisper files.

  See the constants at the top of the file for configuration options.

  Copyright 2015 Ori Livneh <ori@wikimedia.org>
  Copyright 2018 Ian Marlier <imarlier@wikimedia.org>

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

"""
import argparse
import collections
from confluent_kafka import Consumer, KafkaError, KafkaException
import dateutil.parser
import errno
import json
import logging
import os
import os.path
import time
import whisper


UPDATE_INTERVAL = 60  # How often we log values, in seconds
WINDOW_SPAN = UPDATE_INTERVAL * 5  # Size of sliding window, in seconds.
RETENTION = 525949    # How many datapoints we retain. (One year's worth.)
METRICS = (
    'connectEnd',
    'connectStart',
    'dnsLookup',
    'domainLookupStart',
    'domainLookupEnd',
    'domComplete',
    'domContentLoadedEventStart',
    'domContentLoadedEventEnd',
    'domInteractive',
    'fetchStart',
    'firstPaint',
    'loadEventEnd',
    'loadEventStart',
    'mediaWikiLoadComplete',
    'mediaWikiLoadStart',
    'mediaWikiLoadEnd',
    'redirectCount',
    'redirecting',
    'redirectStart',
    'redirectEnd',
    'requestStart',
    'responseEnd',
    'responseStart',
    'saveTiming',
    'secureConnectionStart',
    'unloadEventStart',
    'unloadEventEnd',
)
ARCHIVES = [(UPDATE_INTERVAL, RETENTION)]

class WhisperLogger(object):
    def __init__(self):
        arg_parser = argparse.ArgumentParser()
        arg_parser.add_argument('--whisper-dir', default=os.getcwd(),
            help='Path for Whisper files.  Defaults to current working dir')
        arg_parser.add_argument('--brokers', required=True,
            help='Comma-separated list of Kafka brokers')
        arg_parser.add_argument('--consumer-group', required=True,
            dest='consumer_group', help='Name of the Kafak consumer group')
        arg_parser.add_argument('--topic', required=False,
            default='eventlogging_NavigationTiming',
            help='Kafka topic from which to consume')
        arg_parser.add_argument('-n', '--dry-run', required=False, dest='dry_run',
            action='store_true', default=False, 
            help='Don\'t create whisper files, just output')
        arg_parser.add_argument('-v', '--verbose', dest='verbose',
            required=False, default=False, action='store_true',
            help='Increase verbosity of output')
        self.args = arg_parser.parse_args()
        self.windows = collections.defaultdict(list)
        self.now = time.time() # make it possible to be in the past
        # Log config
        self.log = logging.getLogger(__name__)
        self.log.setLevel(logging.DEBUG if self.args.verbose else logging.INFO)
        ch = logging.StreamHandler()
        ch.setLevel(logging.DEBUG if self.args.verbose else logging.INFO)
        formatter = logging.Formatter(
            '%(asctime)s [%(levelname)s] (%(funcName)s:%(lineno)d) %(msg)s')
        formatter.converter = time.gmtime
        ch.setFormatter(formatter)
        self.log.addHandler(ch)

    def median(self, population):
        population = list(sorted(population))
        length = len(population)
        if length == 0:
            raise ValueError('Cannot compute median of empty list.')
        index = (length - 1) // 2
        if length % 2:
            return population[index]
        middle_terms = population[index] + population[index + 1]
        return middle_terms / 2.0

    def get_whisper_file(self, metric):
        return os.path.join(self.args.whisper_dir, metric + '.wsp')

    def create_whisper_files(self):
        if self.args.dry_run:
            self.log.info(
                'Skipping creating whisper files because dry-run flag is set')
            return
        for metric in METRICS:
            try:
                whisper.create(self.get_whisper_file(metric), ARCHIVES)
            except whisper.InvalidConfiguration as e:
                pass  # Already exists.

    def handle_event(self, meta):
        if meta['schema'] not in ('NavigationTiming', 'SaveTiming'):
            return None

        # dt is main EventCapsule timestamp field in ISO-8601
        if 'dt' in meta:
            timestamp = int(dateutil.parser.parse(meta['dt']).strftime("%s"))
        # timestamp is backwards compatible int, this shouldn't be used anymore.
        elif 'timestamp' in meta:
            timestamp = meta['timestamp']
        # else we can't find one, just use the current time.
        else:
            # timestamp = int(time.time()) # Not sure this is actually good...
            return None

        event = meta['event']
        for metric in METRICS:
            value = event.get(metric)
            if value:
                window = self.windows[metric]
                window.append((timestamp, value))
        return timestamp

    def flush_data(self):
        self.log.debug('Flushing data')
        for metric, window in sorted(self.windows.items()):
            window.sort()
            # Always start with the oldest sample
            first_timestamp = int(window[0][0])

            while True:
                if len(window) == 0:
                    break

                # Establish a sample period that begins with the oldest sample
                end_timestamp = first_timestamp + WINDOW_SPAN

                # If we've processed enough that the window pushes past the end
                # of the current data set, then move along
                if end_timestamp > int(window[-1][0]):
                    self.log.debug('Last message timestamp {}, current window ends at {}'.format(
                        int(window[-1][0]), end_timestamp))
                    break

                # Write the value of this window to the whisper file
                current_value = self.median(
                    [value for timestamp, value in window if timestamp <= end_timestamp])
                if self.args.dry_run:
                    self.log.info('[{}] [{}] {}'.format(end_timestamp, metric,
                        current_value))
                else:
                    whisper.update(self.get_whisper_file(metric), current_value,
                        timestamp=end_timestamp)

                # Get rid of the first interval worth of items from the window
                for item in [item for item in window if item[0] < (first_timestamp + UPDATE_INTERVAL)]:
                    window.remove(item)

                # Move to the next window and loop again
                first_timestamp += UPDATE_INTERVAL

    def run(self):
        window_start = time.time()
        self.create_whisper_files()

        # There are basically 3 ways to handle timers
        #  1. On each received Kafka message, check whether we've tipped over into
        #     the next window, and if so, do processing.  The problem with this
        #     is that if no message is received for some period of time, then
        #     processing never happens.
        #  2. Run the Kafka poller in one thread, and the timer in another.  Use
        #     a lock to temporarily block the poller while processing is happening.
        #     While this is pretty straightforward, it does require dealing with
        #     threads and shared vars.
        #  3. Run the poller inside of one aio function, and the timer inside of
        #     another.  Leave it to asyncio to handle the coordination.  This is,
        #     honestly, the most complete and easiest to reason about of these
        #     solutions.  But it also requires py >= 3.4, aiokafka, and potentially
        #     other aio libraries that aren't packaged natively.
        #
        # This method, as written, implements #1.
        self.log.info('Starting Kafka connection to brokers ({}).'.format(
            self.args.brokers))
        consumer = Consumer({
            'bootstrap.servers': self.args.brokers,
            'group.id': self.args.consumer_group,
        })
        consumer.subscribe([self.args.topic])

        self.log.info('Beginning poll cycle')

        intervals_since_last_event = 0

        while True:
            try:
                message = consumer.poll(timeout=1.0)
                # Timeout, we'll just loop again
                if message is None:
                    continue

                # Message was received
                if message.error():
                    # There are two error cases:
                    #  - An actual error
                    #  - The end of the partition was reached (aka, no more
                    #    messages in queue at this point in time)

                    # Actual error
                    if message.error().code() != KafkaError._PARTITION_EOF:
                        self.log.error(
                            'Error attempting to retrieve message from Kafka: %s',
                            message.error())
                        raise KafkaException(message.error())

                    # No more messages in queue at the moment
                    else:
                        if time.time() > (window_start + WINDOW_SPAN):
                            self.flush_data()
                        else:
                            self.log.debug(
                                'EOF received, but time {} not reached'.format(
                                window_start + WINDOW_SPAN))

                else:
                    try:
                        value = json.loads(message.value())
                    except ValueError as e:
                        # If incoming messages aren't well-formatted, log them
                        # so we can see that and handle it.
                        self.log.error(
                            'ValueError raised trying to load JSON message: %s',
                            message.value())
                        continue

                    # Get the incoming event on to the pile.  Get back the
                    # event timestamp.  If the timestamp is old (likely because
                    # we had a queue of old messages to handle), then reset the
                    # window start value.
                    event_ts = self.handle_event(value)
                    if event_ts is not None and event_ts < window_start:
                        window_start = event_ts

            # Allow for a clean quit on interrupt
            except KeyboardInterrupt:
                self.log.info('Stopping the Kafka consumer and shutting down')
                consumer.close()
                break
            except IOError as e:
                if e.errno != errno.EINTR:
                    self.log.exception('Error in main loop:')
                    raise
            except Exception as e:
                self.log.error(e)
                raise


if __name__ == '__main__':
    app = WhisperLogger()
    app.run()
