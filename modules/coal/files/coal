#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
  coal
  ~~~~
  Coal logs Navigation Timing metrics to Whisper files.

  Copyright 2015 Ori Livneh <ori@wikimedia.org>

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

"""
import sys
reload(sys)
sys.setdefaultencoding('utf-8')

import argparse
import json
import collections
import dateutil.parser
import errno
import fcntl
import logging
import os
import os.path
import select
import signal
import time

import whisper
from kafka import KafkaConsumer


WINDOW_SPAN = 60 * 5  # Size of sliding window, in seconds.
UPDATE_INTERVAL = 60  # How often we log values.
RETENTION = 525949    # How many datapoints we retain. (One year's worth.)
METRICS = (
    'connectEnd',
    'connectStart',
    'dnsLookup',
    'domainLookupStart',
    'domainLookupEnd',
    'domComplete',
    'domContentLoadedEventStart',
    'domContentLoadedEventEnd',
    'domInteractive',
    'fetchStart',
    'firstPaint',
    'loadEventEnd',
    'loadEventStart',
    'mediaWikiLoadComplete',
    'mediaWikiLoadStart',
    'mediaWikiLoadEnd',
    'redirectCount',
    'redirecting',
    'redirectStart',
    'redirectEnd',
    'requestStart',
    'responseEnd',
    'responseStart',
    'saveTiming',
    'secureConnectionStart',
    'unloadEventStart',
    'unloadEventEnd',
)
ARCHIVES = [(UPDATE_INTERVAL, RETENTION)]


def median(population):
    population = list(sorted(population))
    length = len(population)
    if length == 0:
        raise ValueError('Cannot compute median of empty list.')
    index = (length - 1) // 2
    if length % 2:
        return population[index]
    middle_terms = population[index] + population[index + 1]
    return middle_terms / 2.0


def create_interval_timer_fd(initial_delay, interval_seconds):
    r, w = os.pipe()
    flags = fcntl.fcntl(w, fcntl.F_GETFL, 0)
    fcntl.fcntl(w, fcntl.F_SETFL, flags | os.O_NONBLOCK)
    signal.set_wakeup_fd(w)
    signal.signal(signal.SIGALRM, cmp)
    signal.setitimer(signal.ITIMER_REAL, initial_delay, interval_seconds)
    return r


class WhisperLogger(object):

    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument(
        '--whisper-dir',
        default=os.getcwd(),
        help='Path for Whisper files. Defaults to working directory.'
    )
    ap.add_argument('--brokers', required=True,
        help='Comma-separated list of kafka brokers')
    ap.add_argument('--consumer-group', required=True,
        help='Consumer group to register with Kafka')

    def __init__(self):
        self.args = self.arg_parser.parse_args()
        self.windows = collections.defaultdict(collections.deque)
        self.log = logging.getLogger(__name__)

    def get_whisper_file(self, metric):
        return os.path.join(self.args.whisper_dir, metric + '.wsp')

    def create_whisper_files(self):
        for metric in METRICS:
            try:
                whisper.create(self.get_whisper_file(metric), ARCHIVES)
            except whisper.InvalidConfiguration as e:
                pass  # Already exists.

    def run(self):
        self.create_whisper_files()

        # Based on webperf/navtiming.py
        kafka_bootstrap_servers = tuple(self.args.brokers.split(','))
        kafka_topics = ('eventlogging_NavigationTiming', 'eventlogging_SaveTiming')
        kafka_consumer_timeout_seconds = 60
        consumer = KafkaConsumer(
            *kafka_topics,
            bootstrap_servers=kafka_bootstrap_servers,
            group_id=self.args.consumer_group,
            auto_offset_reset='latest',
            enable_auto_commit=False,
            consumer_timeout_ms=kafka_consumer_timeout_seconds * 1000
        )

        self.log.info('Starting Kafka consumer')
        socket = zmq.Context().socket(zmq.SUB)
        socket.connect(self.args.endpoint)
        socket.subscribe = b''

        # Accumulate a full WINDOW_SPAN's worth of samples before the first
        # update. After that, update every UPDATE_INTERVAL seconds.
        timer_fd = create_interval_timer_fd(WINDOW_SPAN, UPDATE_INTERVAL)

        poll = select.epoll()
        poll.register(socket.fd, select.EPOLLIN)
        poll.register(timer_fd, select.EPOLLIN)

        self.log.info('Entering main loop.')

        intervals_since_last_event = 0

        while 1:
            try:
                for fd, _ in poll.poll():
                    if fd == timer_fd:
                        os.read(timer_fd, 1)
                        self.flush_data()
                        intervals_since_last_event += 1
                        if intervals_since_last_event > 2:
                            raise RuntimeError('No events in %d seconds.' % (
                                UPDATE_INTERVAL * intervals_since_last_event))
                    if fd == socket.fd:
                        while socket.events & zmq.POLLIN:
                            meta = socket.recv_json()
                            intervals_since_last_event = 0
                            self.handle_event(meta)
            except IOError as e:
                if e.errno != errno.EINTR:
                    self.log.exception('Error in main loop:')
                    raise

    def handle_event(self, meta):
        if meta['schema'] not in ('NavigationTiming', 'SaveTiming'):
            return

        # dt is main EventCapsule timestamp field in ISO-8601
        if 'dt' in meta:
            timestamp = int(dateutil.parser.parse(meta['dt']).strftime("%s"))
        # timestamp is backwards compatible int, this shouldn't be used anymore.
        elif 'timestamp' in meta:
            timestamp = meta['timestamp']
        # else we can't find one, just use the current time.
        else:
            timestamp = int(time.time())

        event = meta['event']
        for metric in METRICS:
            value = event.get(metric)
            if value:
                window = self.windows[metric]
                window.append((timestamp, value))

    def flush_data(self):
        for metric, window in sorted(self.windows.items()):
            while window[-1][0] - window[0][0] > WINDOW_SPAN:
                window.popleft()
            current_value = median(value for timestamp, value in window)
            whisper.update(self.get_whisper_file(metric), current_value)
            self.log.debug('%s: %d' % (metric, current_value))


if __name__ == '__main__':
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    stderr_handler = logging.StreamHandler(stream=sys.stderr)
    logger.addHandler(stderr_handler)

    app = WhisperLogger()
    app.run()
