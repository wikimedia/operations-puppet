#!/bin/bash
### THIS FILE IS MANAGED BY PUPPET
### puppet:///modules/mediawiki/jobrunner/jobs-loop.sh.erb
#
# NAME
# jobs-loop.sh -- Continuously process a MediaWiki jobqueue
#
# SYNOPSIS
# jobs-loop.sh [-t timeout] [-v virtualmemory] [job_type]
#
# DESCRIPTION
# jobs-loop.sh is an infinite "while loop" used to call MediaWiki runJobs.php
# and eventually attempt to process any job enqueued. MediaWiki jobs are split
# into several types, by default jobs-loop.sh will:
#  - first attempt to run the priorized jobs
#  - proceed to run any default jobs
#
# MediaWiki configuration variable $wgJobTypesExcludedFromDefaultQueue is used
# to exclude some types from the default processing. Those excluded job types
# could be processed on dedicated boxes by running jobs-loop.sh using the
# job_type parameter.
#
# You will probably want to run this script under your webserver username.
#
# Example:
# // Process job queues:
# jobs-loop.sh
#
# // Process jobs of type `webVideoTranscode` with a maxtime of 4 hours
# jobs-loop.sh -t 14400 webVideoTranscode
#

# How long can low priority jobs be run until some high priority
# jobs should be checked for and run if they exist.
hpmaxdelay=120

# The maxtime parameter for runJobs.php for low priority jobs.
# The lower this value is, the less jobs of one wiki can hog attention
# from the jobs on other wikis, though more overhead is incurred.
# This should be lower than hpmaxdelay.
lpmaxtime=60

# How long can high priority jobs be run until some low priority
# jobs should be checked for and run if they exist.
lpmaxdelay=600

# The maxtime parameter for runJobs.php for high priority jobs.
# The lower this value is, the less jobs of one wiki/type can hog attention
# from jobs of another wiki/type, though more overhead is incurred.
# This should be lower than lpmaxdelay.
hpmaxtime=30

# The procs parameter to runJobs.php.
# Lower values keep the process pipeline fuller and lead to more
# even picking of jobs from wikis, though more overhead is incurred.
forkcount=1

# default total memory limit (in kb) for this process and its sub-processes
maxvirtualmemory=800000

# default total memory limit for each runJobs.php process
maxsubprocmemory=300M

while getopts "t:v:" flag
do
	case $flag in
		t)
			hpmaxtime=$OPTARG
			;;
		v)
			maxvirtualmemory=$OPTARG
			;;
	esac
done
shift $(($OPTIND - 1))

# Limit virtual memory
if [ "$maxvirtualmemory" -gt 0 ]; then
	ulimit -v $maxvirtualmemory
fi

# Function to also get rid of the child processes.
# This function can be used as a SIGINT trap callback.
function terminateProcessPipeline() {
	local pids=$(jobs -pr);
	if [ -n "$pids" ]; then
		kill $pids
	fi
	exit
}

# Limit the number of concurrent sub-processes by waiting until the
# number of immediate child processes is less than the allowed limit.
# Arguments: (process pipeline size)
function processPipelineWaitForSlot() {
	local pipelinesize=$1
	local subprocscreate=0 # number of free process slots
	while [ "$subprocscreate" -le 0 ]; do
		local subproccount=$((`jobs -pr | wc -w`))
		subprocscreate=$((pipelinesize/forkcount - subproccount))
		if [ "$subprocscreate" -gt 0 ]; then
			break # another immediate child process can start
		else
			echo "Pipeline full ($subproccount immediate sub-processes)..."
			sleep 1 # reduce polling load
		fi
	done
}

# Run an infinite loop that checks for jobs and runs them.
# The first argument is the "job types" parameter. If supplied,
# only that space-separated list of jobs will be run. Otherwise,
# the default high priority and low priority jobs will be run.
# Arguments: (space-separated job types, run default jobs? (y/n), process pipeline size)
function runJobsLoopService() {
	# When killed, make sure we also get rid of the child processes
	trap 'terminateProcessPipeline' SIGTERM

	local hptypes=$1
	local dodefault=$2
	local pipelinesize=$3

	if [[ "$pipelinesize" -lt 1 ]]; then
		exit # nothing to do
	fi

	while [ 1 ]; do
		local foundjobs=n

		if [ -n "$hptypes" ]; then
			# Do the prioritized job types...
			local started=`date +%s` # UNIX timestamp
			local morehpjobs=y
			while [[ "$morehpjobs" == "y" ]]; do
				processPipelineWaitForSlot "$pipelinesize"

				morehpjobs=n
				local res=(`php MWScript.php nextJobDB.php --wiki=aawiki --types="$hptypes"`)
				local db=${res[0]}
				local type=${res[1]}

				if [ "$?" -ne "0" ]; then
					echo "Could not determine the next wiki."
				elif [ -n "$db" -a -n "$type" ]; then
					foundjobs=y
					echo "$db $type"
					nice -n 20 php MWScript.php runJobs.php --wiki="$db" --procs="$forkcount" --type="$type" --maxtime=$hpmaxtime --memory-limit=$maxsubprocmemory &
					# Do not spend too much time on prioritized jobs
					local timestamp=`date +%s` # UNIX timestamp
					if [ "$timestamp" -lt "$started" -o $(( timestamp-started )) -ge "$lpmaxdelay" ]; then
						break
					else
						morehpjobs=y
					fi
				fi
			done
		fi

		if [[ "$dodefault" == "y" ]]; then
			# Do the unprioritized job types...
			local started=`date +%s` # UNIX timestamp
			local morelpjobs=y
			while [[ "$morelpjobs" == "y" ]]; do
				processPipelineWaitForSlot "$pipelinesize"

				morelpjobs=n
				local db=`php MWScript.php nextJobDB.php --wiki=aawiki`

				if [ "$?" -ne "0" ]; then
					echo "Could not determine the next wiki."
				elif [ -n "$db" ]; then
					foundjobs=y
					echo "$db"
					nice -n 20 php MWScript.php runJobs.php --wiki="$db" --procs="$forkcount" --maxtime=$lpmaxtime --memory-limit=$maxsubprocmemory &
					# Do not spend too much time on unprioritized jobs
					local timestamp=`date +%s` # UNIX timestamp
					if [ "$timestamp" -lt "$started" -o $(( timestamp-started )) -ge "$hpmaxdelay" ]; then
						break
					else
						morelpjobs=y
					fi
				fi
			done
		fi

		if [[ "$foundjobs" == "n" ]]; then
			echo "No jobs..."
			sleep 5 # reduce polling load
		fi
	done
}

# Make it easy to call MWScript.php
cd `readlink -f /usr/local/apache/common/multiversion`

# When killed, make sure we also get rid of the child processes
trap 'terminateProcessPipeline' SIGTERM SIGINT

if [ -n "$1" ]; then
	echo "Creating type-specific job runner pipeline"
	(runJobsLoopService "$1" "n" <%= dprioprocs %>) &
else
	echo "Creating default and immediate job runner pipelines"
	# Start a loop for immediate priority jobs
	ipriotypes="AssembleUploadChunks PublishStashedFile" # upload
	(runJobsLoopService "$ipriotypes" "n" <%= iprioprocs %>) &
	# Start a loop for default high and low priority jobs
	hpriotypes="sendMail enotifNotify MoodBarHTMLMailerJob MWEchoNotificationEmailBundleJob ArticleFeedbackv5MailerJob" # mail
	hpriotypes="$hpriotypes EchoNotificationJob" # echo notification
	hpriotypes="$hpriotypes TranslateRenderJob TranslateMoveJob TranslateDeleteJob" # translate
	hpriotypes="$hpriotypes uploadFromUrl" # upload
	hpriotypes="$hpriotypes MassMessageJob MassMessageSubmitJob" # MassMessage
	hpriotypes="$hpriotypes cirrusSearchLinksUpdatePrioritized" # CirrusSearch priority jobs
	(runJobsLoopService "$hpriotypes" "y" <%= dprioprocs %>) &

	# Start loops for highly I/O bound jobs that work on special services (e.g. not the DBs)

	# Lower priority for Parsoid dependency changes (template / file
	# updates)
	(runJobsLoopService "ParsoidCacheUpdateJobOnDependencyChange" "n"  <%= procs_per_iobound_type %> ) &
	# High priority for updates to edited pages themselves
	(runJobsLoopService "ParsoidCacheUpdateJobOnEdit" "n"  $((<%= procs_per_iobound_type %> * 2)) ) &

	# GWToolset
	(runJobsLoopService "gwtoolsetUploadMetadataJob gwtoolsetUploadMediafileJob gwtoolsetGWTFileBackendCleanupJob" "n"  <%= procs_per_iobound_type %> ) &
fi

wait
echo "All job runner pipelines closed"
