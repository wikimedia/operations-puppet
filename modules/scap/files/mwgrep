#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
  usage: mwgrep [-h] [--max-results N] [--timeout N] [--user | --module]
                [--title TITLE] term

  Grep for Lua or CSS and JS code fragments on (per default) MediaWiki wiki pages

  positional arguments:
    term             text to search for

  optional arguments:
    -h, --help       show this help message and exit
    --max-results N  show at most this many results (default: 100)
    --timeout N      abort search after this many seconds (default: 30)
    --user           search NS_USER rather than NS_MEDIAWIKI
    --module         search NS_MODULE rather than NS_MEDIAWIKI
    --title TITLE    restrict search to pages with this title

  mwgrep will grep the MediaWiki namespace across Wikimedia wikis. specify
  --user to search the user namespace instead.

"""
import sys
import argparse
import bisect
import json
import urllib
import urllib2

reload(sys)
sys.setdefaultencoding('utf-8')


TIMEOUT = 30
BASE_URI = 'http://search.svc.eqiad.wmnet:9200/_all/page/_search'
NS_MEDIAWIKI = 8
NS_USER = 2
NS_MODULE = 828
PREFIX_NS = {
    NS_MEDIAWIKI: 'MediaWiki:',
    NS_USER: 'User:',
    NS_MODULE: 'Module:'
}

ap = argparse.ArgumentParser(
    prog='mwgrep',
    description='Grep for CSS and JS code fragments in MediaWiki wiki pages',
    epilog='mwgrep will grep the MediaWiki namespace across Wikimedia wikis. '
           'specify --user to search the user namespace instead.'
)
ap.add_argument('term', help='text to search for')

ap.add_argument(
    '--max-results',
    metavar='N',
    type=int, default=100,
    help='show at most this many results (default: 100)'
)

ap.add_argument(
    '--timeout',
    metavar='N',
    type='{0}s'.format,
    default='30',
    help='abort search after this many seconds (default: 30)'
)

ap.add_argument(
    '--title',
    help='Restrict search to pages with this title (sans namespace)'
)

ns_group = ap.add_mutually_exclusive_group()
ns_group.add_argument(
    '--user',
    action='store_const',
    const=NS_USER,
    default=NS_MEDIAWIKI,
    dest='ns',
    help='search NS_USER rather than NS_MEDIAWIKI'
)

ns_group.add_argument(
    '--module',
    action='store_const',
    const=NS_MODULE,
    default=NS_MEDIAWIKI,
    dest='ns',
    help='search NS_MODULE rather than NS_MEDIAWIKI'
)

args = ap.parse_args()

filters = [
    {'term': {'namespace': str(args.ns)}},
    {'script': {
        'file': 'mwgrep',
        'lang': 'groovy',
        'params': {'query': args.term}
    }},
]

if args.ns == NS_USER or args.ns == NS_MEDIAWIKI:
    filters.append({'regexp': {'title.keyword': '.*\\.(js|css)'}})

if args.title is not None:
    filters.append({'term': {'title.keyword': args.title}})

search = {
    'size': args.max_results,
    '_source': ['namespace', 'title'],
    'query': {'filtered': {'filter': {'bool': {'must': filters}}}},
    'stats': ['mwgrep'],
}

query = {
    'timeout': args.timeout,
}

matches = {'public': [], 'private': []}
uri = BASE_URI + '?' + urllib.urlencode(query)
req = urllib2.urlopen(uri, json.dumps(search))
result = json.load(req)['hits']

private_wikis = open('/srv/mediawiki/dblists/private.dblist').read().splitlines()

for hit in result['hits']:
    db_name = hit['_index'].rsplit('_', 2)[0]
    title = hit['_source']['title']
    page_name = '%s%s' % (PREFIX_NS[args.ns], title)
    if db_name in private_wikis:
        bisect.insort(matches['private'], (db_name, page_name))
    else:
        bisect.insort(matches['public'], (db_name, page_name))

if matches['public']:
    print('## Public wiki results')
    for db_name, page_name in matches['public']:
        print('{:<20}{}'.format(db_name, page_name))

if matches['private']:
    if matches['public']:
        print('')
    print('## Private wiki results')
    for db_name, page_name in matches['private']:
        print('{:<20}{}'.format(db_name, page_name))

print('')
print('(total: %s, shown: %s)' % (result['total'], len(result['hits'])))
