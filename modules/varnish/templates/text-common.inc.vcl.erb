// Common functions for the Text Varnish cluster

include "normalize_path.inc.vcl";

/******************************************************************************
 * For MediaWiki's purposes, building on the RFC explanation in
 * normalize_path_encoding(), all of the 16 characters in the Customizable Set
 * can be put into a specific subsets that are either always-decoded or
 * always-encoded, giving us "complete" normalization:
 *
 * 10 Always-Decode: : / @ ! $ ( ) * , ;
 * 6 Always-Encode: [ ] & ' + =
 *
 * The set of 10 Always-Decode exactly matches MW's "wfUrlencode".  For the
 * Always-Encode set: the square brackers are part of MediaWiki's set of
 * disallowed Title characters, and the rest come from observing Wikipedia's
 * live behavior for these bytes when sent in either form (the canonical title
 * always normalizes these as encoded).  The rest of the normal generic decoder
 * settings were also observationally tested to conform to the RFC.
 *
 * XXX However, for this first commit, we're merely switching to this new
 * normalization code with minimal behavior change from the old code, which
 * means not enforcing the Always-Encode set above.
 *****************************************************************************/

sub normalize_mediawiki_path { C{
    static const size_t mediawiki_decoder_ring[256] = {
      // 0x00-0x1F (all unprintable)
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
      //  ! " # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ?
        0,1,0,2,1,0,2,2,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,2,0,2,
      //@ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \ ] ^ _
        1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,0,2,0,1,
      //` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ~ <DEL>
        0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,
      // 0x80-0xFF (all unprintable)
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
    };

    normalize_path_encoding(ctx, mediawiki_decoder_ring);
}C }

/******************************************************************************
 * Restbase:
 * Believed to use MW encoding rules above, but has a special exception for
 *   forward-slash: We can neither encode nor decode either form of the
 *   forward-slash for RB; it must be preserved.  This is because RB needs
 *   forward-slashes from MediaWiki titles to be in %2F form, but still needs
 *   its own functional path-delimiting slashes unencoded.  Ref: T127387
 * Therefore, the table below is almost identical to the MW one above, but note
 *   that the '/' is set to 2 (as it would be in generic_decoder_ring), which
 *   means do not change its encoding status.
 *****************************************************************************/

sub normalize_rest_path { C{
    static const size_t restbase_decoder_ring[256] = {
      // 0x00-0x1F (all unprintable)
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
      //  ! " # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ?
        0,1,0,2,1,0,2,2,1,1,1,2,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,0,2,0,2,
      //@ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \ ] ^ _
        1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,0,2,0,1,
      //` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ~ <DEL>
        0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,
      // 0x80-0xFF (all unprintable)
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
    };

    normalize_path_encoding(ctx, restbase_decoder_ring);
}C }

sub text_common_misspass_restore_cookie {
	// Restore the original Cookie header for upstream
	// Assumes client header X-Orig-Cookie has been filtered!
	if (bereq.http.X-Orig-Cookie) {
		set bereq.http.Cookie = bereq.http.X-Orig-Cookie;
		unset bereq.http.X-Orig-Cookie;
	}
}

sub evaluate_cookie {
	// For hash->lookup purposes, if there's a valid-seeming session|token
	// cookie, replace it with the shared Cookie value "Token=1",
	// otherwise remove the Cookie completely.  In either case, in
	// vcl_(pass|miss) on the way to any backend fetch, it will be
	// restored to the original value.
	// Later in text_common_backend_response, we'll create hit-for-pass objects for any
	// response with Vary:Cookie and Cookie:Token=1.  This will allow
	// logged-in users to share the anonymous users' cache for
	// non-Vary:Cookie responses, and then share a single hit-for-pass
	// object in a single Vary slot per object for those responses that
	// should Vary:Cookie.
	if (req.restarts == 0) {
		unset req.http.X-Orig-Cookie;
		if (req.http.Cookie) {
			set req.http.X-Orig-Cookie = req.http.Cookie;
			if (req.http.Cookie ~ "([sS]ession|Token)=") {
				set req.http.Cookie = "Token=1";
			} else {
				unset req.http.Cookie;
			}
		}
	}
}

sub pass_authorization {
	// As a general rule, per the RFCs, shared caches shouldn't use cached
	// responses for requests with Authorization headers (with a few
	// notable exceptions aside).  The default vcl_recv (which text
	// doesn't use) in varnish does this alongside its pass for Cookie.
	// We deal with Cookies properly elsewhere.
	//
	// By letting Authorization reqs cache, we're breaking our own OAuth
	// in some corner cases: https://phabricator.wikimedia.org/T105387
	// Any browser/curl fetch of an OAuth URL without the header can
	// cause a redirect to be cached, which then affects real fetches that
	// have the Authorization header set, which is Bad.
	//
	// It's *probably* best to just pass all Authorization-headered
	// requests in general, but on the other hand, I see a lot of random
	// Authorization headers in our traffic that don't look like stuff
	// we're using or want to pay attention to, and we've otherwise seemed
	// ok on this stuff.  So for now, only (pass) on ones that seem to
	// have OAuth data, with the same header-matching as in:
	// https://phabricator.wikimedia.org/diffusion/EOAU/browse/master/backend/MWOAuthUtils.php;8029ef146211a1016b1f8e676944c3750f78b0eb$89
	if (req.http.Authorization ~ "^OAuth ") {
		return (pass);
	}
}

sub text_normalize_path {
	// Don't decode percent-encoded slashes in paths for REST APIs
	if (req.url ~ "^/api/rest_v1/" || req.http.host ~ "^cxserver\.wikimedia\.org$") {
		call normalize_rest_path;
	} else {
		call normalize_mediawiki_path;
	}

	// Normalize Accept headers for the REST API: Ignore unless a profile is
	// specified or this is a request for the root REST URI
	if (req.url ~ "^/api/rest_v1/?(\?doc)?$" && req.http.Accept ~ "text/html") {
		set req.http.Accept = "text/html";
	} else if (req.url ~ "^/api/rest_v1/" && req.http.Accept !~ {"profile="https://www.mediawiki.org/wiki/Specs/"}) {
		unset req.http.Accept;
	}
	// Normalise the Accept-Language header for REST API requests
	// For the time being we only take the first lang variant passed in and ignore the rest
	if (req.url ~ "^/api/rest_v1/" && req.http.Accept-Language) {
		set req.http.Accept-Language = regsub(req.http.Accept-Language, "^[a-zA-Z]+(-[a-zA-Z]+)?", "\0");
	}
}

// fe+be common recv code
sub text_common_recv {
	if (req.method != "GET" && req.method != "HEAD") {
		return (pass);
	}

	if (req.http.X-Wikimedia-Debug || req.http.X-Wikimedia-Security-Audit == "1") {
		return (pass);
	}

	call evaluate_cookie;
	call pass_authorization;
}

sub text_common_hash {
	// The cookies below represent mobile preferences that can be set for anonymous users.
	if (req.http.X-Subdomain) {
		// Split the cache for the beta variant of the mobile site.
		if (req.http.X-Orig-Cookie ~ "(^|;\s*)optin=beta" || req.http.Cookie ~ "(^|;\s*)optin=beta") {
			hash_data("optin=beta");
		}
	}
}

sub text_common_backend_response_early {
	// Re-do the same changes as evalute_cookie above, but on the bereq
	// Cookie header and without bothering to save originals.  The reason we
	// have to do this very late in the game here is that Varnish 4 uses
	// bereq.http.Cookie to set the cache storage Vary:Cookie slot for a
	// fetched object shortly after vcl_backend_response.  Under Varnish3
	// when all this session-cookie related VCL was developed, the vary slot
	// was being set from req.http.Cookie.
	// Note this must happen in the "backend_response_early" hook before the
	// global common VCL tries to create generic hit-for-pass objects for
	// TTL<=0, or else the Vary-slotting will be wrong there!
	if (bereq.http.Cookie ~ "([sS]ession|Token)=") {
		set bereq.http.Cookie = "Token=1";
	} else {
		unset bereq.http.Cookie;
	}
}

sub text_common_backend_response {
	if (bereq.http.X-Subdomain && (bereq.url ~ "mobileaction=" || bereq.url ~ "useformat=")) {
		set beresp.ttl = 60 s;
	}

	// set a 607s hit-for-pass object based on response conditions in vcl_backend_response:
	//    Token=1 + Vary:Cookie:
	//    All requests with real login session|token cookies share the
	//    Cookie:Token=1 value for Vary purposes.  This allows them to
	//    share a single hit-for-pass object here if the response
	//    shouldn't be shared between users (Vary:Cookie).
	if (
		bereq.http.Cookie == "Token=1"
		&& beresp.http.Vary ~ "(?i)(^|,)\s*Cookie\s*(,|$)"
	) {
		set beresp.grace = 31s;
		set beresp.keep = 0s;
		set beresp.http.X-CDIS = "pass";
		// HFP
		return(pass(607s));
	}
}
