# This file is managed by Puppet!

import std;

# this is needed by geoip.inc.vcl and zero.inc.vcl, and in general is the only
#   way to sanely do Set-Cookie in the face of multiple independent cookies
#   being set from different code.
import header;

<%
def backend_option(backend, option, default)
	if @varnish_backend_options.kind_of?(Array)
		# List of hashes of options, 'backend_match' key is a regexp against the FQDN
		@varnish_backend_options.each do |be_options|
			if Regexp.new(be_options.fetch("backend_match", "^.*$")).match(backend)
				if be_options.has_key?(option)
					return be_options[option]
				end
			end
		end
		return default
	else
		return @varnish_backend_options.fetch(option, default)
	end
end

# Calculates number of director-level retries necessary for chash to hit all
# "n" backends with probability percentage "p", given they're randomly-mixed
# into an array considerably larger in size than "n".  This is an
# overestimation in that it assumes an infinite array, but the values still
# come out reasonably small compared to doing anything based on our actual
# weight*num_backends.
# Blame _joe_ for the math! :)
def chash_def_retries(p, n)
	x = n - 1
	if (x <= 0)
		return n
	end
	return ((Math.log10(100 - p) - 2) / (Math.log10(x) - Math.log10(n))).ceil
end
-%>

<% if @vcl_config.fetch( "enable_geoiplookup", false ) -%>
include "geoip.inc.vcl";
<% end -%>

# ACLs

acl purge { 
	"127.0.0.1";
}

acl wikimedia_nets {
<% scope.lookupvar('::network::constants::all_networks_lo').each do |entry|
	subnet, mask = entry.split("/", 2)
-%>
	"<%= subnet %>"/<%= mask %>;
<% end -%>
}

# Backend probes

# frontends in front of other varnish instances should send
# probes that don't depend on the app backend
probe varnish {
	.request =
		"GET /check HTTP/1.1"
		"Host: varnishcheck"
		"User-agent: Varnish backend check"
		"Connection: close";
	.timeout = 500ms;
	.interval = 100ms;
	.window = 3;
	.threshold = 2;
}

probe logstash {
	.url = "/status";
	.interval = 5s;
	.timeout = 1s;
	.window = 5;
	.threshold = 3;
}

probe maps {
	.url = "/_info";
	.interval = 5s;
	.timeout = 1s;
	.window = 5;
	.threshold = 3;
}

probe wdqs {
	.url = "/";
	.interval = 5s;
	.timeout = 1s;
	.window = 5;
	.threshold = 3;
}

# Backends

# List of Puppet generated backends
<%
@varnish_backends.each do |backend|
	name = /^[0-9\.]+$/.match(backend) ? "ipv4_" + backend.gsub(".", "_") : "be_" + backend.split(".")[0].gsub("-", "_")
	probe = backend_option(backend, "probe", nil)
-%>
backend <%= name %> {
	.host = "<%= backend %>";
	.port = "<%= backend_option(backend, "port", "80") %>";
	.connect_timeout = <%= backend_option(backend, "connect_timeout", "2s") %>;
	.first_byte_timeout = <%= backend_option(backend, "first_byte_timeout", "35s") %>;
	.between_bytes_timeout = <%= backend_option(backend, "between_bytes_timeout", "2s") %>;
	.max_connections = <%= backend_option(backend, "max_connections", "100") %>;
<% if probe -%>
	.probe = <%= probe %>;
<% end -%>
}

<% end -%>

<%
# Expected directors data format: (all keys required!)
# @varnish_directors = {
#     'director name' => {
#         'dynamic' => 'yes', # or 'no'
#         'type' => 'chash',
#         'backends' => [ "backend1", "backend2" ],
#     }
# }
if @use_dynamic_directors and @dynamic_directors -%>
include "directors.<%= @inst %>.vcl";

<% end -%>
<% @varnish_directors.keys.sort.each do |director_name|
director = @varnish_directors[director_name] 
if (!@dynamic_directors or director['dynamic'] != 'yes')
	backends = director['backends']
	if (!backends.empty?)
-%>
director <%= director_name %> <%= director['type'] %> {
<% if director['type'] == 'chash' -%>
	.retries = <%= chash_def_retries(99, backends.size) %>;
<% end -%>
<%
	backends.each do |backend|
		name = /^[0-9\.]+$/.match(backend) ? "ipv4_" + backend.gsub(".", "_") : "be_" + backend.split(".")[0].gsub("-", "_")
-%>
	{
		.backend = <%= name %>;
		.weight = <%= backend_option(backend, "weight", 10) %>;
	}
<% 	end -%>
}
<% end #if !empty -%>
<% end #if !dynamic -%> 
<% end #director loop -%>

# Functions

<% if @vcl_config.fetch("layer", "") == "frontend" -%>
// filter_headers + filter_noise are used (consistently) by the text+mobile frontends

sub filter_headers {
	if (req.restarts == 0) {
		unset req.http.X-Orig-Cookie;
	}
}

sub filter_noise {
	// Forged UAs on zerodot. This largely handles lazywebtools below, incidentally.
	if (req.http.host ~ "zero\.wikipedia\.org" && req.http.User-Agent && req.http.User-Agent ~ "Facebookbot|Googlebot") {
		error 403 "Noise";
	}

	if (req.http.referer && req.http.referer ~ "^http://(www\.(keeprefreshing|refreshthis|refresh-page|urlreload)\.com|tuneshub\.blogspot\.com|itunes24x7\.blogspot\.com|autoreload\.net|www\.lazywebtools\.co\.uk)/") {
		error 403 "Noise";
	}

	if (req.request == "POST" && req.url ~ "index\.php\?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=") {
		error 403 "Noise";
	}
}

// end frontend block
<% end -%>

// start frontend-only block for HTTPS
<% if @vcl_config.fetch("layer", "") == "frontend" && @vcl_config.fetch("https_redirects", false) -%>

// *** HTTPS recv code - domain-based 301/302->HTTPS decisions happen here
// if GET/HEAD filter is modified/removed later, keep in mind we need to not affect
//   the PURGE traffic here, as purge is called after this.
sub https_recv_redirect {
	if (req.http.X-Forwarded-Proto != "https") {
		if (req.request == "GET" || req.request == "HEAD") {
			// This is all of our unified cert wildcard domains which are TLS-clean (cert matches all extant hostnames within)
			// The lone exception now is wikimedia.org, in the next block
			if (req.http.Host ~ "(?i)(^|\.)(wikipedia|wikibooks|wikinews|wikiquote|wikisource|wikiversity|wikivoyage|wikidata|wikimediafoundation|wiktionary|mediawiki)\.org$") {
				set req.http.Location = "https://" + req.http.Host + req.url;
				error 751 "TLS Redirect";
			}
			// wikimedia.org has multi-level subdomains used for HTTP for which we have no certs, so they must be avoided here:
			// Ref: T102826 + T102827
			else if(req.http.Host ~ "(?i)^([^.]+\.)?(m\.)?wikimedia\.org$") {
				// For now, avoid matching commons for MW UAs. Ref: T102566
				if (!(req.http.Host ~ "(?i)^commons\.wikimedia\.org$" && req.http.User-Agent ~ "^MediaWiki/")) {
					set req.http.Location = "https://" + req.http.Host + req.url;
					error 751 "TLS Redirect";
				}
			}
		}
<% if @vcl_config.fetch("secure_post", true) -%>
		if (req.request == "POST" && !(req.http.Host ~ "(?i)\.beta\.wmflabs\.org$")) {
			error 403 "Insecure POST Forbidden - use HTTPS";
		}
<% end %>
	}
}

// *** HTTPS error code - implements 301 response for recv code
sub https_error_redirect {
	if (obj.status == 751) {
		set obj.http.Location = req.http.Location;
		set obj.status = 301;
		set obj.http.Content-Length = "0"; // T64245
		return(deliver);
	}
}

// *** HTTPS deliver code - domain-based HSTS headers
sub https_deliver_hsts {
	// The reason we don't need the stricter domain restrictions here,
	// like we do on the recv side for redirects, is that in order for
	// HSTS to reach a client, the client implicitly has to have already
	// successfully reached us over HTTPS for the given domainname.
	if (req.http.X-Forwarded-Proto == "https") {
		// This is the same regex as the first one in https_recv_redirect (all unified except wikimedia.org)
		if (req.http.Host ~ "(?i)(^|\.)(wikipedia|wikibooks|wikinews|wikiquote|wikisource|wikiversity|wikivoyage|wikidata|wikimediafoundation|wiktionary|mediawiki)\.org$") {
			set resp.http.Strict-Transport-Security = "max-age=31536000; includeSubDomains; preload";
		}
		else {
			set resp.http.Strict-Transport-Security = "max-age=31536000";
		}
	}
}

<% end -%>
// ^ end frontend + https_redirects block

// We shouldn't even legally be receiving proxy-style requests, as we're not a
// proxy from any client's point of view.  Just in case, we support it anyways
// according to RFC7230 rules: we ignore any Host header sent along with it
// and set a new Host header based on the host part we strip from the abs URI.
// ref: http://tools.ietf.org/html/rfc7230#section-5.4
sub rewrite_proxy_urls {
	if(req.url ~ "(?i)^https?://[^/]") {
		set req.http.Host = regsub(req.url, "(?i)^https?://([^/]+).*$", "\1");
		set req.url = regsub(req.url, "(?i)^https?://[^/]+", "");
	}
}

C{
	#include <string.h>
}C
sub normalize_path {
	/* Rewrite the path part of the URL, replacing unnecessarily escaped
	 * punctuation with the actual characters. The character list is from
	 * MediaWiki's wfUrlencode(), so the URLs produced here will be the same as
	 * the ones produced by MediaWiki in href attributes. Doing this reduces
	 * cache fragmentation and fixes T29935, i.e. stale cache entries due to
	 * MediaWiki purging only the wfUrlencode'd version of the URL.
	 */
	C{
		/* DIY hexadecimal conversion, since it is simple enough for a fixed
		 * width, and all the relevant standard C library functions promise to
		 * malfunction if the locale is set to anything other than "C"
		 */
		#define NP_HEX_DIGIT(c) ( \
			(c) >= '0' && (c) <= '9' ? (c) - '0' : ( \
				(c) >= 'A' && (c) <= 'F' ? (c) - 'A' + 0x0a : ( \
					(c) >= 'a' && (c) <= 'f' ? (c) - 'a' + 0x0a : -1 ) ) )
		#define NP_IS_HEX(c) (NP_HEX_DIGIT(c) != -1)
		#define NP_HEXCHAR(c1, c2) (char)( (NP_HEX_DIGIT(c1) << 4) | NP_HEX_DIGIT(c2) )
		const char * url = VRT_r_req_url(sp);
		size_t i, outPos;
		const size_t urlLength = strlen(url);
                // index for the last position %XX can start at:
		const size_t lastConvertIdx = urlLength > 2 ? urlLength - 3 : 0;
		char c;
		int dirty = 0;

		/* Allocate destination memory from the stack using the C99
		 * variable-length automatic feature. We know the length in advance
		 * because this function can only shorten the input string.
		 */
		char destBuffer[urlLength + 1];
		if (url) {
			for (i = 0, outPos = 0; i < urlLength; i++) {
				if (i <= lastConvertIdx && url[i] == '%' && NP_IS_HEX(url[i+1]) && NP_IS_HEX(url[i+2])) {
					c = NP_HEXCHAR(url[i+1], url[i+2]);
					if (c == ';'
						|| c == '@'
						|| c == '$'
						|| c == '!'
						|| c == '*'
						|| c == '('
						|| c == ')'
						|| c == ','
						|| c == '/'
						|| c == ':')
					{
						destBuffer[outPos++] = c;
						dirty = 1;
						i += 2;
					} else {
						destBuffer[outPos++] = url[i];
					}
				} else if (url[i] == '?') {
					/* Reached the query part. Just copy the rest of the URL
					 * to the destination.
					 */
					memcpy(destBuffer + outPos, url + i, sizeof(char) * (urlLength - i));
					outPos += urlLength - i;
					i = urlLength;
				} else {
					destBuffer[outPos++] = url[i];
				}
			}
			destBuffer[outPos] = '\0';

			/* Set req.url. This will copy our stack buffer into the workspace.
			 * VRT_l_req_url() is varadic, and concatenates its arguments. The
			 * vrt_magic_string_end marks the end of the list.
			 */
			if (dirty) {
				VRT_l_req_url(sp, destBuffer, vrt_magic_string_end);
			}
		}
		#undef NP_IS_HEX
		#undef NP_HEX_DIGIT
		#undef NP_HEXCHAR
	}C
}

sub vcl_recv_purge {
	/* Support HTTP PURGE */
	if (req.request == "PURGE") {
		if (!client.ip ~ purge) {
			error 405 "Denied.";
		} elsif (req.http.Host ~ "<%= @vcl_config.fetch('purge_host_regex') %>") {
			set req.hash_ignore_busy = true;
			return (lookup);
		} else {
			error 204 "Domain not cached here.";
		}
	}
}

// XFF / XFP / XRIP -handling (order is important below, should be at start of recv)
// TODO: Integration with zero-like XFF-handling, to skip over authorized proxies for XRIP
sub recv_ip_processing {
	// Everything here is req-mangling based on the req
	// itself, and some parts are non-idempotent, so do not
	// re-run this on a request restart
	if (req.restarts == 0) {
		if (client.ip !~ wikimedia_nets) {
			// Ensure we only accept XFP and XRIP from our nets.
			// Note this *should* be just the local tlsproxy
			// terminator, or passing through layers of varnish,
			// but we have known cases where applayer internal
			// things are faking XFP...
			unset req.http.X-Forwarded-Proto;
			unset req.http.X-Real-IP;
		}

		// Ensure XRIP is always set, in the case of varnish-fe and a
		// direct external request (otherwise nginx will have set it).
		if (!req.http.X-Real-IP) {
			set req.http.X-Real-IP = client.ip;
		}

		// All proxies need to update XFF with client.ip hop-by-hop
		if (req.http.X-Forwarded-For) {
			set req.http.X-Forwarded-For = req.http.X-Forwarded-For + ", " + client.ip;
		} else {
			set req.http.X-Forwarded-For = client.ip;
		}
	}

	// From this (very early) point forward:
	// client.ip -> the network-level source address, hop-by-hop
	// X-Real-IP -> the network-level source address when this
	// request first entered our infrastructure at the edge, without any
	// regard to external XFF header processing, and does not become
	// "wrong" from traversing our internal proxy layers.
}

sub vcl_recv {
	call recv_ip_processing;

<% if @vcl_config.fetch("layer", "frontend") != "frontend" -%>
	if (client.ip !~ wikimedia_nets) {
		// Do not allow direct access to non-frontend layers
		error 403 "Access denied";
	}
<% end -%>

	if (req.request !~ "<%= @vcl_config.fetch("allowed_methods", "^(GET|HEAD|POST|PURGE)$") %>"
		&& !(req.request == "OPTIONS" && req.http.Origin)) {
		error 403 "HTTP method not allowed.";
	}

	<% if @vcl_config.fetch("has_def_backend", "yes") == "yes" -%>
	/* Select the default backend/director, which is always the one named 'backend'.
	 * If an instance has no default 'backend', it must declare has_def_backend==no,
	 * and its own VCL must handle all possible req.backend cases.
	 */
	set req.backend = backend;

	if (req.backend.healthy) {
		set req.grace = 5m;
	} else {
		set req.grace = 60m;
	}
	<% end -%>
	
<% if @vcl_config.fetch("layer", "") == "frontend" -%>
	call rewrite_proxy_urls;
<% end -%>

<% if @vcl_config.fetch("layer", "") == "frontend" && @vcl_config.fetch("https_redirects", false) -%>
	call https_recv_redirect;
<% end -%>

	if ( req.http.host ~ "^varnishcheck" ) {
		error 200 "OK"; 
	}

	if (req.url ~ "^/beacon\/[^/?]+") {
		// Logging beacon endpoints
		//
		// They are handled by log tailers (varnishkafka and varnishncsa) that filter the
		// Varnish shm log for reqs to these endpoints and forward them to log processors
		// for storage and analysis.
		error 204;
	}

<% if @vcl_config.fetch("enable_geoiplookup", false) -%>
	if (req.url == "/geoiplookup" || req.http.host == "geoiplookup.wikimedia.org") {
		error 668 "geoiplookup";
	}
<% end -%>

	/* Function vcl_recv in <%= @vcl %>.inc.vcl will be appended here */
}

sub vcl_fetch {
	// default hard cap of max 30d life on all cache objects everywhere
	if (beresp.ttl > 30d) {
		set beresp.ttl = 30d;
	}

	/* Don't cache private, no-cache, no-store objects */
	if (beresp.http.Cache-Control ~ "(private|no-cache|no-store)") {
		set beresp.ttl = 0s;
		/* This should be translated into hit_for_pass later */
	}
	elsif (beresp.status >= 400 && beresp.status <= 499 && beresp.ttl > <%= @vcl_config.fetch("cache4xx", "5m") %>) {
		set beresp.ttl = <%= @vcl_config.fetch("cache4xx", "5m") %>;
	}
<% if @vcl_config.fetch("retry5xx", "0") == "1" -%>
	if (beresp.status >= 500 && beresp.status < 505) {
		# Retry the backend request 3 times, then give up and display
		# the backend's error page, instead of our own.
		#
		# Note that max_restarts is 4 by default, so Varnish would
		# otherwise detect this as a loop and present its own 503.
		if (req.restarts < 3) {
			return(restart);
		} else {
			return(deliver);
		}
	}
<% end -%>
	set beresp.grace = 60m;

<% if @vcl_config.fetch("do_gzip", false) -%>
	// Compress compressible things if the backend didn't already
	if (beresp.http.content-type ~ "^text/|^image/(x-icon|vnd\.microsoft\.icon)|(xml|html|javascript|json)$") {
		set beresp.do_gzip = true;
	}
<% end -%>

	/* Function vcl_fetch in <%= @vcl %>.inc.vcl will be appended here */
}

sub vcl_hit {
	if (req.request == "PURGE") {
		purge;
		error 204 "Purged";
	}
	
	/* Function vcl_hit in <%= @vcl %>.inc.vcl will be appended here */
}

sub vcl_miss {
	if (req.request == "PURGE") {
		purge;
		error 204 "Cache miss";
	}

	/* Function vcl_miss in <%= @vcl %>.inc.vcl will be appended here */
}

sub vcl_deliver {
<% if @vcl_config.fetch("layer", "") == "frontend" -%>
	std.collect(resp.http.Via);
	std.collect(resp.http.X-Varnish);
<% end -%>

	if (resp.http.X-Cache) {
		if (obj.hits > 0) {
			set resp.http.X-Cache = resp.http.X-Cache + ", <%= @hostname + (@name.empty? ? "" : " " + @name) %> hit (" + obj.hits + ")";
		} else {
			set resp.http.X-Cache = resp.http.X-Cache + ", <%= @hostname + (@name.empty? ? "" : " " + @name) %> miss (0)";
		}
	} else {
		if (obj.hits > 0) {
			set resp.http.X-Cache = "<%= @hostname + (@name.empty? ? "" : " " + @name) %> hit (" + obj.hits + ")";
		} else {
			set resp.http.X-Cache = "<%= @hostname + (@name.empty? ? "" : " " + @name) %> miss (0)";
		}
	}

<% if @vcl_config.fetch("layer", "") == "frontend" && @vcl_config.fetch("https_redirects", false) -%>
	call https_deliver_hsts;
<% end -%>

	/* Function vcl_deliver in <%= @vcl %>.inc.vcl will be appended here */
}

sub vcl_error {
<% if @vcl_config.fetch("layer", "") == "frontend" && @vcl_config.fetch("https_redirects", false) -%>
	call https_error_redirect;
<% end -%>

	if (obj.status == 400 || obj.status == 413) {
		return(deliver);
	}

<% if @vcl_config.fetch("retry503", "0") != "0" -%>
	if (obj.status == 503 && req.restarts < <%= @vcl_config["retry503"].to_i %>) {
		return(restart);
	}
<% end -%>
	if (obj.status == 204 && req.request == "PURGE") {
		set obj.http.Connection = "keep-alive";
	}

<% if @vcl_config.fetch("enable_geoiplookup", false) -%>
	// Support geoiplookup
	if (obj.status == 668) {
		call geoip_lookup;
		set obj.status = 200;
		set obj.http.Connection = "keep-alive";
		return (deliver);
	}
<% end -%>

	/* Function vcl_error in <%= @vcl %>.inc.vcl will be appended here */
}

sub vcl_hash {
	if (req.http.X-Wikimedia-Debug) {
		hash_data(req.http.X-Wikimedia-Debug);
	}
	if (req.http.X-Wikimedia-Security-Audit) {
		hash_data(req.http.X-Wikimedia-Security-Audit);
	}
	/* Function vcl_hash in <%= @vcl %>.inc.vcl will be appended here */
}


/* Include the VCL file for this role */
include "<%= @vcl %>.inc.vcl";
