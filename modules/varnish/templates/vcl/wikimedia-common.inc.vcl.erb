# This file is managed by Puppet!
// layer-common code for all clusters

import std;

# this is needed by geoip.inc.vcl and zero.inc.vcl, and in general is the only
#   way to sanely do Set-Cookie in the face of multiple independent cookies
#   being set from different code.
import header;

<%
def backend_option(backend, option, default)
	if @varnish_backend_options.kind_of?(Array)
		# List of hashes of options, 'backend_match' key is a regexp against the FQDN
		@varnish_backend_options.each do |be_options|
			if Regexp.new(be_options.fetch("backend_match", "^.*$")).match(backend)
				if be_options.has_key?(option)
					return be_options[option]
				end
			end
		end
		return default
	else
		return @varnish_backend_options.fetch(option, default)
	end
end

# Calculates number of director-level retries necessary for chash to hit all
# "n" backends with probability percentage "p", given they're randomly-mixed
# into an array considerably larger in size than "n".  This is an
# overestimation in that it assumes an infinite array, but the values still
# come out reasonably small compared to doing anything based on our actual
# weight*num_backends.
# Blame _joe_ for the math! :)
def chash_def_retries(p, n)
	x = n - 1
	if (x <= 0)
		return n
	end
	return ((Math.log10(100 - p) - 2) / (Math.log10(x) - Math.log10(n))).ceil
end
-%>

# ACLs

acl local_host {
	"127.0.0.1";
	"<%= @ipaddress %>"; // note this matches nginx proxy_pass for TLS
}

acl wikimedia_nets {
<% scope.lookupvar('::network::constants::all_networks_lo').each do |entry|
	subnet, mask = entry.split("/", 2)
-%>
	"<%= subnet %>"/<%= mask %>;
<% end -%>
}

# Backend probes

# frontends in front of other varnish instances should send
# probes that don't depend on the app backend
probe varnish {
	.request =
		"GET /check HTTP/1.1"
		"Host: varnishcheck"
		"User-agent: Varnish backend check"
		"Connection: close";
	.timeout = 500ms;
	.interval = 100ms;
	.window = 3;
	.threshold = 2;
}

probe logstash {
	.url = "/status";
	.interval = 5s;
	.timeout = 1s;
	.window = 5;
	.threshold = 3;
}

probe maps {
	.url = "/_info";
	.interval = 5s;
	.timeout = 1s;
	.window = 5;
	.threshold = 3;
}

probe wdqs {
	.url = "/";
	.interval = 5s;
	.timeout = 1s;
	.window = 5;
	.threshold = 3;
}

# Backends

# List of Puppet generated backends
<%
@varnish_backends.each do |backend|
	name = /^[0-9\.]+$/.match(backend) ? "ipv4_" + backend.gsub(".", "_") : "be_" + backend.split(".")[0].gsub("-", "_")
	probe = backend_option(backend, "probe", nil)
-%>
backend <%= name %> {
	.host = "<%= backend %>";
	.port = "<%= backend_option(backend, "port", "80") %>";
	.connect_timeout = <%= backend_option(backend, "connect_timeout", "2s") %>;
	.first_byte_timeout = <%= backend_option(backend, "first_byte_timeout", "35s") %>;
	.between_bytes_timeout = <%= backend_option(backend, "between_bytes_timeout", "2s") %>;
	.max_connections = <%= backend_option(backend, "max_connections", "100") %>;
<% if probe -%>
	.probe = <%= probe %>;
<% end -%>
}

<% end -%>

<%
# Expected directors data format: (all keys required!)
# @varnish_directors = {
#     'director name' => {
#         'dynamic' => 'yes', # or 'no'
#         'type' => 'chash',
#         'backends' => [ "backend1", "backend2" ],
#     }
# }
if @use_dynamic_directors and @dynamic_directors -%>
include "directors.<%= @inst %>.vcl";

<% end -%>
<% @varnish_directors.keys.sort.each do |director_name|
director = @varnish_directors[director_name] 
if (!@dynamic_directors or director['dynamic'] != 'yes')
	backends = director['backends']
	if (!backends.empty?)
-%>
director <%= director_name %> <%= director['type'] %> {
<% if director['type'] == 'chash' -%>
	.retries = <%= chash_def_retries(99, backends.size) %>;
<% end -%>
<%
	backends.each do |backend|
		name = /^[0-9\.]+$/.match(backend) ? "ipv4_" + backend.gsub(".", "_") : "be_" + backend.split(".")[0].gsub("-", "_")
-%>
	{
		.backend = <%= name %>;
		.weight = <%= backend_option(backend, "weight", 10) %>;
	}
<% 	end -%>
}
<% end #if !empty -%>
<% end #if !dynamic -%> 
<% end #director loop -%>

# Functions

sub recv_purge {
	/* Support HTTP PURGE */
	if (req.request == "PURGE") {
		if (client.ip !~ local_host) {
			error 405 "Denied.";
		} elsif (req.http.Host ~ "<%= @vcl_config.fetch('purge_host_regex') %>") {
			set req.hash_ignore_busy = true;
			return (lookup);
		} else {
			error 204 "Domain not cached here.";
		}
	}
}

sub wm_common_recv_before_access {
	unset req.http.X-CDIS; // clear internal cache-disposition header

	// XFF-appending is non-idempotent for restart purposes..
	if (req.restarts == 0) {
		// All layers need to update XFF with client.ip hop-by-hop so that it
		// looks right to layers beneath, including the app layer
		if (req.http.X-Forwarded-For) {
			set req.http.X-Forwarded-For = req.http.X-Forwarded-For + ", " + client.ip;
		} else {
			set req.http.X-Forwarded-For = client.ip;
		}
	}
}

sub wm_common_recv_after_access {
	if ( req.http.host ~ "^varnishcheck" ) {
		error 200 "OK"; 
	}

	<% if @vcl_config.fetch("has_def_backend", "yes") == "yes" -%>
	/* Select the default backend/director, which is always the one named 'backend'.
	 * If an instance has no default 'backend', it must declare has_def_backend==no,
	 * and its own VCL must handle all possible req.backend cases.
	 */
	set req.backend = backend;

	if (req.backend.healthy) {
		set req.grace = 5m;
	} else {
		set req.grace = 60m;
	}
	<% end -%>
}

sub wm_common_hit {
	set req.http.X-CDIS = "hit";
	if (req.request == "PURGE") {
		purge;
		error 204 "Purged";
	}
}

sub wm_common_miss {
	set req.http.X-CDIS = "miss";
	if (req.request == "PURGE") {
		purge;
		error 204 "Cache miss";
	}
}

sub wm_common_pass {
	if (req.http.X-CDIS) {
		// _pass can theoretically be called after moving through _hit or _miss
		set req.http.X-CDIS = req.http.X-CDIS + "+pass";
	} else {
		set req.http.X-CDIS = "pass";
	}
}

sub wm_common_fetch {
<% if @vcl_config.fetch("ttl_fixed", false) -%>
	// Fixed TTL (rare/questionable, only used on upload backend right now)
	// Note the ttl_cap comes after this and takes precedence!
	if (beresp.status <= 400) {
		set beresp.ttl = <%= @vcl_config.fetch("ttl_fixed", "invalid") %>;
	}
<% end -%>

	// Hard TTL cap on all fetched objects (default 21d)
	if (beresp.ttl > <%= @vcl_config.fetch("ttl_cap", "21d") %>) {
		set beresp.ttl = <%= @vcl_config.fetch("ttl_cap", "21d") %>;
	}

	/* Don't cache private, no-cache, no-store objects */
	if (beresp.http.Cache-Control ~ "(private|no-cache|no-store)") {
		set beresp.ttl = 0s;
		/* This should be translated into hit_for_pass later */
	}
	elsif (beresp.status >= 400 && beresp.status <= 499 && beresp.ttl > <%= @vcl_config.fetch("cache4xx", "5m") %>) {
		set beresp.ttl = <%= @vcl_config.fetch("cache4xx", "5m") %>;
	}

	set beresp.grace = 60m;

<% if @vcl_config.fetch("do_gzip", false) -%>
	// Compress compressible things if the backend didn't already
	if (beresp.http.content-type ~ "json|text|html|script|xml|icon|ms-fontobject|ms-opentype|x-font") {
		set beresp.do_gzip = true;
	}
<% end -%>
}

sub wm_common_deliver {
	if (!req.http.X-CDIS) {
		set req.http.X-CDIS = "int"; // internally-generated response (not a cache object hit, and not a miss|pass to a deeper layer either)
	}

	if (resp.http.X-Cache) {
		set resp.http.X-Cache = resp.http.X-Cache + ", <%= @hostname + (@name.empty? ? "" : " " + @name) %> " + req.http.X-CDIS + "(" + obj.hits + ")";
	} else {
		set resp.http.X-Cache = "<%= @hostname + (@name.empty? ? "" : " " + @name) %> " + req.http.X-CDIS + "(" + obj.hits + ")";
	}
}

sub wm_common_error {
	if (obj.status == 204 && req.request == "PURGE") {
		set obj.http.Connection = "keep-alive";
	}
}

/* Include the VCL file for this role */
include "<%= @vcl %>.inc.vcl";
