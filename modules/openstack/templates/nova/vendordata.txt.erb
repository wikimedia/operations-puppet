MIME-Version: 1.0
Content-Type: multipart/mixed; boundary="XXXXboundary text"

This is a multipart config in MIME format.
It contains a cloud-init config followed by
a first boot script.

--XXXXboundary text
MIME-Version: 1.0
Content-Type: text/jinja2; charset="us-ascii"

## template: jinja
#cloud-config

datasource:
  OpenStack:
    timeout: 10
    max_wait: 120
    retries: 10

hostname: {{ds.meta_data.name}}
fqdn: {{ds.meta_data.name}}.{{ds.meta_data.project_id}}.<%= @dhcp_domain %>

# Prevent puppet from doing things before we're ready.
# APT fails to acquire GPG keys if packages dirmngr or gpg are missing
bootcmd:
  - [ cloud-init-per, once, disablepuppet, puppet, agent, --disable, "prevent puppet runs during cloud-init" ]
  - [ cloud-init-per, once, dirmngr-aptupdate, apt-get, update ]
  - [ cloud-init-per, once, dirmngr-aptinstall, apt-get, install, "-y", dirmngr ]
  - [ cloud-init-per, once, gnupginstall, apt-get, install, "-y", gnupg ]


# /etc/block-ldap-key-lookup:
#   Prevent non-root logins while the VM is being setup
#   The ssh-key-ldap-lookup script rejects non-root user logins if this file
#   is present.
#
# /etc/rsyslog.d/60-puppet.conf:
#   Enable console logging for puppet
#
# /etc/systemd/system/serial-getty@ttyS0.service.d/override.conf:
#   Enable root console on serial0
#   (cloud-init will create any needed parent dirs)
write_files:
    - content: "VM is work in progress"
      path: /etc/block-ldap-key-lookup
    - content: "daemon.* |/dev/console"
      path: /etc/rsyslog.d/60-puppet.conf
    - content: |
        [Service]
        ExecStart=
        ExecStart=-/sbin/agetty --autologin root --noclear %I $TERM
      path: /etc/systemd/system/serial-getty@ttyS0.service.d/override.conf

# resetting ttys0 so root is logged in
runcmd:
    - [systemctl, enable, serial-getty@ttyS0.service]
    - [systemctl, restart, serial-getty@ttyS0.service]

ssh_pwauth: False

manage_etc_hosts: true


# gpg: needed for apt to function properly
# pciutils: used by puppet to determine is_virtual
packages:
    - gpg
    - pciutils

# Clear default partitioning/mounting behavior so puppet can manage it
disk_setup:
fs_setup:

# Mark out ephemeral0 so that cloud-init leaves it alone;
#  we'll let puppet mount it later.
mounts:
    - [ephemeral0]

# You'll see that we're setting apt_preserve_sources_list twice here.  That's
#  because there's a bug in cloud-init where it tries to reconcile the
#  two settings and if they're different the stage fails. That means that
#  if one of them is set differently from the default (True) then nothing
#  works.
apt_preserve_sources_list: False
apt:
    preserve_sources_list: False
    sources:
        wikimedia.list:
            source: |
                deb http://apt.wikimedia.org/wikimedia $RELEASE-wikimedia main
                deb-src http://apt.wikimedia.org/wikimedia $RELEASE-wikimedia main
            filename: wikimedia.list
            keyid: 9D392D3FFADF18FB

package_update: true
package_upgrade: true

# This configures puppet and starts the puppet agent (which we don't want).
#  When the boot script starts we'll kill the agent and run puppet synchronously.
puppet:
    start_service: false
    exec: false
    conf:
        agent:
            server: 'puppet'
            certname: '%f'


--XXXXboundary text
MIME-Version: 1.0
Content-Type: text/text/x-shellscript; charset="us-ascii"
#!/bin/bash

set -x

# Function to test if puppet is running or not
puppet_is_running() {
    PUPPETLOCK=`puppet agent --configprint agent_catalog_run_lockfile`
    echo "checking if puppet is running by looking at ${PUPPETLOCK}"
    # If no lockfile is defined, we can't really tell what's happening.
    # Assume puppet is not running in this case
    test -n "$PUPPETLOCK" || return 1

    # If the lockfile is not present, puppet is not running
    test -e "$PUPPETLOCK" || return 1

    # Now let's see if the PID at $PUPPETLOCK is indeed present and running
    local puppetpid=$(cat "$PUPPETLOCK")
    local cmdline_file="/proc/${puppetpid}/cmdline"
    if [ -f "$cmdline_file" ]; then
        # Puppet is indeed running
        grep -q puppet "$cmdline_file" && return 0
    fi

    # The lock file is stale, delete it so that we're all on the same page
    echo "Clearing stale puppet agent lock for pid ${puppetpid}"
    rm $PUPPETLOCK
    return 1
}

# loop function to wait for puppet to finish its execution
wait_for_puppet() {
    local attempts=${1:-30}
    for _ in $(seq "$attempts"); do
        if ! puppet_is_running; then
            return 0
        fi
        echo "Waiting for puppet run to complete"
        sleep 10
    done
    # If puppet is still running at this point, report an error
    echo "Tired of waiting for puppet run to complete; giving up"
    return 1
}

# Upstream debian cloud base images come with chrony installed,
# as well as a service user with an invalid (>499) ID.  We can't
# just uninstall chrony because systemd depends on it; installing
# systemd-timesyncd uninstalls chrony but then runs afoul of the
# user ID that was created by chrony. To work around all that,
# we detect if chrony is installed and if it is, remove the
# user it installed (without uninstalling chrony).  That unblocks
# us for a subsequent install of systemd-timesyncd (which /does/
# uninstall chrony).
if /usr/bin/dpkg -l chrony | grep -q ^ii; then
    /usr/sbin/userdel systemd-timesync
    /usr/sbin/userdel systemd-coredump
    apt install -y systemd-timesyncd
fi

systemctl restart nscd.service
dpkg-reconfigure -fnoninteractive -pcritical openssh-server
systemctl restart ssh.service
nscd -i hosts

# Run puppet, twice.  The second time is just to pick up packages
#  that may have been unavailable in apt before the first puppet run
#  updated sources.list
puppet agent --enable

# Just in case puppet is already running for some reason, wait
#  for it to finish.
wait_for_puppet 120
service puppet stop
wait_for_puppet 120

# Sometimes the previous --enable doesn't take. Don't know why but
#  it's harmless to try again
puppet agent --enable
puppet agent --onetime --verbose --no-daemonize --no-splay --show_diff

# Refresh ldap now that puppet has updated our ldap.conf
systemctl restart nslcd.service

# Ensure all NFS mounts are mounted
mount_attempts=1
until [ $mount_attempts -gt 10 ]
do
    echo "Ensuring all NFS mounts are mounted, attempt ${mount_attempts}"
    echo "Ensuring all NFS mounts are mounted, attempt ${mount_attempts}" >> /etc/block-ldap-key-lookup
    ((mount_attempts++))
    /usr/bin/timeout --preserve-status -k 10s 20s /bin/mount -a && break
    # Sleep for 10s before next attempt
    sleep 10
done

# Run puppet again post mounting NFS mounts (if all the mounts hadn't been mounted
# before, the puppet code that ensures the symlinks are created, etc may not
# have run)
wait_for_puppet 10
puppet agent -t

# One last puppet run to avoid possible races. I'm (AGB) not totally sure what's
# happening here but occasionally the very first run fails but subsequent runs
# work and we really need two good ones before logins work.
wait_for_puppet 10
puppet agent -t

# sssd might be caching sudo rules and other ldap things from before puppet
# finished setting up ldap integration. Restart sssd and clear its cache.
# On some old images we won't have sssd installed; in that case this will
# throw some harmless errors.
systemctl stop sssd
rm -rf /var/lib/sss/db/*
systemctl restart sssd


# Remove the non-root login restriction
rm /etc/block-ldap-key-lookup

# mark the host as ready, from now on alerts and checks will send notifications (ex. puppet_alert.py)
touch /.cloud-init-finished
exit 0
--XXXXboundary text
