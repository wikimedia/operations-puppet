# SPDX-License-Identifier: Apache-2.0

[DEFAULT]

use_syslog = true
syslog_log_facility = LOG_LOCAL0

# set the wsgi loglevel to 'WARN' to filter out haproxy pings
default_log_levels = neutron.wsgi=WARN

# debug = true

transport_url = rabbit://<%= @rabbitmq_nodes.map{ |rabbit_host| "#{@rabbit_user}:#{@rabbit_pass}\@#{rabbit_host}:5671" }.join(',') %>

auth_strategy = keystone
bind_port = <%= @bind_port %>

# New standards would dictate this should be the 
# terse 'ml2' only but the Debian package for
# neutron-server is doing some dynamic path loading
# for the ini file that expects the verbose name here.
# Use the verbose name until it is an issue
core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin

# metering?
service_plugins = router

# Do not reschedule the dataplane components
# just because a control plane l3-agent is maint
# or restarted. This also means it will not get changes
# so we need to monitor l3-agent state administratively.
allow_automatic_l3agent_failover = false

# allow vxlan use for VRRP without
# enabling tenant created networks
l3_ha_network_type = vxlan

# run 2 dhcp agents per for HA
dhcp_agents_per_network = 2

dns_domain = <%= @dhcp_domain %>

# https://docs.openstack.org/mitaka/networking-guide/config-az.html#l3-high-availability
# make routers HA "by default"
l3_ha = True
# maximum number of network nodes to use for the HA router
max_l3_agents_per_router = 2

# The default for this is 60.  Leaving it at 60 causes us to lose track
#  of services at times (T205524) but cutting it all the way down to
#  3 (which worked in m,n,o,p,q) causes things to timeout in Rocky.
#
# Update 2022-06-17 (AGB) I'm seeing lots of timeouts here
#  that encourage me to adjust this limit. For now I'm commenting
#  this setting out and letting it fall back on the default. Before
#  this change it was set to 30.
# rpc_response_timeout = 60

#notify_nova_on_port_status_changes = True
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True

agent_down_time = <%= @agent_down_time %>

# db connection contention
api_workers = 12
rpc_workers = 16

[agent]
root_helper = sudo neutron-rootwrap /etc/neutron/rootwrap.conf
log_agent_heartbeats = <%= @log_agent_heartbeats %>

[cors]
[cors.subdomain]


<%= scope.call_function('template', ["openstack/#{@version}/common/database.erb"]) %>

<%= scope.call_function('template', ["openstack/#{@version}/common/keystone_authtoken.erb"]) %>

[matchmaker_redis]

[nova]
region_name = <%= @region %>
auth_url = https://<%= @keystone_fqdn %>:25000/v3
auth_type = v3password
password = <%= @ldap_user_pass %>
project_domain_name = default
project_name = admin
tenant_name = admin
user_domain_id = default
user_domain_name = default
username = novaadmin

[oslo_concurrency]
lock_path = /var/lock/neutron

[oslo_messaging_amqp]
[oslo_messaging_notifications]

<%= scope.call_function('template', ["openstack/#{@version}/common/oslo_messaging_rabbit.erb"]) %>

[oslo_policy]
enforce_scope = <%= @enforce_policy_scope %>
enforce_new_defaults = <%= @enforce_new_policy_defaults %>
policy_file = policy.yaml

[quotas]
quota_floatingip = 0
quota_security_group = 40
quota_security_group_rule = 100

[ssl]

[securitygroup]
# Be warned that this section also exists in plugins/ml2/ml2_conf.ini and
#  it's probably good to keep the two in sync.  Or maybe that other file
#  doesn't do anything, it's unclear.

enable_security_group = True
# Networking uses iptables to achieve security group functions.
# L2 agent with enable_ipset option enabled, it makes use of
# IPset to improve security groupâ€™s performance, as it represents
# a hash set which is insensitive to the number of elements.
# When a port is created, L2 agent will add an additional IPset chain
# to its iptables chain, if the security group that this port belongs
# to has rules between other security group, the member of that
# security group will be added to the ipset chain.
#
enable_ipset = True
# neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
firewall_driver = iptables

[experimental]
linuxbridge = true
