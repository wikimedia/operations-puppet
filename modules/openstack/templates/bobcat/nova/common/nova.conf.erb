# SPDX-License-Identifier: Apache-2.0

[DEFAULT]
# nova general settings
my_ip = <%= @ipaddress %>
osapi_compute_listen_port = <%= @osapi_compute_listen_port %>
metadata_listen_port = <%= @metadata_listen_port %>
# Note: as of Wallaby this setting seems to have no effect;
#  The worker count is actually controlled by the 'processes' setting
#  in nova-api-uwsgi.ini.
osapi_compute_workers = <%= @compute_workers %>

# Note: as of Wallaby this setting seems to have no effect;
#  The worker count is actually controlled by the 'processes' setting
#  in nova-api-metadata-uwsgi.ini.
metadata_workers = <%= @compute_workers %>
state_path = /var/lib/nova
rootwrap_config = /etc/nova/rootwrap.conf
log_dir=/var/log/nova
pybasedir = /usr/lib/python2.7/dist-packages
#use_rootwrap_daemon = True ### FIXME may be used only in compute nodes?

use_syslog = True
syslog_log_facility = LOG_LOCAL0
default_log_levels = nova=WARN
#debug = true


# ALERT ALERT ALERT
#
# The following setting (transport_url) is mirrored in the nova api database;
#  changing it here is necessary but not sufficient. The setting must also
#  be manually updated via sql. In the eqiad1 deployment the setting is
#  in nova_api_eqiad1.cell_mappings in the 'NULL' cell.
#
# ALERT ALERT ALERT
transport_url = rabbit://<%= @rabbitmq_nodes.map{ |rabbit_host| "#{@rabbit_user}:#{@rabbit_pass}\@#{rabbit_host}:5671" }.join(',') %>

# Schedule things that are for some reason not in the [scheduler] section
ram_allocation_ratio=1.0
initial_ram_allocation_ratio=1.0

disk_allocation_ratio=1.5
initial_disk_allocation_ratio=1.5

cpu_allocation_ratio=4.0
initial_cpu_allocation_ratio=4.0


default_schedule_zone = <%= @default_schedule_zone %>

# compute
compute_driver=libvirt.LibvirtDriver
connection_type=libvirt
instance_name_template=i-%08x
#daemonize=1 ### FIXME unknown yet, it seems like 'daemon' in some configs
enabled_apis=osapi_compute, metadata
osapi_compute_unique_server_name_scope='project'
# rpc_response_timeout=180
instance_usage_audit = False
compute_monitors=virt_driver


# APIs
enabled_apis = osapi_compute,metadata

max_concurrent_live_migrations = 3

[quota]
# quotas
cores = 8
instances = 8
ram = 16384

<% if @is_control_node -%>
[api]
# we use this vendordata JSON file to provide the domain to VM instances
vendordata_jsonfile_path = /etc/nova/vendor_data.json

# These should only be present for apis and nova-conductor.
[api_database]
connection = mysql+pymysql://<%= @db_user %>:<%= @db_pass %>@<%= @db_host %>/<%= @db_name_api %>

# ALERT ALERT ALERT
#
# The following setting (connection) is mirrored in the nova api database;
#  changing it here is necessary but not sufficient. The setting must also
#  be manually updated via sql. In the eqiad1 deployment the setting is
#  in nova_api_eqiad1.cell_mappings in the 'NULL' cell.
#
# ALERT ALERT ALERT
<%= scope.call_function('template', ["openstack/#{@version}/common/database.erb"]) %>
<% end -%>

[scheduler]
workers = 3

[filter_scheduler]
enabled_filters = <%= @scheduler_filters.join(",") %>
weight_classes=nova.scheduler.weights.metrics.MetricsWeigher, nova.scheduler.weights.affinity.ServerGroupSoftAntiAffinityWeigher, nova.scheduler.weights.affinity.ServerGroupSoftAffinityWeigher
host_subset_size = 2

<%= scope.call_function('template', ["openstack/#{@version}/common/keystone_authtoken.erb"]) %>

<%= scope.call_function('template', ["openstack/#{@version}/common/servicetoken.erb"]) %>

<%= scope.call_function('template', ["openstack/#{@version}/common/oslo_messaging_rabbit.erb"]) %>

[oslo_concurrency]
lock_path = /var/lock/nova

[oslo_messaging_notifications]
# The Drivers(s) to handle sending notifications. Possible values are messaging,
# messagingv2, routing, log, test, noop (multi valued)
driver = messagingv2

# AMQP topic used for OpenStack notifications. (list value)
# Deprecated group/name - [rpc_notifier2]/topics
# Deprecated group/name - [DEFAULT]/notification_topics
topics = notifications

[vnc]
enabled=False

# Why enable Spice when we aren't set up with a spice proxy?
# It's because enabling this results in a serial1 interface being
# created in new VMs, which we can connect to with virsh for
# debugging.
#
# Additionally, our Stretch image seems to not boot at
# all without this.
[spice]
enabled = True
agent_enabled = False

[conductor]
workers = 4

[METRICS]
weight_setting = cpu.percent=-1.0

[neutron]
service_metadata_proxy = true
metadata_proxy_shared_secret = <%= @metadata_proxy_shared_secret %>
auth_url = https://<%= @keystone_fqdn %>:25000/v3
auth_type = v3password
password = <%= @ldap_user_pass %>
project_domain_name = default
project_name = admin
tenant_name = admin
user_domain_id = default
user_domain_name = default
username = novaadmin
default_floating_pool = ext-nat

[placement]
region_name = <%= @region %>
project_domain_name = Default
project_name = admin
auth_type = password
user_domain_name = Default
auth_url = https://<%= @keystone_fqdn %>:25357/v3
username = novaadmin
password = <%= @ldap_user_pass %>

[wsgi]
api_paste_config=/etc/nova/api-paste.ini

[oslo_policy]
enforce_scope = <%= @enforce_policy_scope %>
enforce_new_defaults = <%= @enforce_new_policy_defaults %>
policy_file = policy.yaml

[image_cache] 
remove_unused_base_images=True
