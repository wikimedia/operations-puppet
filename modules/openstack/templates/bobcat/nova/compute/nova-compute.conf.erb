# SPDX-License-Identifier: Apache-2.0

[DEFAULT]
# Are these the same?
#compute_driver=nova.virt.libvirt.LibvirtDriver
compute_driver=libvirt.LibvirtDriver

# The default here is vfat for some reason
default_ephemeral_format = ext4

# Ensure VMs have the same state after a host reboot
resume_guests_state_on_host_boot = True

# Path to genisoimage, used for config drive
mkisofs_cmd = /usr/bin/genisoimage

# No plans to use ovs
# neutron_ovs_bridge=br-int

[libvirt]
virt_type=kvm
vif_driver=nova.virt.libvirt.vif.LibvirtGenericVIFDriver
use_virtio_for_bridges=true

# live_migration_bandwidth is documented in the code, and nowhere else.
# 'Maximum bandwidth to be used during migration, in Mbps'
# Limit this to around a third of available 1Gbps connection so we don't
# throttle running instances when migrating.
live_migration_bandwidth=300

# This setting is deprecated but it's not clear that we can
#  actually live without it. In theory we can handle this
#  by using live_migration_with_native_tls but I (AGB)
#  haven't had much luck so far.
#
# There's some context for another user having a similar issue
#  at https://bugs.launchpad.net/nova/+bug/1671288
live_migration_uri=qemu://%s.<%= scope.lookupvar("::site") %>.wmnet/system?pkipath=/var/lib/nova

# in seconds:
live_migration_completion_timeout=600

# in milliseconds:
live_migration_downtime = 2000

# Throttle CPU if we are otherwise failing to migrate.
live_migration_permit_auto_converge = True

# Last chance: if we still can't migrate with the above settings,
#  just pause the VM and get it moved.
live_migration_timeout_action = 'force_complete'


# Number of incremental steps to reach max downtime value.
# The default is 10, decrease to 8 so we get more aggressive about migration
# faster.
live_migration_downtime_steps = 8

# Time to wait, in seconds, between each step increase of the migration downtime.
# The default value for this is 75; let's fail quicker so we can get on with things.
live_migration_downtime_delay = 30


<% if @enable_nova_rbd %>
# Ceph RBD ephemeral config
images_type = rbd
images_rbd_pool = <%= @ceph_rbd_pool %>
images_rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = <%= @ceph_rbd_client_name %>
rbd_secret_uuid = <%= @libvirt_rbd_uuid %>
disk_cachemodes = "network=writeback"

# Define custom CPU restriction to the lowest
# common subset of features across all hypervisors.
# This varies by deployment depending on what hypervisors
# are active.  Details at https://wikitech.wikimedia.org/wiki/Portal:Cloud_VPS/Admin/Ceph#CPU_Model_Type
cpu_mode=custom
cpu_models=<%= @libvirt_cpu_model %>
<% end -%>
