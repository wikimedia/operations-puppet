<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true"  scanPeriod="5 minutes" packagingData="false">

    <!-- ugly trick to ensure ${HOSTNAME} is evaluated -->
    <property scope="context" name="hostname" value="${HOSTNAME}" />

    <%- if @evaluators -%>
    <evaluator name="SYNTAX_ERROR">
        <expression>
            throwable != null &amp;&amp; throwable.getCause() != null &amp;&amp;
            throwable instanceof java.util.concurrent.ExecutionException &amp;&amp;
            throwable.getCause() instanceof org.openrdf.query.MalformedQueryException
        </expression>
    </evaluator>
    <evaluator name="QUERY_TIMEOUT">
        <expression>
            throwable != null &amp;&amp; throwable instanceof java.util.concurrent.TimeoutException
        </expression>
    </evaluator>
    <%- end -%>

    <!--
        File based logs:
        * rolling every day or when size > 100MB
    -->
    <appender name="file" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file><%= @log_dir %>/<%= @title %>.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <!-- daily rollover -->
            <fileNamePattern><%= @log_dir %>/<%= @title %>.%d{yyyy-MM-dd}.%i.log.gz</fileNamePattern>
            <maxFileSize>100MB</maxFileSize>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern><%= @pattern %></pattern>
            <outputPatternAsHeader>true</outputPatternAsHeader>
        </encoder>
    </appender>
    <appender name="async-file" class="ch.qos.logback.classic.AsyncAppender">
        <neverBlock>true</neverBlock>
        <appender-ref ref="file" />
    </appender>

    <!--
        Console based logs:
        * per logger / message throttling is enabled
        * limited to 10 messages per second
        * level => ERROR
    -->
    <appender name="stdout" class="ch.qos.logback.core.ConsoleAppender">
        <filter class="org.wikidata.query.rdf.common.log.PerLoggerThrottler" />
        <filter class="org.wikidata.query.rdf.common.log.RateLimitFilter">
            <bucketCapacity>10</bucketCapacity>
            <refillIntervalInMillis>1000</refillIntervalInMillis>
        </filter>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>error</level>
        </filter>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern><%= @pattern %></pattern>
            <outputPatternAsHeader>true</outputPatternAsHeader>
        </encoder>
    </appender>
    <appender name="async-stdout" class="ch.qos.logback.classic.AsyncAppender">
        <neverBlock>true</neverBlock>
        <appender-ref ref="stdout" />
    </appender>

    <!--
        Logstash:
        * per logger / message throttling is enabled
        * limited to 100 messages per second
        * level => INFO
    -->
    <appender name="logstash" class="net.logstash.logback.appender.LogstashSocketAppender">
        <filter class="org.wikidata.query.rdf.common.log.PerLoggerThrottler" />
        <filter class="org.wikidata.query.rdf.common.log.RateLimitFilter">
            <bucketCapacity>100</bucketCapacity>
            <refillIntervalInMillis>1000</refillIntervalInMillis>
        </filter>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>info</level>
        </filter>
        <host><%= @logstash_host %></host>
        <port><%= @logstash_port %></port>
        <customFields>{"program":"<%= @title %>"}</customFields>
    </appender>
    <appender name="async-logstash" class="ch.qos.logback.classic.AsyncAppender">
        <neverBlock>true</neverBlock>
        <appender-ref ref="logstash" />
    </appender>

    <%- if @sparql -%>
    <appender name="file-sparql" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file><%= @log_dir %>/<%= @title %>-sparql.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <!-- daily rollover -->
            <fileNamePattern><%= @log_dir %>/<%= @title %>.%d{yyyy-MM-dd}-sparql.%i.log.gz</fileNamePattern>
            <maxFileSize>1500MB</maxFileSize>
            <maxHistory>3</maxHistory>
        </rollingPolicy>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>%d{HH:mm:ss.SSS} %msg%n</pattern>
            <outputPatternAsHeader>false</outputPatternAsHeader>
        </encoder>
    </appender>
    <appender name="async-file-sparql" class="ch.qos.logback.classic.AsyncAppender">
        <neverBlock>true</neverBlock>
        <appender-ref ref="file-sparql" />
    </appender>

    <logger name="org.wikidata.query.rdf.tool.rdf.client.RdfClient" level="trace" additivity="false">
        <appender-ref ref="async-file-sparql"/>
    </logger>
    <%- end -%>

    <root level="warn">
        <appender-ref ref="async-file"/>
        <appender-ref ref="async-stdout"/>
        <appender-ref ref="async-logstash"/>
    </root>

    <logger name="org.wikidata.query.rdf" level="info"/>
    <logger name="org.wikidata.query.rdf.blazegraph.inline.literal.AbstractMultiTypeExtension" level="error"/>
    <logger name="com.bigdata.util.concurrent.Haltable" level="off"/>
    <logger name="com.bigdata.rdf.internal.LexiconConfiguration" level="off"/> <!-- disabled temp. ref: T207643 -->

</configuration>
